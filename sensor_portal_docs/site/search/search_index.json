{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ARISE MDS sensor portal documentation Documentation for MDS Sensor portal. General overviews of the data structure can be found under \"Data structure\" . Help for administrators can be found under \"For Admins\" . Technical details can be found under \"For developers\" .","title":"Home"},{"location":"#arise-mds-sensor-portal-documentation","text":"Documentation for MDS Sensor portal. General overviews of the data structure can be found under \"Data structure\" . Help for administrators can be found under \"For Admins\" . Technical details can be found under \"For developers\" .","title":"ARISE MDS sensor portal documentation"},{"location":"about/","text":"About The ARISE MDS sensor portal is an improved open source version of software originally developed to manage data as part of the ARISE project's monitoring demonstration sites. The portal allows you to - Register sensors - Register deployments of sensors - Groups sensors into projects - Import data automatically via the API or by linking to external storage. - Directly import data. - View metadata - Annotate data. - Send data to AI models. - Export data in a variety of formats. - Push data to TAR archives. This version has been designed to be as generic as possible and not be tied to any particular infrastructure. MIT License Copyright (c) 2024 Julian Evans Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"About"},{"location":"about/#about","text":"The ARISE MDS sensor portal is an improved open source version of software originally developed to manage data as part of the ARISE project's monitoring demonstration sites. The portal allows you to - Register sensors - Register deployments of sensors - Groups sensors into projects - Import data automatically via the API or by linking to external storage. - Directly import data. - View metadata - Annotate data. - Send data to AI models. - Export data in a variety of formats. - Push data to TAR archives. This version has been designed to be as generic as possible and not be tied to any particular infrastructure. MIT License Copyright (c) 2024 Julian Evans Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"About"},{"location":"admin/","text":"","title":"Overview"},{"location":"admin/models/","text":"Admin only models This is an overview of models that can only be registered in the admin panel. Data types Device models External storage Archive","title":"Admin only models"},{"location":"admin/models/#admin-only-models","text":"This is an overview of models that can only be registered in the admin panel.","title":"Admin only models"},{"location":"admin/models/#data-types","text":"","title":"Data types"},{"location":"admin/models/#device-models","text":"","title":"Device models"},{"location":"admin/models/#external-storage","text":"","title":"External storage"},{"location":"admin/models/#archive","text":"","title":"Archive"},{"location":"admin/permissions/","text":"Permissions MANAGER A user that is designated as a manager can add and edit projects, sensors and deployments. A manager is automatically designated as a viewer and an annotater, with the corresponding rights. ANNOTATOR A user designated as an annotater can add species observations to datafiles. An annotator can validate uncertain observations from other users, and observations made by AI algorithms for species identification. VIEWER A user that is designated as a viewer can view datafiles within the projects, deployments, and/or files that s/he has been granted access to.","title":"Permissions"},{"location":"admin/permissions/#permissions","text":"","title":"Permissions"},{"location":"admin/permissions/#manager","text":"A user that is designated as a manager can add and edit projects, sensors and deployments. A manager is automatically designated as a viewer and an annotater, with the corresponding rights.","title":"MANAGER"},{"location":"admin/permissions/#annotator","text":"A user designated as an annotater can add species observations to datafiles. An annotator can validate uncertain observations from other users, and observations made by AI algorithms for species identification.","title":"ANNOTATOR"},{"location":"admin/permissions/#viewer","text":"A user that is designated as a viewer can view datafiles within the projects, deployments, and/or files that s/he has been granted access to.","title":"VIEWER"},{"location":"admin/detail/","text":"WORDS","title":"Index"},{"location":"admin/detail/archive/","text":"Archive Description: Represents a remote archive storage system used for storing TAR files of collected data. Fields Field Type Description name CharField Human-readable name for the archive. username CharField SSH login username. password EncryptedCharField Encrypted SSH login password. address CharField Network address of the archive server. owner ForeignKey User who owns this archive. root_folder CharField Root folder path on the archive server. Methods __str__() : Returns the archive name. init_ssh_client() : Initializes an SSH client for this archive. check_projects() : Invokes logic to check and sync archive projects. check_upload() : Invokes logic to validate and upload files.","title":"Archive"},{"location":"admin/detail/archive/#archive","text":"Description: Represents a remote archive storage system used for storing TAR files of collected data.","title":"Archive"},{"location":"admin/detail/archive/#fields","text":"Field Type Description name CharField Human-readable name for the archive. username CharField SSH login username. password EncryptedCharField Encrypted SSH login password. address CharField Network address of the archive server. owner ForeignKey User who owns this archive. root_folder CharField Root folder path on the archive server.","title":"Fields"},{"location":"admin/detail/archive/#methods","text":"__str__() : Returns the archive name. init_ssh_client() : Initializes an SSH client for this archive. check_projects() : Invokes logic to check and sync archive projects. check_upload() : Invokes logic to validate and upload files.","title":"Methods"},{"location":"admin/detail/data_storage_input/","text":"DataStorageInput Description: Represents a remote data storage input used for importing sensor data. Stores access credentials and manages linked user accounts and data files. Fields Field Type Description name CharField A unique name for this data storage input. username CharField Username for accessing the external storage. password EncryptedCharField Encrypted password for the storage username. address CharField Network address (IP or hostname) of the storage. owner ForeignKey User who owns this data storage input. Methods __str__() : Returns the name of the storage input. init_ssh_client() : Initializes and returns an SSH client for this storage input. check_users_input() : Sets up users and verifies access for linked devices. setup_users() : Ensures required user accounts exist on the storage system. check_connection() : Attempts to connect and returns connection status and client. check_input(remove_bad=False) : Checks and optionally cleans files on storage.","title":"DataStorageInput"},{"location":"admin/detail/data_storage_input/#datastorageinput","text":"Description: Represents a remote data storage input used for importing sensor data. Stores access credentials and manages linked user accounts and data files.","title":"DataStorageInput"},{"location":"admin/detail/data_storage_input/#fields","text":"Field Type Description name CharField A unique name for this data storage input. username CharField Username for accessing the external storage. password EncryptedCharField Encrypted password for the storage username. address CharField Network address (IP or hostname) of the storage. owner ForeignKey User who owns this data storage input.","title":"Fields"},{"location":"admin/detail/data_storage_input/#methods","text":"__str__() : Returns the name of the storage input. init_ssh_client() : Initializes and returns an SSH client for this storage input. check_users_input() : Sets up users and verifies access for linked devices. setup_users() : Ensures required user accounts exist on the storage system. check_connection() : Attempts to connect and returns connection status and client. check_input(remove_bad=False) : Checks and optionally cleans files on storage.","title":"Methods"},{"location":"admin/detail/projectjob/","text":"ProjectJob Description: Represents a project-level job configuration. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. job_name CharField Name of job. celery_job_name CharField Name of registered Celery task. job_args JSONField Additional arguments. Methods __str__() : Returns the job name. get_job_signature(file_pks) : Generate a job signature for a Celery task given a list of file primary keys.","title":"ProjectJob"},{"location":"admin/detail/projectjob/#projectjob","text":"Description: Represents a project-level job configuration.","title":"ProjectJob"},{"location":"admin/detail/projectjob/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. job_name CharField Name of job. celery_job_name CharField Name of registered Celery task. job_args JSONField Additional arguments.","title":"Fields"},{"location":"admin/detail/projectjob/#methods","text":"__str__() : Returns the job name. get_job_signature(file_pks) : Generate a job signature for a Celery task given a list of file primary keys.","title":"Methods"},{"location":"admin/detail/tarfile/","text":"TarFile Description: Represents a compressed TAR archive containing related data files. Handles cleanup and deletion from local and remote storage. Fields Field Type Description name CharField Filename of the TAR archive (no extension). archived_dt DateTimeField Date and time when this TAR was archived. uploading BooleanField Whether the TAR is currently being uploaded. local_storage BooleanField True if stored locally. archived BooleanField True if stored on remote archive. path CharField File system path to the TAR archive. archive ForeignKey Associated archive for this TAR. Methods __str__() : Returns the name of the TAR file. clean_tar(delete_obj=False, force_delete=False) : Removes the TAR file from local or remote storage and updates its record. Signals pre_remove_tar : Cleans the TAR before deletion; raises error if cleanup fails.","title":"TarFile"},{"location":"admin/detail/tarfile/#tarfile","text":"Description: Represents a compressed TAR archive containing related data files. Handles cleanup and deletion from local and remote storage.","title":"TarFile"},{"location":"admin/detail/tarfile/#fields","text":"Field Type Description name CharField Filename of the TAR archive (no extension). archived_dt DateTimeField Date and time when this TAR was archived. uploading BooleanField Whether the TAR is currently being uploaded. local_storage BooleanField True if stored locally. archived BooleanField True if stored on remote archive. path CharField File system path to the TAR archive. archive ForeignKey Associated archive for this TAR.","title":"Fields"},{"location":"admin/detail/tarfile/#methods","text":"__str__() : Returns the name of the TAR file. clean_tar(delete_obj=False, force_delete=False) : Removes the TAR file from local or remote storage and updates its record.","title":"Methods"},{"location":"admin/detail/tarfile/#signals","text":"pre_remove_tar : Cleans the TAR before deletion; raises error if cleanup fails.","title":"Signals"},{"location":"developers/","text":"Information for developers In this section I provide information about the ways in which the platform is designed to be extended in modular ways. Key documentation for the source code can also be found here.","title":"Overview"},{"location":"developers/#information-for-developers","text":"In this section I provide information about the ways in which the platform is designed to be extended in modular ways. Key documentation for the source code can also be found here.","title":"Information for developers"},{"location":"developers/data_handlers/","text":"Data handlers A data handler is a generic class for handling data from different sensor models in different ways. These are read on startup and used by the file import functionality. Attributes data_types: A list of DataType__name s that the handler supports e.g. [\"wildlifecamera\"] device_models: A list of DeviceType__name s that the handler supports e.g [\"default\",\"BadgerCamera9000\"] safe_formats: A list of save file formats that the handler will deal with e.g. [\".jpg\"] full_name: Name of the data handler. description: HTML general description of the handler. validity_description: HTML description of the validity checks the handler makes. handling_description: HTML description of how the files are handled. post_handling_description: HTML description of post-upload tasks run on the file. Methods to override. handle_file(file, recording_dt=None, extra_data=None, data_type=None) Handle a file after download, performing any post-processing or validation. The method is expected to return: DateTime of when the file was recorded recording_dt . Dict of extra_data including any extra information that might be important that does not fit into the pre-defined database columns. String of data_type__name , in case the handler overrides the default datatype of the deployment, e.g. a camera that takes both motion detected and timelapse images. String of a post-download celery task that will be carried out when the import is complete. This is typically returned by the get_post_download_task method. get_post_download_task(file_extension, first_time=True) Returns the name of a post download celery task. get_valid_files(files, device_label=None) Return a list of files filtered by custom validity checks. Creating a custom data handler Create a new handler class in your handlers directory. from datetime import datetime from typing import Tuple from data_handlers.base_data_handler_class import DataTypeHandler class CustomFileHandler(DataTypeHandler): data_types = [\"mydatatype\"] device_models = [\"mydevice\"] safe_formats = [\".txt\", \".csv\"] full_name = \"My Custom File Handler\" description = \"Handles custom text or CSV files.\" validity_description = \"Files must be .txt or .csv.\" handling_description = \"Parses text files to extract structured data.\" post_handling_description = \"Triggers downstream processing tasks if needed.\" def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: # Call base handler recording_dt, extra_data, data_type, task = super().handle_file(file, recording_dt, extra_data, data_type) # Add custom file parsing logic here with open(file.path, 'r') as f: content = f.read() extra_data['line_count'] = content.count('\\n') return recording_dt, extra_data, data_type, task def get_post_download_task(self, file_extension: str, first_time: bool = True): # Return a task name for post-download processing return \"data_handler_run_analysis\" Register the handler Ensure your handler module is located in the data_handlers/handlers/ directory. The DataTypeHandlerCollection class automatically discovers and instantiates all subclasses of DataTypeHandler . Use in processing The DataTypeHandlerCollection will route files to your custom handler based on data_type and device_model . Example: handler_collection = DataTypeHandlerCollection() handler = handler_collection.get_handler(\"mydatatype\", \"mydevice\") valid_files = handler.get_valid_files(uploaded_files) Example Reference: DefaultImageHandler The DefaultImageHandler class demonstrates subclassing to handle .jpg , .jpeg , and .png image files. It overrides: handle_file to extract EXIF metadata get_post_download_task to specify the task \"data_handler_generate_thumbnails\" You can follow a similar structure to implement your own custom logic. Using post_upload_task_handler as a Wrapper The post_upload_task_handler is a utility function designed to apply a custom task function to a list of DataFile instances after upload. It ensures each file is safely locked during processing and restored to its original state afterward. Purpose To perform post-upload operations (e.g. metadata extraction, file transformation) on a list of files. To isolate errors so one file\u2019s failure doesn\u2019t affect the rest. To safely toggle the do_not_remove flag on each file during processing. Signature post_upload_task_handler( file_pks: List[int], task_function: Callable[[DataFile], Tuple[DataFile | None, List[str] | None]] ) -> None How to Use Define your task function: def my_custom_task(data_file: DataFile) -> Tuple[DataFile, List[str]]: # perform some logic on the data_file data_file.custom_field = \"Updated\" return data_file, [\"custom_field\"] Call post_upload_task_handler with file PKs and your function:: file_pks = [1, 2, 3] post_upload_task_handler(file_pks, my_custom_task)","title":"Data handlers"},{"location":"developers/data_handlers/#data-handlers","text":"A data handler is a generic class for handling data from different sensor models in different ways. These are read on startup and used by the file import functionality.","title":"Data handlers"},{"location":"developers/data_handlers/#attributes","text":"data_types: A list of DataType__name s that the handler supports e.g. [\"wildlifecamera\"] device_models: A list of DeviceType__name s that the handler supports e.g [\"default\",\"BadgerCamera9000\"] safe_formats: A list of save file formats that the handler will deal with e.g. [\".jpg\"] full_name: Name of the data handler. description: HTML general description of the handler. validity_description: HTML description of the validity checks the handler makes. handling_description: HTML description of how the files are handled. post_handling_description: HTML description of post-upload tasks run on the file.","title":"Attributes"},{"location":"developers/data_handlers/#methods-to-override","text":"","title":"Methods to override."},{"location":"developers/data_handlers/#handle_filefile-recording_dtnone-extra_datanone-data_typenone","text":"Handle a file after download, performing any post-processing or validation. The method is expected to return: DateTime of when the file was recorded recording_dt . Dict of extra_data including any extra information that might be important that does not fit into the pre-defined database columns. String of data_type__name , in case the handler overrides the default datatype of the deployment, e.g. a camera that takes both motion detected and timelapse images. String of a post-download celery task that will be carried out when the import is complete. This is typically returned by the get_post_download_task method.","title":"handle_file(file, recording_dt=None, extra_data=None, data_type=None)"},{"location":"developers/data_handlers/#get_post_download_taskfile_extension-first_timetrue","text":"Returns the name of a post download celery task.","title":"get_post_download_task(file_extension, first_time=True)"},{"location":"developers/data_handlers/#get_valid_filesfiles-device_labelnone","text":"Return a list of files filtered by custom validity checks.","title":"get_valid_files(files, device_label=None)"},{"location":"developers/data_handlers/#creating-a-custom-data-handler","text":"Create a new handler class in your handlers directory. from datetime import datetime from typing import Tuple from data_handlers.base_data_handler_class import DataTypeHandler class CustomFileHandler(DataTypeHandler): data_types = [\"mydatatype\"] device_models = [\"mydevice\"] safe_formats = [\".txt\", \".csv\"] full_name = \"My Custom File Handler\" description = \"Handles custom text or CSV files.\" validity_description = \"Files must be .txt or .csv.\" handling_description = \"Parses text files to extract structured data.\" post_handling_description = \"Triggers downstream processing tasks if needed.\" def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: # Call base handler recording_dt, extra_data, data_type, task = super().handle_file(file, recording_dt, extra_data, data_type) # Add custom file parsing logic here with open(file.path, 'r') as f: content = f.read() extra_data['line_count'] = content.count('\\n') return recording_dt, extra_data, data_type, task def get_post_download_task(self, file_extension: str, first_time: bool = True): # Return a task name for post-download processing return \"data_handler_run_analysis\" Register the handler Ensure your handler module is located in the data_handlers/handlers/ directory. The DataTypeHandlerCollection class automatically discovers and instantiates all subclasses of DataTypeHandler . Use in processing The DataTypeHandlerCollection will route files to your custom handler based on data_type and device_model . Example: handler_collection = DataTypeHandlerCollection() handler = handler_collection.get_handler(\"mydatatype\", \"mydevice\") valid_files = handler.get_valid_files(uploaded_files)","title":"Creating a custom data handler"},{"location":"developers/data_handlers/#example-reference-defaultimagehandler","text":"The DefaultImageHandler class demonstrates subclassing to handle .jpg , .jpeg , and .png image files. It overrides: handle_file to extract EXIF metadata get_post_download_task to specify the task \"data_handler_generate_thumbnails\" You can follow a similar structure to implement your own custom logic.","title":"Example Reference: DefaultImageHandler"},{"location":"developers/data_handlers/#using-post_upload_task_handler-as-a-wrapper","text":"The post_upload_task_handler is a utility function designed to apply a custom task function to a list of DataFile instances after upload. It ensures each file is safely locked during processing and restored to its original state afterward.","title":"Using post_upload_task_handler as a Wrapper"},{"location":"developers/data_handlers/#purpose","text":"To perform post-upload operations (e.g. metadata extraction, file transformation) on a list of files. To isolate errors so one file\u2019s failure doesn\u2019t affect the rest. To safely toggle the do_not_remove flag on each file during processing.","title":"Purpose"},{"location":"developers/data_handlers/#signature","text":"post_upload_task_handler( file_pks: List[int], task_function: Callable[[DataFile], Tuple[DataFile | None, List[str] | None]] ) -> None","title":"Signature"},{"location":"developers/data_handlers/#how-to-use","text":"Define your task function: def my_custom_task(data_file: DataFile) -> Tuple[DataFile, List[str]]: # perform some logic on the data_file data_file.custom_field = \"Updated\" return data_file, [\"custom_field\"] Call post_upload_task_handler with file PKs and your function:: file_pks = [1, 2, 3] post_upload_task_handler(file_pks, my_custom_task)","title":"How to Use"},{"location":"developers/generic_jobs/","text":"Creating and Running Custom Jobs You can define your own reusable and generic jobs by using the utility functions provided in job_handling_functions.py . These are designed to simplify registering, managing, and triggering asynchronous Celery tasks. Step 1: Register Your Task Using @register_job Use the @register_job decorator to register your function as a job. This makes the task discoverable and executable via the job framework. from data_models.job_handling_functions import register_job from celery import shared_task @shared_task(name=\"my_custom_task\") @register_job( name=\"My Custom Task\", task_name=\"my_custom_task\", task_data_type=\"datafile\", # or \"deployment\", etc. task_admin_only=False, default_args={\"some_arg\": \"default_value\"} ) def my_custom_task(datafile_pks: list[int], some_arg: str = \"\", **kwargs): # Your logic here pass Step 2: Trigger the Job Programmatically Use start_job_from_name to run the task with a list of object primary keys and any arguments your job requires. from data_models.job_handling_functions import start_job_from_name success, message, status_code = start_job_from_name( job_name=\"my_custom_task\", obj_type=\"datafile\", obj_pks=[1, 2, 3], job_args={\"some_arg\": \"my value\"}, user_pk=5 # optional ) Optional: Build a Task Signature Only If you want to build the task without executing it immediately, use get_job_from_name : from data_models.job_handling_functions import get_job_from_name signature = get_job_from_name( job_name=\"my_custom_task\", obj_type=\"datafile\", obj_pks=[1, 2, 3], job_args={\"some_arg\": \"value\"}, user_pk=5 ) signature.apply_async() # Execute it later This method is implemented by the ProjectJob objects to execute automated tasks associated with Projects. Default Arguments The default_args dictionary in @register_job defines fallback values if a specific argument is not passed during execution. You can use this to ensure tasks have safe defaults. Example: Modifying DataFiles Here's a simple example that sets a flag on DataFiles: @shared_task(name=\"flag_custom\") @register_job(\"Flag custom\", \"flag_custom\", \"datafile\", True, default_args={\"custom_flag\": True}) def flag_custom(datafile_pks: list[int], custom_flag: bool = False, **kwargs): from .models import DataFile file_objs = DataFile.objects.filter(pk__in=datafile_pks) file_objs.update(my_custom_flag=custom_flag) To run it: start_job_from_name( \"flag_custom\", \"datafile\", [101, 102], {\"custom_flag\": True}, user_pk=1 ) Notes task_name must be unique across the system. Only registered jobs are callable via start_job_from_name . Admin-only tasks can be restricted by setting task_admin_only=True . Max items per job can be controlled with the max_items parameter.","title":"Generic jobs"},{"location":"developers/generic_jobs/#creating-and-running-custom-jobs","text":"You can define your own reusable and generic jobs by using the utility functions provided in job_handling_functions.py . These are designed to simplify registering, managing, and triggering asynchronous Celery tasks.","title":"Creating and Running Custom Jobs"},{"location":"developers/generic_jobs/#step-1-register-your-task-using-register_job","text":"Use the @register_job decorator to register your function as a job. This makes the task discoverable and executable via the job framework. from data_models.job_handling_functions import register_job from celery import shared_task @shared_task(name=\"my_custom_task\") @register_job( name=\"My Custom Task\", task_name=\"my_custom_task\", task_data_type=\"datafile\", # or \"deployment\", etc. task_admin_only=False, default_args={\"some_arg\": \"default_value\"} ) def my_custom_task(datafile_pks: list[int], some_arg: str = \"\", **kwargs): # Your logic here pass","title":"Step 1: Register Your Task Using @register_job"},{"location":"developers/generic_jobs/#step-2-trigger-the-job-programmatically","text":"Use start_job_from_name to run the task with a list of object primary keys and any arguments your job requires. from data_models.job_handling_functions import start_job_from_name success, message, status_code = start_job_from_name( job_name=\"my_custom_task\", obj_type=\"datafile\", obj_pks=[1, 2, 3], job_args={\"some_arg\": \"my value\"}, user_pk=5 # optional )","title":"Step 2: Trigger the Job Programmatically"},{"location":"developers/generic_jobs/#optional-build-a-task-signature-only","text":"If you want to build the task without executing it immediately, use get_job_from_name : from data_models.job_handling_functions import get_job_from_name signature = get_job_from_name( job_name=\"my_custom_task\", obj_type=\"datafile\", obj_pks=[1, 2, 3], job_args={\"some_arg\": \"value\"}, user_pk=5 ) signature.apply_async() # Execute it later This method is implemented by the ProjectJob objects to execute automated tasks associated with Projects.","title":"Optional: Build a Task Signature Only"},{"location":"developers/generic_jobs/#default-arguments","text":"The default_args dictionary in @register_job defines fallback values if a specific argument is not passed during execution. You can use this to ensure tasks have safe defaults.","title":"Default Arguments"},{"location":"developers/generic_jobs/#example-modifying-datafiles","text":"Here's a simple example that sets a flag on DataFiles: @shared_task(name=\"flag_custom\") @register_job(\"Flag custom\", \"flag_custom\", \"datafile\", True, default_args={\"custom_flag\": True}) def flag_custom(datafile_pks: list[int], custom_flag: bool = False, **kwargs): from .models import DataFile file_objs = DataFile.objects.filter(pk__in=datafile_pks) file_objs.update(my_custom_flag=custom_flag) To run it: start_job_from_name( \"flag_custom\", \"datafile\", [101, 102], {\"custom_flag\": True}, user_pk=1 )","title":"Example: Modifying DataFiles"},{"location":"developers/generic_jobs/#notes","text":"task_name must be unique across the system. Only registered jobs are callable via start_job_from_name . Admin-only tasks can be restricted by setting task_admin_only=True . Max items per job can be controlled with the max_items parameter.","title":"Notes"},{"location":"reference/SUMMARY/","text":"ai_integration tasks archiving bagit_functions functions models tar_functions tasks camtrap_dp_export metadata_functions querysets serializers viewsets data_handlers base_data_handler_class functions post_upload_task_handler serializers tasks viewsets data_models file_handling_functions filtersets general_functions job_handling_functions metadata_functions models plotting_functions rules serializers signals tasks validators viewsets data_packages create_zip_functions models serializers tasks viewsets external_storage_import filtersets models serializers tasks viewsets observation_editor filtersets GBIF_functions metadata_functions models rules serializers tasks viewsets user_management filtersets models rules serializers signals views viewsets utils api_renderer email filtersets general models paginators perm_functions querysets rules serializers ssh_client task_functions test_functions validators views viewsets","title":"SUMMARY"},{"location":"reference/ai_integration/","text":"This module provides functionality to passing data to and from AI models running in other containers in the network.","title":"ai_integration"},{"location":"reference/ai_integration/tasks/","text":"do_ultra_inference(datafile_pks, model_name, target_labels=None, chunksize=500, chunksize2=100, exclude_done=False, parallel=False, **kwargs) Runs ultralytic AI model inference on batches of DataFiles. Parameters: datafile_pks ( Union [ int , List [ int ]] ) \u2013 A single primary key or a list of primary keys of DataFile objects. model_name ( str ) \u2013 The name of the ultralytics model to use. target_labels ( Optional [ Union [ str , List [ str ]]] , default: None ) \u2013 Optional; a label or list of labels to target in inference. chunksize ( int , default: 500 ) \u2013 The number of files to process in the outer batch. chunksize2 ( int , default: 100 ) \u2013 The number of files per job batch. exclude_done ( bool , default: False ) \u2013 If True, skips files which already have observations for this model. parallel ( bool , default: False ) \u2013 If True, runs jobs in parallel; otherwise, chains jobs. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Returns: None \u2013 None Source code in ai_integration\\tasks.py @app.task(name=\"do_ultra_inference\") @register_job(\"Do Ultralytic AI model inference\", \"do_ultra_inference\", \"datafile\", True, default_args={\"model_name\": \"yolov8s\"}) def do_ultra_inference( datafile_pks: Union[int, List[int]], model_name: str, target_labels: Optional[Union[str, List[str]]] = None, chunksize: int = 500, chunksize2: int = 100, exclude_done: bool = False, parallel: bool = False, **kwargs: Any ) -> None: \"\"\" Runs ultralytic AI model inference on batches of DataFiles. Args: datafile_pks: A single primary key or a list of primary keys of DataFile objects. model_name: The name of the ultralytics model to use. target_labels: Optional; a label or list of labels to target in inference. chunksize: The number of files to process in the outer batch. chunksize2: The number of files per job batch. exclude_done: If True, skips files which already have observations for this model. parallel: If True, runs jobs in parallel; otherwise, chains jobs. **kwargs: Additional keyword arguments. Returns: None \"\"\" valid_formats = [\".jpg\", \".jpeg\", \".png\"] # should be setting from env target_queue_name = settings.ULTRALYTICS_QUEUE # should be setting from env queue_names = [x[0]['name'] for x in ai_app.control.inspect().active_queues().values()] if target_queue_name not in queue_names: logger.info(f\"No {target_queue_name} queue available\") return if type(datafile_pks) is not list: datafile_pks = [datafile_pks] if target_labels is not None and type(target_labels) is not list: target_labels = [target_labels] file_objs: QuerySet = DataFile.objects.filter( pk__in=datafile_pks, file_format__lower__in=valid_formats) if exclude_done or not parallel: file_objs = file_objs.exclude(observations__source=model_name) datafile_pks = list(file_objs.values_list('pk', flat=True)) if len(datafile_pks) == 0: logger.info(\"No files to analyse\") return file_pks_chunks: List[List[int]] = [datafile_pks[i:i + chunksize] for i in range(0, len(datafile_pks), chunksize)] for i, file_pks_chunk in enumerate(file_pks_chunks): file_objs_chunk = file_objs.filter(pk__in=file_pks_chunk).full_paths() file_pks_job_chunks = [file_pks_chunk[i:i + chunksize2] for i in range(0, len(file_pks_chunk), chunksize2)] all_tasks = [] for file_pk_job_chunk in file_pks_job_chunks: job_qs = file_objs_chunk.filter(pk__in=file_pk_job_chunk) file_paths = list( job_qs.values_list(\"full_path\", flat=True)) all_tasks.append(ai_app.signature('AnalysisTask', [ file_paths, model_name, target_labels], queue=\"ultralytics\", immutable=True)) task_chord = chord(all_tasks, handle_ultra_results.s( target_labels).set(queue=\"main_worker\")) if not parallel: task_chain = chain(task_chord, do_ultra_inference.si(datafile_pks, model_name, target_labels, chunksize, chunksize2, True, False).set(queue=\"main_worker\")) logger.info(task_chain) task_chain.apply_async() return else: task_chord.apply_async() handle_ultra_results(all_results, target_labels=None) Processes results from ultralytics inference, creates Observation objects, and links them to DataFiles. Parameters: all_results ( List [ Dict [ str , Any ]] ) \u2013 List of dictionaries with inference results. target_labels ( Optional [ Union [ str , List [ str ]]] , default: None ) \u2013 Optional; label or list of labels to target in result processing. Returns: None \u2013 None Source code in ai_integration\\tasks.py @app.task() def handle_ultra_results( all_results: List[Dict[str, Any]], target_labels: Optional[Union[str, List[str]]] = None ) -> None: \"\"\" Processes results from ultralytics inference, creates Observation objects, and links them to DataFiles. Args: all_results: List of dictionaries with inference results. target_labels: Optional; label or list of labels to target in result processing. Returns: None \"\"\" through_class = Observation.data_files.through if target_labels is not None and type(target_labels) is not list: target_labels = [target_labels] objs_to_create: List[Observation] = [] file_objs_pks: List[int] = [] file_objs_human_pks: List[int] = [] for results in all_results: source = results.get('source') for file_name, file_results in results.get('files').items(): file_obj = DataFile.objects.get(file_name=file_name) num_results = 0 if len(file_results) > 0: for result in file_results: prediction = result.get('prediction') if target_labels is None or prediction in target_labels: num_results += 1 bounding_box = result.get(\"bbox\") extra_data: Dict[str, Any] = {} if bounding_box is not None: bbox_keys = [\"x1\", \"y1\", \"x2\", \"y2\"] bounding_box = {k: v for k, v in zip( bbox_keys, bounding_box)} confidence = result.get('confidence') if result.get('orig_shape') is not None: extra_data[\"orig_shape\"] = result.get('orig_shape') taxon_obj = Taxon.objects.get_or_create( species_name=prediction)[0] new_observation_object = Observation( label=f\"{prediction}_{file_obj.file_name}\", taxon=taxon_obj, obs_dt=file_obj.recording_dt, bounding_box=bounding_box, confidence=confidence, extra_data=extra_data, source=source ) objs_to_create.append(new_observation_object) file_objs_pks.append(file_obj.pk) if taxon_obj.taxon_code == settings.HUMAN_TAXON_CODE: file_objs_human_pks.append(file_obj.pk) if len(file_results) == 0 or num_results == 0: taxon_obj = Taxon.objects.get_or_create( species_name=\"No detection\")[0] new_observation_object = Observation( label=f\"No_dectection_{file_obj.file_name}\", taxon=taxon_obj, obs_dt=file_obj.recording_dt, extra_data={}, source=source ) objs_to_create.append(new_observation_object) file_objs_pks.append(file_obj.pk) new_observations = Observation.objects.bulk_create( objs_to_create, batch_size=500) new_obervation_pks = [x.pk for x in new_observations] all_through_objs = [] for obs_pk, file_pk in zip(new_obervation_pks, file_objs_pks): new_through_obj = through_class.objects.create( observation_id=obs_pk, datafile_id=file_pk) all_through_objs.append(new_through_obj) through_class.objects.bulk_create( all_through_objs, batch_size=500, ignore_conflicts=True) logger.info(f\"Created {len(new_observations)} observations\") # Update datafiles if human is present DataFile.objects.filter(pk__in=file_objs_human_pks).update( has_human=True, modified_on=timezone.now())","title":"tasks"},{"location":"reference/ai_integration/tasks/#ai_integration.tasks.do_ultra_inference","text":"Runs ultralytic AI model inference on batches of DataFiles. Parameters: datafile_pks ( Union [ int , List [ int ]] ) \u2013 A single primary key or a list of primary keys of DataFile objects. model_name ( str ) \u2013 The name of the ultralytics model to use. target_labels ( Optional [ Union [ str , List [ str ]]] , default: None ) \u2013 Optional; a label or list of labels to target in inference. chunksize ( int , default: 500 ) \u2013 The number of files to process in the outer batch. chunksize2 ( int , default: 100 ) \u2013 The number of files per job batch. exclude_done ( bool , default: False ) \u2013 If True, skips files which already have observations for this model. parallel ( bool , default: False ) \u2013 If True, runs jobs in parallel; otherwise, chains jobs. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Returns: None \u2013 None Source code in ai_integration\\tasks.py @app.task(name=\"do_ultra_inference\") @register_job(\"Do Ultralytic AI model inference\", \"do_ultra_inference\", \"datafile\", True, default_args={\"model_name\": \"yolov8s\"}) def do_ultra_inference( datafile_pks: Union[int, List[int]], model_name: str, target_labels: Optional[Union[str, List[str]]] = None, chunksize: int = 500, chunksize2: int = 100, exclude_done: bool = False, parallel: bool = False, **kwargs: Any ) -> None: \"\"\" Runs ultralytic AI model inference on batches of DataFiles. Args: datafile_pks: A single primary key or a list of primary keys of DataFile objects. model_name: The name of the ultralytics model to use. target_labels: Optional; a label or list of labels to target in inference. chunksize: The number of files to process in the outer batch. chunksize2: The number of files per job batch. exclude_done: If True, skips files which already have observations for this model. parallel: If True, runs jobs in parallel; otherwise, chains jobs. **kwargs: Additional keyword arguments. Returns: None \"\"\" valid_formats = [\".jpg\", \".jpeg\", \".png\"] # should be setting from env target_queue_name = settings.ULTRALYTICS_QUEUE # should be setting from env queue_names = [x[0]['name'] for x in ai_app.control.inspect().active_queues().values()] if target_queue_name not in queue_names: logger.info(f\"No {target_queue_name} queue available\") return if type(datafile_pks) is not list: datafile_pks = [datafile_pks] if target_labels is not None and type(target_labels) is not list: target_labels = [target_labels] file_objs: QuerySet = DataFile.objects.filter( pk__in=datafile_pks, file_format__lower__in=valid_formats) if exclude_done or not parallel: file_objs = file_objs.exclude(observations__source=model_name) datafile_pks = list(file_objs.values_list('pk', flat=True)) if len(datafile_pks) == 0: logger.info(\"No files to analyse\") return file_pks_chunks: List[List[int]] = [datafile_pks[i:i + chunksize] for i in range(0, len(datafile_pks), chunksize)] for i, file_pks_chunk in enumerate(file_pks_chunks): file_objs_chunk = file_objs.filter(pk__in=file_pks_chunk).full_paths() file_pks_job_chunks = [file_pks_chunk[i:i + chunksize2] for i in range(0, len(file_pks_chunk), chunksize2)] all_tasks = [] for file_pk_job_chunk in file_pks_job_chunks: job_qs = file_objs_chunk.filter(pk__in=file_pk_job_chunk) file_paths = list( job_qs.values_list(\"full_path\", flat=True)) all_tasks.append(ai_app.signature('AnalysisTask', [ file_paths, model_name, target_labels], queue=\"ultralytics\", immutable=True)) task_chord = chord(all_tasks, handle_ultra_results.s( target_labels).set(queue=\"main_worker\")) if not parallel: task_chain = chain(task_chord, do_ultra_inference.si(datafile_pks, model_name, target_labels, chunksize, chunksize2, True, False).set(queue=\"main_worker\")) logger.info(task_chain) task_chain.apply_async() return else: task_chord.apply_async()","title":"do_ultra_inference"},{"location":"reference/ai_integration/tasks/#ai_integration.tasks.handle_ultra_results","text":"Processes results from ultralytics inference, creates Observation objects, and links them to DataFiles. Parameters: all_results ( List [ Dict [ str , Any ]] ) \u2013 List of dictionaries with inference results. target_labels ( Optional [ Union [ str , List [ str ]]] , default: None ) \u2013 Optional; label or list of labels to target in result processing. Returns: None \u2013 None Source code in ai_integration\\tasks.py @app.task() def handle_ultra_results( all_results: List[Dict[str, Any]], target_labels: Optional[Union[str, List[str]]] = None ) -> None: \"\"\" Processes results from ultralytics inference, creates Observation objects, and links them to DataFiles. Args: all_results: List of dictionaries with inference results. target_labels: Optional; label or list of labels to target in result processing. Returns: None \"\"\" through_class = Observation.data_files.through if target_labels is not None and type(target_labels) is not list: target_labels = [target_labels] objs_to_create: List[Observation] = [] file_objs_pks: List[int] = [] file_objs_human_pks: List[int] = [] for results in all_results: source = results.get('source') for file_name, file_results in results.get('files').items(): file_obj = DataFile.objects.get(file_name=file_name) num_results = 0 if len(file_results) > 0: for result in file_results: prediction = result.get('prediction') if target_labels is None or prediction in target_labels: num_results += 1 bounding_box = result.get(\"bbox\") extra_data: Dict[str, Any] = {} if bounding_box is not None: bbox_keys = [\"x1\", \"y1\", \"x2\", \"y2\"] bounding_box = {k: v for k, v in zip( bbox_keys, bounding_box)} confidence = result.get('confidence') if result.get('orig_shape') is not None: extra_data[\"orig_shape\"] = result.get('orig_shape') taxon_obj = Taxon.objects.get_or_create( species_name=prediction)[0] new_observation_object = Observation( label=f\"{prediction}_{file_obj.file_name}\", taxon=taxon_obj, obs_dt=file_obj.recording_dt, bounding_box=bounding_box, confidence=confidence, extra_data=extra_data, source=source ) objs_to_create.append(new_observation_object) file_objs_pks.append(file_obj.pk) if taxon_obj.taxon_code == settings.HUMAN_TAXON_CODE: file_objs_human_pks.append(file_obj.pk) if len(file_results) == 0 or num_results == 0: taxon_obj = Taxon.objects.get_or_create( species_name=\"No detection\")[0] new_observation_object = Observation( label=f\"No_dectection_{file_obj.file_name}\", taxon=taxon_obj, obs_dt=file_obj.recording_dt, extra_data={}, source=source ) objs_to_create.append(new_observation_object) file_objs_pks.append(file_obj.pk) new_observations = Observation.objects.bulk_create( objs_to_create, batch_size=500) new_obervation_pks = [x.pk for x in new_observations] all_through_objs = [] for obs_pk, file_pk in zip(new_obervation_pks, file_objs_pks): new_through_obj = through_class.objects.create( observation_id=obs_pk, datafile_id=file_pk) all_through_objs.append(new_through_obj) through_class.objects.bulk_create( all_through_objs, batch_size=500, ignore_conflicts=True) logger.info(f\"Created {len(new_observations)} observations\") # Update datafiles if human is present DataFile.objects.filter(pk__in=file_objs_human_pks).update( has_human=True, modified_on=timezone.now())","title":"handle_ultra_results"},{"location":"reference/archiving/","text":"This module provides functionality for pushing data from the sensor portal to cold storage.","title":"archiving"},{"location":"reference/archiving/bagit_functions/","text":"bag_info_from_files(file_objs, output_path) Generate the necessary BagIt metadata files for a set of data files, writing them to the specified output directory. This function creates 'bagit.txt', 'manifest-md5.txt', and 'tagmanifest-md5.txt' directly in the output directory. The files are generated based on the provided file_objs (which should be a queryset or collection of DataFile instances). Each generated file's full path is returned in a list. Parameters: file_objs ( DataFile ) \u2013 Queryset or iterable of DataFile objects representing files to be included in the bag. output_path ( str ) \u2013 Directory path where the BagIt files should be created. Returns: List [ str ] \u2013 List[str]: List of absolute paths to the generated BagIt files. Source code in archiving\\bagit_functions.py def bag_info_from_files(file_objs: DataFile, output_path: str) -> List[str]: \"\"\" Generate the necessary BagIt metadata files for a set of data files, writing them to the specified output directory. This function creates 'bagit.txt', 'manifest-md5.txt', and 'tagmanifest-md5.txt' directly in the output directory. The files are generated based on the provided `file_objs` (which should be a queryset or collection of DataFile instances). Each generated file's full path is returned in a list. Args: file_objs (DataFile): Queryset or iterable of DataFile objects representing files to be included in the bag. output_path (str): Directory path where the BagIt files should be created. Returns: List[str]: List of absolute paths to the generated BagIt files. \"\"\" os.makedirs(output_path, exist_ok=True) # Write bagit.txt bagit_txt_lines = [ \"BagIt-Version: 0.97\\n\", \"Tag-File-Character-Encoding: UTF-8\\n\" ] bag_path = os.path.join(output_path, \"bagit.txt\") with open(bag_path, \"w\") as f: f.writelines(bagit_txt_lines) # Write manifest-md5.txt file_objs = file_objs.full_paths() all_full_paths = file_objs.values_list(\"full_path\", flat=True) all_relative_paths = file_objs.values_list(\"relative_path\", flat=True) manifest_lines = [ f\"{get_md5(full_path)} {os.path.join('data', relative_path)}\\n\" for full_path, relative_path in zip(all_full_paths, all_relative_paths) ] manifest_path = os.path.join(output_path, \"manifest-md5.txt\") with open(manifest_path, \"w\") as f: f.writelines(manifest_lines) # Write tagmanifest-md5.txt all_paths = [bag_path, manifest_path] tag_manifest_lines = [ f\"{get_md5(path)} {os.path.basename(path)}\\n\" for path in all_paths ] tag_manifest_path = os.path.join(output_path, \"tagmanifest-md5.txt\") with open(tag_manifest_path, \"w\") as f: f.writelines(tag_manifest_lines) all_paths.append(tag_manifest_path) return all_paths","title":"bagit_functions"},{"location":"reference/archiving/bagit_functions/#archiving.bagit_functions.bag_info_from_files","text":"Generate the necessary BagIt metadata files for a set of data files, writing them to the specified output directory. This function creates 'bagit.txt', 'manifest-md5.txt', and 'tagmanifest-md5.txt' directly in the output directory. The files are generated based on the provided file_objs (which should be a queryset or collection of DataFile instances). Each generated file's full path is returned in a list. Parameters: file_objs ( DataFile ) \u2013 Queryset or iterable of DataFile objects representing files to be included in the bag. output_path ( str ) \u2013 Directory path where the BagIt files should be created. Returns: List [ str ] \u2013 List[str]: List of absolute paths to the generated BagIt files. Source code in archiving\\bagit_functions.py def bag_info_from_files(file_objs: DataFile, output_path: str) -> List[str]: \"\"\" Generate the necessary BagIt metadata files for a set of data files, writing them to the specified output directory. This function creates 'bagit.txt', 'manifest-md5.txt', and 'tagmanifest-md5.txt' directly in the output directory. The files are generated based on the provided `file_objs` (which should be a queryset or collection of DataFile instances). Each generated file's full path is returned in a list. Args: file_objs (DataFile): Queryset or iterable of DataFile objects representing files to be included in the bag. output_path (str): Directory path where the BagIt files should be created. Returns: List[str]: List of absolute paths to the generated BagIt files. \"\"\" os.makedirs(output_path, exist_ok=True) # Write bagit.txt bagit_txt_lines = [ \"BagIt-Version: 0.97\\n\", \"Tag-File-Character-Encoding: UTF-8\\n\" ] bag_path = os.path.join(output_path, \"bagit.txt\") with open(bag_path, \"w\") as f: f.writelines(bagit_txt_lines) # Write manifest-md5.txt file_objs = file_objs.full_paths() all_full_paths = file_objs.values_list(\"full_path\", flat=True) all_relative_paths = file_objs.values_list(\"relative_path\", flat=True) manifest_lines = [ f\"{get_md5(full_path)} {os.path.join('data', relative_path)}\\n\" for full_path, relative_path in zip(all_full_paths, all_relative_paths) ] manifest_path = os.path.join(output_path, \"manifest-md5.txt\") with open(manifest_path, \"w\") as f: f.writelines(manifest_lines) # Write tagmanifest-md5.txt all_paths = [bag_path, manifest_path] tag_manifest_lines = [ f\"{get_md5(path)} {os.path.basename(path)}\\n\" for path in all_paths ] tag_manifest_path = os.path.join(output_path, \"tagmanifest-md5.txt\") with open(tag_manifest_path, \"w\") as f: f.writelines(tag_manifest_lines) all_paths.append(tag_manifest_path) return all_paths","title":"bag_info_from_files"},{"location":"reference/archiving/functions/","text":"check_archive_projects(archive) Checks all projects linked to the given archive for data files that need to be archived. For each unique project and device type combination, determines if there is enough data to create a tar archive, and schedules archiving tasks if appropriate. Parameters: archive ( Archive ) \u2013 The Archive instance for which to check projects. Source code in archiving\\functions.py def check_archive_projects(archive: Archive) -> None: \"\"\" Checks all projects linked to the given archive for data files that need to be archived. For each unique project and device type combination, determines if there is enough data to create a tar archive, and schedules archiving tasks if appropriate. Args: archive (Archive): The Archive instance for which to check projects. \"\"\" from .tasks import check_archive_upload_task, create_tar_files_task # Get all projects linked to this archive all_project_combos = list(archive.linked_projects.values_list( \"deployments__combo_project\", flat=True).distinct()) all_tasks = [] for project_combo in all_project_combos: logger.info(f\"Check {project_combo} for archiving\") all_file_objs = DataFile.objects.filter( deployment__combo_project=project_combo, tar_file__isnull=True) if not all_file_objs.exists(): logger.info(f\"Check {project_combo} for archiving: No files\") continue device_types = list(all_file_objs.values_list( \"deployment__device__type\", flat=True).distinct()) for device_type in device_types: logger.info( f\"Check {project_combo} for archiving: check {device_type}\") # Get datafiles for this project, sensor type file_objs = all_file_objs.filter( deployment__device__type=device_type) if not file_objs.exists(): logger.info( f\"Check {project_combo} for archiving: check {device_type}: No files\") continue # check files for archiving total_file_size = file_objs.file_size_unit(\"GB\") if total_file_size > settings.MIN_ARCHIVE_SIZE_GB: logger.info( f\"Check {project_combo} for archiving: check {device_type}: Sufficient files\") file_pks = list(file_objs.values_list('pk', flat=True)) tar_task = create_tar_files_task.si(file_pks, archive.pk) all_tasks.append(tar_task) else: logger.info( f\"Check {project_combo} for archiving: check {device_type}: Insufficient files\") if len(all_tasks) > 0: logger.info(\"Submitting archiving jobs\") task_group = group(all_tasks) task_chord = chord( task_group, check_archive_upload_task.si(archive.pk)) task_chord.apply_async() check_archive_upload(archive) Attempts to upload any tar files associated with the archive that have not yet been uploaded. Handles connection to remote storage and updates file/archive status accordingly. Parameters: archive ( Archive ) \u2013 The Archive instance to process uploads for. Source code in archiving\\functions.py def check_archive_upload(archive: Archive) -> None: \"\"\" Attempts to upload any tar files associated with the archive that have not yet been uploaded. Handles connection to remote storage and updates file/archive status accordingly. Args: archive (Archive): The Archive instance to process uploads for. \"\"\" tars_to_upload = archive.tar_files.filter(archived=False, uploading=False) archive_ssh = archive.init_ssh_client() connect_ftp_success = archive_ssh.connect_to_ftp() connect_scp_success = archive_ssh.connect_to_scp() if not connect_scp_success or not connect_ftp_success: return for tar_obj in tars_to_upload: if tar_obj.uploading or tar_obj.archived: continue logger.info(f\"{tar_obj.name} uploading\") tar_obj.uploading = True tar_obj.save() tar_full_name = tar_obj.name + \".tar.gz\" upload_path = os.path.join(archive.root_folder, os.path.relpath(tar_obj.path, os.path.join(settings.FILE_STORAGE_ROOT, \"archiving\"))) full_tar_upload_path = os.path.join( upload_path, tar_full_name) full_tar_local_path = os.path.join( settings.FILE_STORAGE_ROOT, tar_obj.path, tar_full_name) success = False try: archive_ssh.mkdir_p(upload_path) archive_ssh.scp_c.put(full_tar_local_path, full_tar_upload_path, preserve_times=True) success = True except SCPException as e: logger.info(\"SCP error:\") logger.info(repr(e)) traceback.print_exc() except Exception as e: logger.info(repr(e)) traceback.print_exc() tar_obj.uploading = False if success: logger.info(f\"{tar_obj.name} uploading succesful\") tar_obj.clean_tar() tar_obj.archived = True tar_obj.path = upload_path tar_obj.files.update(archived=True) else: logger.info(f\"{tar_obj.name} uploading failed\") tar_obj.save() archive_ssh.close_connection()","title":"functions"},{"location":"reference/archiving/functions/#archiving.functions.check_archive_projects","text":"Checks all projects linked to the given archive for data files that need to be archived. For each unique project and device type combination, determines if there is enough data to create a tar archive, and schedules archiving tasks if appropriate. Parameters: archive ( Archive ) \u2013 The Archive instance for which to check projects. Source code in archiving\\functions.py def check_archive_projects(archive: Archive) -> None: \"\"\" Checks all projects linked to the given archive for data files that need to be archived. For each unique project and device type combination, determines if there is enough data to create a tar archive, and schedules archiving tasks if appropriate. Args: archive (Archive): The Archive instance for which to check projects. \"\"\" from .tasks import check_archive_upload_task, create_tar_files_task # Get all projects linked to this archive all_project_combos = list(archive.linked_projects.values_list( \"deployments__combo_project\", flat=True).distinct()) all_tasks = [] for project_combo in all_project_combos: logger.info(f\"Check {project_combo} for archiving\") all_file_objs = DataFile.objects.filter( deployment__combo_project=project_combo, tar_file__isnull=True) if not all_file_objs.exists(): logger.info(f\"Check {project_combo} for archiving: No files\") continue device_types = list(all_file_objs.values_list( \"deployment__device__type\", flat=True).distinct()) for device_type in device_types: logger.info( f\"Check {project_combo} for archiving: check {device_type}\") # Get datafiles for this project, sensor type file_objs = all_file_objs.filter( deployment__device__type=device_type) if not file_objs.exists(): logger.info( f\"Check {project_combo} for archiving: check {device_type}: No files\") continue # check files for archiving total_file_size = file_objs.file_size_unit(\"GB\") if total_file_size > settings.MIN_ARCHIVE_SIZE_GB: logger.info( f\"Check {project_combo} for archiving: check {device_type}: Sufficient files\") file_pks = list(file_objs.values_list('pk', flat=True)) tar_task = create_tar_files_task.si(file_pks, archive.pk) all_tasks.append(tar_task) else: logger.info( f\"Check {project_combo} for archiving: check {device_type}: Insufficient files\") if len(all_tasks) > 0: logger.info(\"Submitting archiving jobs\") task_group = group(all_tasks) task_chord = chord( task_group, check_archive_upload_task.si(archive.pk)) task_chord.apply_async()","title":"check_archive_projects"},{"location":"reference/archiving/functions/#archiving.functions.check_archive_upload","text":"Attempts to upload any tar files associated with the archive that have not yet been uploaded. Handles connection to remote storage and updates file/archive status accordingly. Parameters: archive ( Archive ) \u2013 The Archive instance to process uploads for. Source code in archiving\\functions.py def check_archive_upload(archive: Archive) -> None: \"\"\" Attempts to upload any tar files associated with the archive that have not yet been uploaded. Handles connection to remote storage and updates file/archive status accordingly. Args: archive (Archive): The Archive instance to process uploads for. \"\"\" tars_to_upload = archive.tar_files.filter(archived=False, uploading=False) archive_ssh = archive.init_ssh_client() connect_ftp_success = archive_ssh.connect_to_ftp() connect_scp_success = archive_ssh.connect_to_scp() if not connect_scp_success or not connect_ftp_success: return for tar_obj in tars_to_upload: if tar_obj.uploading or tar_obj.archived: continue logger.info(f\"{tar_obj.name} uploading\") tar_obj.uploading = True tar_obj.save() tar_full_name = tar_obj.name + \".tar.gz\" upload_path = os.path.join(archive.root_folder, os.path.relpath(tar_obj.path, os.path.join(settings.FILE_STORAGE_ROOT, \"archiving\"))) full_tar_upload_path = os.path.join( upload_path, tar_full_name) full_tar_local_path = os.path.join( settings.FILE_STORAGE_ROOT, tar_obj.path, tar_full_name) success = False try: archive_ssh.mkdir_p(upload_path) archive_ssh.scp_c.put(full_tar_local_path, full_tar_upload_path, preserve_times=True) success = True except SCPException as e: logger.info(\"SCP error:\") logger.info(repr(e)) traceback.print_exc() except Exception as e: logger.info(repr(e)) traceback.print_exc() tar_obj.uploading = False if success: logger.info(f\"{tar_obj.name} uploading succesful\") tar_obj.clean_tar() tar_obj.archived = True tar_obj.path = upload_path tar_obj.files.update(archived=True) else: logger.info(f\"{tar_obj.name} uploading failed\") tar_obj.save() archive_ssh.close_connection()","title":"check_archive_upload"},{"location":"reference/archiving/models/","text":"Archive Bases: BaseModel Source code in archiving\\models.py class Archive(BaseModel): name = models.CharField( max_length=200, help_text=\"A human-readable name for this archive.\" ) username = models.CharField( max_length=50, unique=True, help_text=\"Username for SSH login to the archive server.\" ) password = EncryptedCharField( max_length=128, help_text=\"Password for SSH login to the archive server. Stored encrypted.\" ) address = models.CharField( max_length=100, unique=True, help_text=\"Network address of the archive server.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_archives\", on_delete=models.SET_NULL, null=True, help_text=\"User who owns this archive.\" ) root_folder = models.CharField( max_length=100, unique=True, help_text=\"Root folder path on the archive server.\" ) def __str__(self) -> str: \"\"\"Return the name of the archive.\"\"\" return self.name def init_ssh_client(self) -> SSH_client: \"\"\"Initialize an SSH client for this archive.\"\"\" return SSH_client(self.username, self.password, self.address, 22) def check_projects(self) -> None: \"\"\"Check and update archive projects.\"\"\" from .functions import check_archive_projects check_archive_projects(self) def check_upload(self) -> None: \"\"\"Check and update archive uploads.\"\"\" from .functions import check_archive_upload check_archive_upload(self) __str__() Return the name of the archive. Source code in archiving\\models.py def __str__(self) -> str: \"\"\"Return the name of the archive.\"\"\" return self.name check_projects() Check and update archive projects. Source code in archiving\\models.py def check_projects(self) -> None: \"\"\"Check and update archive projects.\"\"\" from .functions import check_archive_projects check_archive_projects(self) check_upload() Check and update archive uploads. Source code in archiving\\models.py def check_upload(self) -> None: \"\"\"Check and update archive uploads.\"\"\" from .functions import check_archive_upload check_archive_upload(self) init_ssh_client() Initialize an SSH client for this archive. Source code in archiving\\models.py def init_ssh_client(self) -> SSH_client: \"\"\"Initialize an SSH client for this archive.\"\"\" return SSH_client(self.username, self.password, self.address, 22) TarFile Bases: BaseModel Source code in archiving\\models.py class TarFile(BaseModel): name = models.CharField( max_length=200, help_text=\"Filename of the TAR archive (without extension).\" ) archived_dt = models.DateTimeField( default=djtimezone.now, help_text=\"Datetime this TAR was archived.\" ) uploading = models.BooleanField( default=False, help_text=\"True if the TAR is currently uploading.\" ) local_storage = models.BooleanField( default=True, help_text=\"True if the TAR is stored locally.\" ) archived = models.BooleanField( default=False, help_text=\"True if the TAR is archived remotely.\" ) path = models.CharField( max_length=500, blank=True, help_text=\"Filesystem path to the TAR archive.\" ) archive = models.ForeignKey( Archive, related_name=\"tar_files\", on_delete=models.PROTECT, null=True, help_text=\"Archive to which this TAR file belongs.\" ) def __str__(self) -> str: \"\"\"Return the name of the TAR file.\"\"\" return self.name def clean_tar(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Remove the TAR file from storage and update the database accordingly. Args: delete_obj (bool): If True, delete the database object. force_delete (bool): If True, force deletion even if errors occur. Returns: bool: True if successful, False otherwise. \"\"\" logger.info( f\"Clean TAR file {self.name} - Delete object: {delete_obj}\") if not self.local_storage and not self.archived and not self.uploading: logger.info( f\"Clean TAR file {self.name} - object exists only in database\") return True if self.local_storage: tar_name = self.name if \".tar.gz\" not in tar_name: tar_name = tar_name + \".tar.gz\" tar_path = os.path.join( settings.FILE_STORAGE_ROOT, self.path, tar_name) logger.info( f\"Clean TAR file {self.name} - try to delete local TAR\") success = try_remove_file_clean_dirs(tar_path) if not success and not force_delete: logger.error( f\"Clean TAR file {self.name} - failed - could not delete local TAR\") return False elif success: logger.info( f\"Clean TAR file {self.name} - try to delete local TAR - success\") if not delete_obj: logger.info( f\"Clean TAR file {self.name} - Alter database object\") self.local_storage = False self.save() return True elif not self.local_storage and delete_obj and force_delete: if not all(self.files.values_list(\"local_storage\", flat=True)): logger.error( f\"{self.name}: Some files contained in this TAR are no longer stored locally. The remote TAR cannot be deleted.\") return False self.files.all().update(archived=False) ssh_client = self.archive.init_ssh_client() ssh_connect_success = ssh_client.connect_to_ssh() if not ssh_connect_success: return False remote_path = posixjoin(self.path, self.name + \".tar.gz\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: remote_path = posixjoin(self.path, self.name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: logger.info(f\"{self.name}: Cannot remove remote TAR. {stdout}\") return False else: logger.info(f\"{self.name}: Remote TAR removed.\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"find {self.path} -type d -empty -delete\") return True else: return False __str__() Return the name of the TAR file. Source code in archiving\\models.py def __str__(self) -> str: \"\"\"Return the name of the TAR file.\"\"\" return self.name clean_tar(delete_obj=False, force_delete=False) Remove the TAR file from storage and update the database accordingly. Parameters: delete_obj ( bool , default: False ) \u2013 If True, delete the database object. force_delete ( bool , default: False ) \u2013 If True, force deletion even if errors occur. Returns: bool ( bool ) \u2013 True if successful, False otherwise. Source code in archiving\\models.py def clean_tar(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Remove the TAR file from storage and update the database accordingly. Args: delete_obj (bool): If True, delete the database object. force_delete (bool): If True, force deletion even if errors occur. Returns: bool: True if successful, False otherwise. \"\"\" logger.info( f\"Clean TAR file {self.name} - Delete object: {delete_obj}\") if not self.local_storage and not self.archived and not self.uploading: logger.info( f\"Clean TAR file {self.name} - object exists only in database\") return True if self.local_storage: tar_name = self.name if \".tar.gz\" not in tar_name: tar_name = tar_name + \".tar.gz\" tar_path = os.path.join( settings.FILE_STORAGE_ROOT, self.path, tar_name) logger.info( f\"Clean TAR file {self.name} - try to delete local TAR\") success = try_remove_file_clean_dirs(tar_path) if not success and not force_delete: logger.error( f\"Clean TAR file {self.name} - failed - could not delete local TAR\") return False elif success: logger.info( f\"Clean TAR file {self.name} - try to delete local TAR - success\") if not delete_obj: logger.info( f\"Clean TAR file {self.name} - Alter database object\") self.local_storage = False self.save() return True elif not self.local_storage and delete_obj and force_delete: if not all(self.files.values_list(\"local_storage\", flat=True)): logger.error( f\"{self.name}: Some files contained in this TAR are no longer stored locally. The remote TAR cannot be deleted.\") return False self.files.all().update(archived=False) ssh_client = self.archive.init_ssh_client() ssh_connect_success = ssh_client.connect_to_ssh() if not ssh_connect_success: return False remote_path = posixjoin(self.path, self.name + \".tar.gz\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: remote_path = posixjoin(self.path, self.name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: logger.info(f\"{self.name}: Cannot remove remote TAR. {stdout}\") return False else: logger.info(f\"{self.name}: Remote TAR removed.\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"find {self.path} -type d -empty -delete\") return True else: return False pre_remove_tar(sender, instance, **kwargs) Signal handler to clean up TAR file storage before deleting the TarFile instance. Raises: Exception \u2013 If cleanup fails. Source code in archiving\\models.py @receiver(pre_delete, sender=TarFile) def pre_remove_tar(sender, instance: \"TarFile\", **kwargs) -> None: \"\"\" Signal handler to clean up TAR file storage before deleting the TarFile instance. Raises: Exception: If cleanup fails. \"\"\" success = instance.clean_tar(True) if not success: raise Exception(f\"Unable to remove TAR file {instance.name}\")","title":"models"},{"location":"reference/archiving/models/#archiving.models.Archive","text":"Bases: BaseModel Source code in archiving\\models.py class Archive(BaseModel): name = models.CharField( max_length=200, help_text=\"A human-readable name for this archive.\" ) username = models.CharField( max_length=50, unique=True, help_text=\"Username for SSH login to the archive server.\" ) password = EncryptedCharField( max_length=128, help_text=\"Password for SSH login to the archive server. Stored encrypted.\" ) address = models.CharField( max_length=100, unique=True, help_text=\"Network address of the archive server.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_archives\", on_delete=models.SET_NULL, null=True, help_text=\"User who owns this archive.\" ) root_folder = models.CharField( max_length=100, unique=True, help_text=\"Root folder path on the archive server.\" ) def __str__(self) -> str: \"\"\"Return the name of the archive.\"\"\" return self.name def init_ssh_client(self) -> SSH_client: \"\"\"Initialize an SSH client for this archive.\"\"\" return SSH_client(self.username, self.password, self.address, 22) def check_projects(self) -> None: \"\"\"Check and update archive projects.\"\"\" from .functions import check_archive_projects check_archive_projects(self) def check_upload(self) -> None: \"\"\"Check and update archive uploads.\"\"\" from .functions import check_archive_upload check_archive_upload(self)","title":"Archive"},{"location":"reference/archiving/models/#archiving.models.Archive.__str__","text":"Return the name of the archive. Source code in archiving\\models.py def __str__(self) -> str: \"\"\"Return the name of the archive.\"\"\" return self.name","title":"__str__"},{"location":"reference/archiving/models/#archiving.models.Archive.check_projects","text":"Check and update archive projects. Source code in archiving\\models.py def check_projects(self) -> None: \"\"\"Check and update archive projects.\"\"\" from .functions import check_archive_projects check_archive_projects(self)","title":"check_projects"},{"location":"reference/archiving/models/#archiving.models.Archive.check_upload","text":"Check and update archive uploads. Source code in archiving\\models.py def check_upload(self) -> None: \"\"\"Check and update archive uploads.\"\"\" from .functions import check_archive_upload check_archive_upload(self)","title":"check_upload"},{"location":"reference/archiving/models/#archiving.models.Archive.init_ssh_client","text":"Initialize an SSH client for this archive. Source code in archiving\\models.py def init_ssh_client(self) -> SSH_client: \"\"\"Initialize an SSH client for this archive.\"\"\" return SSH_client(self.username, self.password, self.address, 22)","title":"init_ssh_client"},{"location":"reference/archiving/models/#archiving.models.TarFile","text":"Bases: BaseModel Source code in archiving\\models.py class TarFile(BaseModel): name = models.CharField( max_length=200, help_text=\"Filename of the TAR archive (without extension).\" ) archived_dt = models.DateTimeField( default=djtimezone.now, help_text=\"Datetime this TAR was archived.\" ) uploading = models.BooleanField( default=False, help_text=\"True if the TAR is currently uploading.\" ) local_storage = models.BooleanField( default=True, help_text=\"True if the TAR is stored locally.\" ) archived = models.BooleanField( default=False, help_text=\"True if the TAR is archived remotely.\" ) path = models.CharField( max_length=500, blank=True, help_text=\"Filesystem path to the TAR archive.\" ) archive = models.ForeignKey( Archive, related_name=\"tar_files\", on_delete=models.PROTECT, null=True, help_text=\"Archive to which this TAR file belongs.\" ) def __str__(self) -> str: \"\"\"Return the name of the TAR file.\"\"\" return self.name def clean_tar(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Remove the TAR file from storage and update the database accordingly. Args: delete_obj (bool): If True, delete the database object. force_delete (bool): If True, force deletion even if errors occur. Returns: bool: True if successful, False otherwise. \"\"\" logger.info( f\"Clean TAR file {self.name} - Delete object: {delete_obj}\") if not self.local_storage and not self.archived and not self.uploading: logger.info( f\"Clean TAR file {self.name} - object exists only in database\") return True if self.local_storage: tar_name = self.name if \".tar.gz\" not in tar_name: tar_name = tar_name + \".tar.gz\" tar_path = os.path.join( settings.FILE_STORAGE_ROOT, self.path, tar_name) logger.info( f\"Clean TAR file {self.name} - try to delete local TAR\") success = try_remove_file_clean_dirs(tar_path) if not success and not force_delete: logger.error( f\"Clean TAR file {self.name} - failed - could not delete local TAR\") return False elif success: logger.info( f\"Clean TAR file {self.name} - try to delete local TAR - success\") if not delete_obj: logger.info( f\"Clean TAR file {self.name} - Alter database object\") self.local_storage = False self.save() return True elif not self.local_storage and delete_obj and force_delete: if not all(self.files.values_list(\"local_storage\", flat=True)): logger.error( f\"{self.name}: Some files contained in this TAR are no longer stored locally. The remote TAR cannot be deleted.\") return False self.files.all().update(archived=False) ssh_client = self.archive.init_ssh_client() ssh_connect_success = ssh_client.connect_to_ssh() if not ssh_connect_success: return False remote_path = posixjoin(self.path, self.name + \".tar.gz\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: remote_path = posixjoin(self.path, self.name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: logger.info(f\"{self.name}: Cannot remove remote TAR. {stdout}\") return False else: logger.info(f\"{self.name}: Remote TAR removed.\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"find {self.path} -type d -empty -delete\") return True else: return False","title":"TarFile"},{"location":"reference/archiving/models/#archiving.models.TarFile.__str__","text":"Return the name of the TAR file. Source code in archiving\\models.py def __str__(self) -> str: \"\"\"Return the name of the TAR file.\"\"\" return self.name","title":"__str__"},{"location":"reference/archiving/models/#archiving.models.TarFile.clean_tar","text":"Remove the TAR file from storage and update the database accordingly. Parameters: delete_obj ( bool , default: False ) \u2013 If True, delete the database object. force_delete ( bool , default: False ) \u2013 If True, force deletion even if errors occur. Returns: bool ( bool ) \u2013 True if successful, False otherwise. Source code in archiving\\models.py def clean_tar(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Remove the TAR file from storage and update the database accordingly. Args: delete_obj (bool): If True, delete the database object. force_delete (bool): If True, force deletion even if errors occur. Returns: bool: True if successful, False otherwise. \"\"\" logger.info( f\"Clean TAR file {self.name} - Delete object: {delete_obj}\") if not self.local_storage and not self.archived and not self.uploading: logger.info( f\"Clean TAR file {self.name} - object exists only in database\") return True if self.local_storage: tar_name = self.name if \".tar.gz\" not in tar_name: tar_name = tar_name + \".tar.gz\" tar_path = os.path.join( settings.FILE_STORAGE_ROOT, self.path, tar_name) logger.info( f\"Clean TAR file {self.name} - try to delete local TAR\") success = try_remove_file_clean_dirs(tar_path) if not success and not force_delete: logger.error( f\"Clean TAR file {self.name} - failed - could not delete local TAR\") return False elif success: logger.info( f\"Clean TAR file {self.name} - try to delete local TAR - success\") if not delete_obj: logger.info( f\"Clean TAR file {self.name} - Alter database object\") self.local_storage = False self.save() return True elif not self.local_storage and delete_obj and force_delete: if not all(self.files.values_list(\"local_storage\", flat=True)): logger.error( f\"{self.name}: Some files contained in this TAR are no longer stored locally. The remote TAR cannot be deleted.\") return False self.files.all().update(archived=False) ssh_client = self.archive.init_ssh_client() ssh_connect_success = ssh_client.connect_to_ssh() if not ssh_connect_success: return False remote_path = posixjoin(self.path, self.name + \".tar.gz\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: remote_path = posixjoin(self.path, self.name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm {remote_path}\") if status_code != 0: logger.info(f\"{self.name}: Cannot remove remote TAR. {stdout}\") return False else: logger.info(f\"{self.name}: Remote TAR removed.\") status_code, stdout, stderr = ssh_client.send_ssh_command( f\"find {self.path} -type d -empty -delete\") return True else: return False","title":"clean_tar"},{"location":"reference/archiving/models/#archiving.models.pre_remove_tar","text":"Signal handler to clean up TAR file storage before deleting the TarFile instance. Raises: Exception \u2013 If cleanup fails. Source code in archiving\\models.py @receiver(pre_delete, sender=TarFile) def pre_remove_tar(sender, instance: \"TarFile\", **kwargs) -> None: \"\"\" Signal handler to clean up TAR file storage before deleting the TarFile instance. Raises: Exception: If cleanup fails. \"\"\" success = instance.clean_tar(True) if not success: raise Exception(f\"Unable to remove TAR file {instance.name}\")","title":"pre_remove_tar"},{"location":"reference/archiving/tar_functions/","text":"check_tar_status(ssh_client, tar_path) Check the status of a tar file on a remote system via SSH. Parameters: ssh_client ( SSH_client ) \u2013 SSH client instance for remote command execution. tar_path ( str ) \u2013 Path to the tar file on the remote system. Returns: Tuple [ int , Optional [ str ]] \u2013 Tuple[int, Optional[str]]: (Status code, tar file status string if successful, else None) Source code in archiving\\tar_functions.py def check_tar_status( ssh_client: SSH_client, tar_path: str ) -> Tuple[int, Optional[str]]: \"\"\" Check the status of a tar file on a remote system via SSH. Args: ssh_client (SSH_client): SSH client instance for remote command execution. tar_path (str): Path to the tar file on the remote system. Returns: Tuple[int, Optional[str]]: (Status code, tar file status string if successful, else None) \"\"\" status_code, stdout, stderr = ssh_client.send_ssh_command( f\"dmls -l {posixjoin(tar_path)}\") if status_code != 0: return status_code, None target_tar_status = stdout[0].split(\" \")[-2] return status_code, target_tar_status create_tar_file(file_objs, name_suffix=0) Create a tar.gz archive for the given files, add metadata, and clean up. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects to be archived. name_suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: Tuple [ bool , str , Optional [ str ]] \u2013 Tuple[bool, str, Optional[str]]: (Success status, tar file name, full tar file path if successful, else None) Source code in archiving\\tar_functions.py def create_tar_file( file_objs: QuerySet, name_suffix: int = 0 ) -> Tuple[bool, str, Optional[str]]: \"\"\" Create a tar.gz archive for the given files, add metadata, and clean up. Args: file_objs (QuerySet): QuerySet of DataFile objects to be archived. name_suffix (int, optional): Suffix for the tar file name. Returns: Tuple[bool, str, Optional[str]]: (Success status, tar file name, full tar file path if successful, else None) \"\"\" # get TAR name tar_name = get_tar_name(file_objs, name_suffix) tar_name_format = tar_name+\".tar.gz\" device_type = file_objs.device_type().values_list( \"device_type\", flat=True).first().replace(\" \", \"\") tar_path = os.path.join(settings.FILE_STORAGE_ROOT, \"archiving\", device_type, datetime.now().strftime(\"%Y%m%d\")) os.makedirs(tar_path, exist_ok=True) full_tar_path = os.path.join(tar_path, tar_name_format) # get list of file paths relative_paths = file_objs.relative_paths().values_list(\"relative_path\", flat=True) metadata_dir_path = os.path.join(tar_path, tar_name) relative_metadata_dir_path = os.path.relpath( metadata_dir_path, settings.FILE_STORAGE_ROOT) logger.info(f\"{tar_name}: generating bagit data\") # Generate bagit metadata all_metadata_paths = bag_info_from_files(file_objs, metadata_dir_path) # Generate metadata file metadata_json_path = metadata_json_from_files(file_objs, metadata_dir_path) all_metadata_paths.append(metadata_json_path) relative_metadata_paths = [os.path.relpath( x, settings.FILE_STORAGE_ROOT) for x in all_metadata_paths] # TAR files # Use transform command to generate data dir inside the TAR, move metadata files to root tar_command = [\"tar\", \"zcvf\", full_tar_path, \"--transform\", f\"s,^,data/,;s,data/{relative_metadata_dir_path}/,,\",] + relative_paths + relative_metadata_paths success, output = call_with_output(tar_command, settings.FILE_STORAGE_ROOT) # regardless of status, we remove the metadata files [try_remove_file_clean_dirs(x) for x in all_metadata_paths] if not success: logger.info(f\"{tar_name}: Error creating TAR\") logger.info(output) return False, tar_name, None logger.info(f\"{tar_name}: succesfully created\") return True, tar_name, full_tar_path create_tar_file_and_obj(file_objs, archive_obj, name_suffix=0) Create a tar file from file_objs and register a TarFile object in the database. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects to be archived. archive_obj ( Archive ) \u2013 Archive instance to associate TarFile with. name_suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: bool ( bool ) \u2013 True if tar file creation succeeded, False otherwise. Source code in archiving\\tar_functions.py def create_tar_file_and_obj( file_objs: QuerySet, archive_obj: Archive, name_suffix: int = 0 ) -> bool: \"\"\" Create a tar file from file_objs and register a TarFile object in the database. Args: file_objs (QuerySet): QuerySet of DataFile objects to be archived. archive_obj (Archive): Archive instance to associate TarFile with. name_suffix (int, optional): Suffix for the tar file name. Returns: bool: True if tar file creation succeeded, False otherwise. \"\"\" success, tar_name, full_tar_path = create_tar_file( file_objs, name_suffix) if not success: # Free data file objects file_objs.update(tar_file=None) return False else: new_tar_obj = TarFile.objects.create( name=tar_name, path=os.path.split(full_tar_path)[0], archive=archive_obj) file_objs.update(tar_file=new_tar_obj) return True create_tar_files(file_pks, archive_pk) Split files into appropriately sized groups and create tar files for each group. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys of DataFile objects to be archived. archive_pk ( int ) \u2013 Primary key of the Archive object to associate tar files with. Source code in archiving\\tar_functions.py def create_tar_files(file_pks: List[int], archive_pk: int) -> None: \"\"\" Split files into appropriately sized groups and create tar files for each group. Args: file_pks (List[int]): List of primary keys of DataFile objects to be archived. archive_pk (int): Primary key of the Archive object to associate tar files with. \"\"\" file_objs = DataFile.objects.filter(pk__in=file_pks) # Assign these files to a dummy TAR in_progress_tar, created = TarFile.objects.get_or_create( name=\"in_progress\", uploading=True) file_objs.update(tar_file=in_progress_tar) file_splits_ok = get_tar_splits(file_objs) archive_obj = Archive.objects.get(pk=archive_pk) for idx, file_split in enumerate(file_splits_ok): file_split_pks = file_split['file_pks'] file_split_objs = DataFile.objects.filter(pk__in=file_split_pks) create_tar_file_and_obj(file_split_objs, archive_obj, idx) get_tar_name(file_objs, suffix=0) Generate a descriptive tar file name based on file attributes and date range. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects. suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: str ( str ) \u2013 The generated tar file name (without file extension). Source code in archiving\\tar_functions.py def get_tar_name(file_objs: QuerySet, suffix: int = 0) -> str: \"\"\" Generate a descriptive tar file name based on file attributes and date range. Args: file_objs (QuerySet): QuerySet of DataFile objects. suffix (int, optional): Suffix for the tar file name. Returns: str: The generated tar file name (without file extension). \"\"\" min_date = file_objs.min_date() min_date_str = min_date.strftime(\"%Y%m%d\") max_date = file_objs.max_date() max_date_str = max_date.strftime(\"%Y%m%d\") combo_project = file_objs.values_list( \"deployment__combo_project\", flat=True).first().replace(\"-\", \"\").replace(\" \", \"-\") device_type = file_objs.device_type().values_list( \"device_type\", flat=True).first().replace(\" \", \"\") creation_dt = datetime.now().strftime(\"%Y%m%d_%H%M%S\") tar_name = (\"_\").join([combo_project, device_type, min_date_str, max_date_str, creation_dt, str(suffix)]) return tar_name get_tar_splits(file_objs) Split a set of files into groups suitable for tarring, based on size. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects. Returns: List [ Dict [ str , Any ]] \u2013 List[Dict[str, Any]]: List of dictionaries describing file splits that meet size requirements. Source code in archiving\\tar_functions.py def get_tar_splits(file_objs: QuerySet) -> List[Dict[str, Any]]: \"\"\" Split a set of files into groups suitable for tarring, based on size. Args: file_objs (QuerySet): QuerySet of DataFile objects. Returns: List[Dict[str, Any]]: List of dictionaries describing file splits that meet size requirements. \"\"\" file_splits = group_files_by_size(file_objs) too_small_split_pks = [ x for y in file_splits if y[\"total_size_gb\"] < settings.MIN_ARCHIVE_SIZE_GB for x in y['file_pks']] # Remove files whose TAR would not be large enough from the in progress tar too_small_file_objs = DataFile.objects.filter(pk__in=too_small_split_pks) n_removed_files = too_small_file_objs.update(tar_file=None) logger.info(f\"{n_removed_files} in too small a grouping\") file_splits_ok = [ x for x in file_splits if x[\"total_size_gb\"] >= settings.MIN_ARCHIVE_SIZE_GB] return file_splits_ok","title":"tar_functions"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.check_tar_status","text":"Check the status of a tar file on a remote system via SSH. Parameters: ssh_client ( SSH_client ) \u2013 SSH client instance for remote command execution. tar_path ( str ) \u2013 Path to the tar file on the remote system. Returns: Tuple [ int , Optional [ str ]] \u2013 Tuple[int, Optional[str]]: (Status code, tar file status string if successful, else None) Source code in archiving\\tar_functions.py def check_tar_status( ssh_client: SSH_client, tar_path: str ) -> Tuple[int, Optional[str]]: \"\"\" Check the status of a tar file on a remote system via SSH. Args: ssh_client (SSH_client): SSH client instance for remote command execution. tar_path (str): Path to the tar file on the remote system. Returns: Tuple[int, Optional[str]]: (Status code, tar file status string if successful, else None) \"\"\" status_code, stdout, stderr = ssh_client.send_ssh_command( f\"dmls -l {posixjoin(tar_path)}\") if status_code != 0: return status_code, None target_tar_status = stdout[0].split(\" \")[-2] return status_code, target_tar_status","title":"check_tar_status"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.create_tar_file","text":"Create a tar.gz archive for the given files, add metadata, and clean up. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects to be archived. name_suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: Tuple [ bool , str , Optional [ str ]] \u2013 Tuple[bool, str, Optional[str]]: (Success status, tar file name, full tar file path if successful, else None) Source code in archiving\\tar_functions.py def create_tar_file( file_objs: QuerySet, name_suffix: int = 0 ) -> Tuple[bool, str, Optional[str]]: \"\"\" Create a tar.gz archive for the given files, add metadata, and clean up. Args: file_objs (QuerySet): QuerySet of DataFile objects to be archived. name_suffix (int, optional): Suffix for the tar file name. Returns: Tuple[bool, str, Optional[str]]: (Success status, tar file name, full tar file path if successful, else None) \"\"\" # get TAR name tar_name = get_tar_name(file_objs, name_suffix) tar_name_format = tar_name+\".tar.gz\" device_type = file_objs.device_type().values_list( \"device_type\", flat=True).first().replace(\" \", \"\") tar_path = os.path.join(settings.FILE_STORAGE_ROOT, \"archiving\", device_type, datetime.now().strftime(\"%Y%m%d\")) os.makedirs(tar_path, exist_ok=True) full_tar_path = os.path.join(tar_path, tar_name_format) # get list of file paths relative_paths = file_objs.relative_paths().values_list(\"relative_path\", flat=True) metadata_dir_path = os.path.join(tar_path, tar_name) relative_metadata_dir_path = os.path.relpath( metadata_dir_path, settings.FILE_STORAGE_ROOT) logger.info(f\"{tar_name}: generating bagit data\") # Generate bagit metadata all_metadata_paths = bag_info_from_files(file_objs, metadata_dir_path) # Generate metadata file metadata_json_path = metadata_json_from_files(file_objs, metadata_dir_path) all_metadata_paths.append(metadata_json_path) relative_metadata_paths = [os.path.relpath( x, settings.FILE_STORAGE_ROOT) for x in all_metadata_paths] # TAR files # Use transform command to generate data dir inside the TAR, move metadata files to root tar_command = [\"tar\", \"zcvf\", full_tar_path, \"--transform\", f\"s,^,data/,;s,data/{relative_metadata_dir_path}/,,\",] + relative_paths + relative_metadata_paths success, output = call_with_output(tar_command, settings.FILE_STORAGE_ROOT) # regardless of status, we remove the metadata files [try_remove_file_clean_dirs(x) for x in all_metadata_paths] if not success: logger.info(f\"{tar_name}: Error creating TAR\") logger.info(output) return False, tar_name, None logger.info(f\"{tar_name}: succesfully created\") return True, tar_name, full_tar_path","title":"create_tar_file"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.create_tar_file_and_obj","text":"Create a tar file from file_objs and register a TarFile object in the database. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects to be archived. archive_obj ( Archive ) \u2013 Archive instance to associate TarFile with. name_suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: bool ( bool ) \u2013 True if tar file creation succeeded, False otherwise. Source code in archiving\\tar_functions.py def create_tar_file_and_obj( file_objs: QuerySet, archive_obj: Archive, name_suffix: int = 0 ) -> bool: \"\"\" Create a tar file from file_objs and register a TarFile object in the database. Args: file_objs (QuerySet): QuerySet of DataFile objects to be archived. archive_obj (Archive): Archive instance to associate TarFile with. name_suffix (int, optional): Suffix for the tar file name. Returns: bool: True if tar file creation succeeded, False otherwise. \"\"\" success, tar_name, full_tar_path = create_tar_file( file_objs, name_suffix) if not success: # Free data file objects file_objs.update(tar_file=None) return False else: new_tar_obj = TarFile.objects.create( name=tar_name, path=os.path.split(full_tar_path)[0], archive=archive_obj) file_objs.update(tar_file=new_tar_obj) return True","title":"create_tar_file_and_obj"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.create_tar_files","text":"Split files into appropriately sized groups and create tar files for each group. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys of DataFile objects to be archived. archive_pk ( int ) \u2013 Primary key of the Archive object to associate tar files with. Source code in archiving\\tar_functions.py def create_tar_files(file_pks: List[int], archive_pk: int) -> None: \"\"\" Split files into appropriately sized groups and create tar files for each group. Args: file_pks (List[int]): List of primary keys of DataFile objects to be archived. archive_pk (int): Primary key of the Archive object to associate tar files with. \"\"\" file_objs = DataFile.objects.filter(pk__in=file_pks) # Assign these files to a dummy TAR in_progress_tar, created = TarFile.objects.get_or_create( name=\"in_progress\", uploading=True) file_objs.update(tar_file=in_progress_tar) file_splits_ok = get_tar_splits(file_objs) archive_obj = Archive.objects.get(pk=archive_pk) for idx, file_split in enumerate(file_splits_ok): file_split_pks = file_split['file_pks'] file_split_objs = DataFile.objects.filter(pk__in=file_split_pks) create_tar_file_and_obj(file_split_objs, archive_obj, idx)","title":"create_tar_files"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.get_tar_name","text":"Generate a descriptive tar file name based on file attributes and date range. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects. suffix ( int , default: 0 ) \u2013 Suffix for the tar file name. Returns: str ( str ) \u2013 The generated tar file name (without file extension). Source code in archiving\\tar_functions.py def get_tar_name(file_objs: QuerySet, suffix: int = 0) -> str: \"\"\" Generate a descriptive tar file name based on file attributes and date range. Args: file_objs (QuerySet): QuerySet of DataFile objects. suffix (int, optional): Suffix for the tar file name. Returns: str: The generated tar file name (without file extension). \"\"\" min_date = file_objs.min_date() min_date_str = min_date.strftime(\"%Y%m%d\") max_date = file_objs.max_date() max_date_str = max_date.strftime(\"%Y%m%d\") combo_project = file_objs.values_list( \"deployment__combo_project\", flat=True).first().replace(\"-\", \"\").replace(\" \", \"-\") device_type = file_objs.device_type().values_list( \"device_type\", flat=True).first().replace(\" \", \"\") creation_dt = datetime.now().strftime(\"%Y%m%d_%H%M%S\") tar_name = (\"_\").join([combo_project, device_type, min_date_str, max_date_str, creation_dt, str(suffix)]) return tar_name","title":"get_tar_name"},{"location":"reference/archiving/tar_functions/#archiving.tar_functions.get_tar_splits","text":"Split a set of files into groups suitable for tarring, based on size. Parameters: file_objs ( QuerySet ) \u2013 QuerySet of DataFile objects. Returns: List [ Dict [ str , Any ]] \u2013 List[Dict[str, Any]]: List of dictionaries describing file splits that meet size requirements. Source code in archiving\\tar_functions.py def get_tar_splits(file_objs: QuerySet) -> List[Dict[str, Any]]: \"\"\" Split a set of files into groups suitable for tarring, based on size. Args: file_objs (QuerySet): QuerySet of DataFile objects. Returns: List[Dict[str, Any]]: List of dictionaries describing file splits that meet size requirements. \"\"\" file_splits = group_files_by_size(file_objs) too_small_split_pks = [ x for y in file_splits if y[\"total_size_gb\"] < settings.MIN_ARCHIVE_SIZE_GB for x in y['file_pks']] # Remove files whose TAR would not be large enough from the in progress tar too_small_file_objs = DataFile.objects.filter(pk__in=too_small_split_pks) n_removed_files = too_small_file_objs.update(tar_file=None) logger.info(f\"{n_removed_files} in too small a grouping\") file_splits_ok = [ x for x in file_splits if x[\"total_size_gb\"] >= settings.MIN_ARCHIVE_SIZE_GB] return file_splits_ok","title":"get_tar_splits"},{"location":"reference/archiving/tasks/","text":"check_all_archive_projects_task() Check all archive projects by iterating through all Archive objects and calling their check_projects method. Source code in archiving\\tasks.py @app.task() def check_all_archive_projects_task() -> None: \"\"\" Check all archive projects by iterating through all Archive objects and calling their check_projects method. \"\"\" all_archives = Archive.objects.all() # Iterate through every archive and check its projects for archive in all_archives: archive.check_projects() check_all_uploads_task() Check uploads for all Archive objects by calling their check_upload method. Source code in archiving\\tasks.py @app.task() def check_all_uploads_task() -> None: \"\"\" Check uploads for all Archive objects by calling their check_upload method. \"\"\" all_archives = Archive.objects.all() # Iterate through every archive and check its uploads for archive in all_archives: archive.check_upload() check_archive_upload_task(archive_pk) Check upload for a specific archive. Parameters: archive_pk ( int ) \u2013 Primary key of the archive. Source code in archiving\\tasks.py @app.task() def check_archive_upload_task(archive_pk: int) -> None: \"\"\" Check upload for a specific archive. Args: archive_pk (int): Primary key of the archive. \"\"\" # Find the archive by PK and trigger upload check archive = Archive.objects.get(pk=archive_pk) archive.check_upload() create_tar_files_task(file_pks, archive_pk) Task wrapper for create_tar_files function. Parameters: file_pks ( List [ int ] ) \u2013 Primary keys of files to be TARred. archive_pk ( int ) \u2013 Primary key of archive to which these TARs will be attached. Source code in archiving\\tasks.py @app.task() def create_tar_files_task(file_pks: List[int], archive_pk: int) -> None: \"\"\" Task wrapper for create_tar_files function. Args: file_pks (List[int]): Primary keys of files to be TARred. archive_pk (int): Primary key of archive to which these TARs will be attached. \"\"\" # Calls the actual function to handle TAR file creation create_tar_files(file_pks, archive_pk) get_files_from_archive_task(file_pks, callback=None) For a list of DataFile PKs, orchestrate their retrieval from archives using Celery groups and chords. Parameters: file_pks ( List [ int ] ) \u2013 Primary keys of files to retrieve. callback ( Optional [ Callable ] , default: None ) \u2013 Optional callback task to run after retrieval. Source code in archiving\\tasks.py @app.task() def get_files_from_archive_task(file_pks: List[int], callback: Optional[Callable] = None) -> None: \"\"\" For a list of DataFile PKs, orchestrate their retrieval from archives using Celery groups and chords. Args: file_pks (List[int]): Primary keys of files to retrieve. callback (Optional[Callable]): Optional callback task to run after retrieval. \"\"\" # Filter for only archived files matching the provided PKs file_objs = DataFile.objects.filter(pk__in=file_pks, archived=True) # Get unique TAR files containing these files tar_file_objs = TarFile.objects.filter( pk__in=file_objs.values_list('tar_file__pk', flat=True).distinct()) tar_file_pks = list(tar_file_objs.values_list('pk', flat=True)) all_tasks = [] # For each TAR, create an async job for its files for tar_file_pk in tar_file_pks: target_file_pks = list(file_objs.filter( tar_file__pk=tar_file_pk).values_list('pk', flat=True)) all_tasks.append(get_files_from_archived_tar_task.si( tar_file_pk, target_file_pks)) task_group = group(all_tasks) # Celery group for parallel execution # Callback tasks to run after all file retrieval jobs post_tasks = [post_get_file_from_archive_task.s()] if callback is not None: post_tasks.append(callback) post_task_group = group(post_tasks) # Chord schedules post-tasks after all group tasks task_chord = chord(task_group, post_task_group) task_chord.apply_async() get_files_from_archived_tar_task(self, tar_file_pk, target_file_pks) Retrieve specified files from a TAR archive, handling staging, extraction, and local placement. Parameters: self ( Any ) \u2013 Task instance (provided by Celery when bind=True). tar_file_pk ( int ) \u2013 Primary key of the TarFile object. target_file_pks ( List [ int ] ) \u2013 List of file PKs to retrieve. Returns: List [ int ] \u2013 List[int]: Primary keys of successfully retrieved DataFile objects. Source code in archiving\\tasks.py @app.task(autoretry_for=(TooManyTasks, TAROffline), max_retries=None, retry_backoff=2*60, retry_backoff_max=5 * 60, retry_jitter=True, bind=True) def get_files_from_archived_tar_task(self: Any, tar_file_pk: int, target_file_pks: List[int]) -> List[int]: \"\"\" Retrieve specified files from a TAR archive, handling staging, extraction, and local placement. Args: self (Any): Task instance (provided by Celery when bind=True). tar_file_pk (int): Primary key of the TarFile object. target_file_pks (List[int]): List of file PKs to retrieve. Returns: List[int]: Primary keys of successfully retrieved DataFile objects. \"\"\" # Limit the number of simultaneous tasks of this type to avoid overloading resources check_simultaneous_tasks(self, 4) tar_file_obj = TarFile.objects.get(pk=tar_file_pk) file_objs = DataFile.objects.filter(pk__in=target_file_pks) file_names = file_objs.full_names().values_list(\"full_name\", flat=True) # Connect to the archive server via SSH archive_obj = tar_file_obj.archive ssh_client = archive_obj.init_ssh_client() # Try to locate the compressed TAR file first tar_name = tar_file_obj.name + '.tar.gz' tar_path = posixjoin(tar_file_obj.path, tar_name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"dals -l {tar_path}\") # Check if the TAR file is online or needs to be staged from tape status_code, target_tar_status = check_tar_status(ssh_client, tar_path) logger.info(f\"{tar_path}: Get TAR status {status_code}\") # If not found, try without .tar.gz extension if status_code == 1: tar_name = tar_file_obj.name tar_path = posixjoin(tar_file_obj.path, tar_name) status_code, target_tar_status = check_tar_status(ssh_client, tar_path) logger.info(f\"{tar_path}: Get TAR status {status_code}\") if status_code == 1: raise Exception(f\"{tar_path}: TAR file not present at this path\") logger.info( f\"{tar_path}: Get TAR status {status_code} {target_tar_status}\") # If not online, try to retrieve from tape storage (or handle unmigrating state) online_statuses = ['(REG)', '(DUL)', '(MIG)'] if target_tar_status not in online_statuses: initial_offline = True logger.info(f\"{tar_path}: Offline\") if target_tar_status != '(UNM)': # Request data to be staged from tape status_code, stdout, stderr = ssh_client.send_ssh_command( f\"daget {tar_path}\") logger.info( f\"{tar_path}: Get TAR from tape {status_code} {stdout}\") status_code, target_tar_status = check_tar_status( ssh_client, tar_path) else: raise (TAROffline(f\"{tar_path}: already unmigrating\")) if target_tar_status not in online_statuses: # If still not online after staging attempt, raise error raise Exception(f\"{tar_path}: TAR file could not be staged\") else: initial_offline = False # Create a temporary extraction directory for this job temp_path = posixjoin(tar_file_obj.path, \"temp\", self.request.id) ftp_connection_success = ssh_client.connect_to_ftp() if not ftp_connection_success: raise Exception(\"Unable to connect to FTP\") ssh_client.mkdir_p(temp_path) # List files inside the TAR to locate the desired files status_code, stdout, stderr = ssh_client.send_ssh_command( f\"tar tvf {tar_path}\", return_strings=False) logger.info(f\"{tar_path}: List files in TAR\") in_tar_file_paths: List[str] = [] in_tar_found_files: List[Any] = [] for file_line in stdout: # Parse each line of tar output to extract the file path split_file_line = file_line.split(\" \") line_file_path = split_file_line[-1].replace(\"\\n\", \"\") # Check if this is one of our requested files found_file_paths = [x for x in file_names if x in line_file_path] if len(found_file_paths) > 0: in_tar_file_paths.append(line_file_path) in_tar_found_files.append(found_file_paths[0]) logger.info( f\"{tar_path}: {len(in_tar_file_paths)}/{len(file_names)}\") if len(in_tar_file_paths) == len(file_names): # Stop early if all target files found logger.info(f\"{tar_path}: All files_found\") break if len(in_tar_file_paths) == 0: # None of the requested files were found in the TAR archive raise Exception(f\"{tar_path}: No files found in TAR\") else: # Log any requested files not found in the archive missing_files = [x for x in file_names if x not in in_tar_found_files] if len(missing_files) > 0: logger.info(f\"{tar_path}: Files not found: {missing_files}\") # Extract files in manageable chunks to avoid command length limits chunked_in_tar_file_paths = [ x for x in divide_chunks(in_tar_file_paths, 500)] for idx, in_tar_file_paths_set in enumerate(chunked_in_tar_file_paths): logger.info( f\"{tar_path}: Extract file chunk {idx}/{len(chunked_in_tar_file_paths)}\") combined_in_tar_file_paths = ( \" \".join([f\"'{x}'\" for x in in_tar_file_paths_set])) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"tar -zxvf {tar_path} -C {temp_path} {combined_in_tar_file_paths}\") logger.info( f\"{tar_path}: Extract file chunk {idx}/{len(chunked_in_tar_file_paths)} {status_code}\") # Connect to SCP for file transfer from archive to local storage ssh_client.connect_to_scp() file_objs_to_update: List[DataFile] = [] all_pks: List[int] = [] for idx, in_tar_file_path in enumerate(in_tar_file_paths): try: full_file_name = os.path.split(in_tar_file_path)[1] file_name = os.path.splitext(full_file_name)[0] # Get the corresponding DataFile object file_obj = file_objs.get(file_name=file_name) # Prepare the local storage directory local_dir = os.path.join(settings.FILE_STORAGE_ROOT, file_obj.path) os.makedirs(local_dir, exist_ok=True) local_file_path = os.path.join(local_dir, full_file_name) temp_file_path = posixjoin(temp_path, in_tar_file_path) if not os.path.exists(local_file_path): # Transfer file from archive temp path to local storage ssh_client.scp_c.get( temp_file_path, local_file_path, preserve_times=True) # Mark file as locally available and update metadata file_obj.modified_on = djtimezone.now() file_obj.local_path = settings.FILE_STORAGE_ROOT file_obj.local_storage = True file_objs_to_update.append(file_obj) all_pks.append(file_obj.pk) except Exception as e: # Log and continue on any per-file errors logger.info(f\"{tar_path}: Error retrieving file: {repr(e)}\") logger.info(f\"{tar_path}: Update database\") DataFile.objects.bulk_update(file_objs_to_update, fields=[ \"local_path\", \"local_storage\", \"modified_on\"]) logger.info(f\"{tar_path}: Clear temporary files\") # Remove temporary extraction files from the archive server status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm -rf {temp_path}\") ssh_client.close_connection() return all_pks post_get_file_from_archive_task(all_file_pks) After files are retrieved from archive, process them by sensor model and file format. Parameters: all_file_pks ( List [ List [ int ]] ) \u2013 Nested lists of file PKs. Source code in archiving\\tasks.py @app.task() def post_get_file_from_archive_task(all_file_pks: List[List[int]]) -> None: \"\"\" After files are retrieved from archive, process them by sensor model and file format. Args: all_file_pks (List[List[int]]): Nested lists of file PKs. \"\"\" # Flatten the list of lists of PKs into a single list all_file_pks_flat = [item for items in all_file_pks for item in items] # Query all DataFile objects just downloaded file_objs = DataFile.objects.filter(pk__in=all_file_pks_flat) # Find all unique device models (sensor models) present in these files device_models = file_objs.values_list( \"deployment__device__model\", flat=True).distinct() # For each unique device model, process files by format for device_model in device_models: sensor_model_file_objs = file_objs.filter( deployment__device__model=device_model) # Look up the handler for this model data_handler = settings.DATA_HANDLERS.get_handler( device_model.type.name, device_model.name) # Find all unique file formats for files from this model device_file_formats = sensor_model_file_objs.values_list( \"file_format\", flat=True).distinct() # For each file format, create and dispatch a post-download task if available for device_file_format in device_file_formats: device_model_format_file_objs = sensor_model_file_objs.filter( file_format=device_file_format) device_model_format_file_file_names = list( device_model_format_file_objs.values_list(\"file_name\", flat=True)) task_name = data_handler.get_post_download_task( device_file_format, False) if task_name is not None: new_task = app.signature( task_name, [device_model_format_file_file_names], immutable=True) new_task.apply_async() # Schedule the post-download processing","title":"tasks"},{"location":"reference/archiving/tasks/#archiving.tasks.check_all_archive_projects_task","text":"Check all archive projects by iterating through all Archive objects and calling their check_projects method. Source code in archiving\\tasks.py @app.task() def check_all_archive_projects_task() -> None: \"\"\" Check all archive projects by iterating through all Archive objects and calling their check_projects method. \"\"\" all_archives = Archive.objects.all() # Iterate through every archive and check its projects for archive in all_archives: archive.check_projects()","title":"check_all_archive_projects_task"},{"location":"reference/archiving/tasks/#archiving.tasks.check_all_uploads_task","text":"Check uploads for all Archive objects by calling their check_upload method. Source code in archiving\\tasks.py @app.task() def check_all_uploads_task() -> None: \"\"\" Check uploads for all Archive objects by calling their check_upload method. \"\"\" all_archives = Archive.objects.all() # Iterate through every archive and check its uploads for archive in all_archives: archive.check_upload()","title":"check_all_uploads_task"},{"location":"reference/archiving/tasks/#archiving.tasks.check_archive_upload_task","text":"Check upload for a specific archive. Parameters: archive_pk ( int ) \u2013 Primary key of the archive. Source code in archiving\\tasks.py @app.task() def check_archive_upload_task(archive_pk: int) -> None: \"\"\" Check upload for a specific archive. Args: archive_pk (int): Primary key of the archive. \"\"\" # Find the archive by PK and trigger upload check archive = Archive.objects.get(pk=archive_pk) archive.check_upload()","title":"check_archive_upload_task"},{"location":"reference/archiving/tasks/#archiving.tasks.create_tar_files_task","text":"Task wrapper for create_tar_files function. Parameters: file_pks ( List [ int ] ) \u2013 Primary keys of files to be TARred. archive_pk ( int ) \u2013 Primary key of archive to which these TARs will be attached. Source code in archiving\\tasks.py @app.task() def create_tar_files_task(file_pks: List[int], archive_pk: int) -> None: \"\"\" Task wrapper for create_tar_files function. Args: file_pks (List[int]): Primary keys of files to be TARred. archive_pk (int): Primary key of archive to which these TARs will be attached. \"\"\" # Calls the actual function to handle TAR file creation create_tar_files(file_pks, archive_pk)","title":"create_tar_files_task"},{"location":"reference/archiving/tasks/#archiving.tasks.get_files_from_archive_task","text":"For a list of DataFile PKs, orchestrate their retrieval from archives using Celery groups and chords. Parameters: file_pks ( List [ int ] ) \u2013 Primary keys of files to retrieve. callback ( Optional [ Callable ] , default: None ) \u2013 Optional callback task to run after retrieval. Source code in archiving\\tasks.py @app.task() def get_files_from_archive_task(file_pks: List[int], callback: Optional[Callable] = None) -> None: \"\"\" For a list of DataFile PKs, orchestrate their retrieval from archives using Celery groups and chords. Args: file_pks (List[int]): Primary keys of files to retrieve. callback (Optional[Callable]): Optional callback task to run after retrieval. \"\"\" # Filter for only archived files matching the provided PKs file_objs = DataFile.objects.filter(pk__in=file_pks, archived=True) # Get unique TAR files containing these files tar_file_objs = TarFile.objects.filter( pk__in=file_objs.values_list('tar_file__pk', flat=True).distinct()) tar_file_pks = list(tar_file_objs.values_list('pk', flat=True)) all_tasks = [] # For each TAR, create an async job for its files for tar_file_pk in tar_file_pks: target_file_pks = list(file_objs.filter( tar_file__pk=tar_file_pk).values_list('pk', flat=True)) all_tasks.append(get_files_from_archived_tar_task.si( tar_file_pk, target_file_pks)) task_group = group(all_tasks) # Celery group for parallel execution # Callback tasks to run after all file retrieval jobs post_tasks = [post_get_file_from_archive_task.s()] if callback is not None: post_tasks.append(callback) post_task_group = group(post_tasks) # Chord schedules post-tasks after all group tasks task_chord = chord(task_group, post_task_group) task_chord.apply_async()","title":"get_files_from_archive_task"},{"location":"reference/archiving/tasks/#archiving.tasks.get_files_from_archived_tar_task","text":"Retrieve specified files from a TAR archive, handling staging, extraction, and local placement. Parameters: self ( Any ) \u2013 Task instance (provided by Celery when bind=True). tar_file_pk ( int ) \u2013 Primary key of the TarFile object. target_file_pks ( List [ int ] ) \u2013 List of file PKs to retrieve. Returns: List [ int ] \u2013 List[int]: Primary keys of successfully retrieved DataFile objects. Source code in archiving\\tasks.py @app.task(autoretry_for=(TooManyTasks, TAROffline), max_retries=None, retry_backoff=2*60, retry_backoff_max=5 * 60, retry_jitter=True, bind=True) def get_files_from_archived_tar_task(self: Any, tar_file_pk: int, target_file_pks: List[int]) -> List[int]: \"\"\" Retrieve specified files from a TAR archive, handling staging, extraction, and local placement. Args: self (Any): Task instance (provided by Celery when bind=True). tar_file_pk (int): Primary key of the TarFile object. target_file_pks (List[int]): List of file PKs to retrieve. Returns: List[int]: Primary keys of successfully retrieved DataFile objects. \"\"\" # Limit the number of simultaneous tasks of this type to avoid overloading resources check_simultaneous_tasks(self, 4) tar_file_obj = TarFile.objects.get(pk=tar_file_pk) file_objs = DataFile.objects.filter(pk__in=target_file_pks) file_names = file_objs.full_names().values_list(\"full_name\", flat=True) # Connect to the archive server via SSH archive_obj = tar_file_obj.archive ssh_client = archive_obj.init_ssh_client() # Try to locate the compressed TAR file first tar_name = tar_file_obj.name + '.tar.gz' tar_path = posixjoin(tar_file_obj.path, tar_name) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"dals -l {tar_path}\") # Check if the TAR file is online or needs to be staged from tape status_code, target_tar_status = check_tar_status(ssh_client, tar_path) logger.info(f\"{tar_path}: Get TAR status {status_code}\") # If not found, try without .tar.gz extension if status_code == 1: tar_name = tar_file_obj.name tar_path = posixjoin(tar_file_obj.path, tar_name) status_code, target_tar_status = check_tar_status(ssh_client, tar_path) logger.info(f\"{tar_path}: Get TAR status {status_code}\") if status_code == 1: raise Exception(f\"{tar_path}: TAR file not present at this path\") logger.info( f\"{tar_path}: Get TAR status {status_code} {target_tar_status}\") # If not online, try to retrieve from tape storage (or handle unmigrating state) online_statuses = ['(REG)', '(DUL)', '(MIG)'] if target_tar_status not in online_statuses: initial_offline = True logger.info(f\"{tar_path}: Offline\") if target_tar_status != '(UNM)': # Request data to be staged from tape status_code, stdout, stderr = ssh_client.send_ssh_command( f\"daget {tar_path}\") logger.info( f\"{tar_path}: Get TAR from tape {status_code} {stdout}\") status_code, target_tar_status = check_tar_status( ssh_client, tar_path) else: raise (TAROffline(f\"{tar_path}: already unmigrating\")) if target_tar_status not in online_statuses: # If still not online after staging attempt, raise error raise Exception(f\"{tar_path}: TAR file could not be staged\") else: initial_offline = False # Create a temporary extraction directory for this job temp_path = posixjoin(tar_file_obj.path, \"temp\", self.request.id) ftp_connection_success = ssh_client.connect_to_ftp() if not ftp_connection_success: raise Exception(\"Unable to connect to FTP\") ssh_client.mkdir_p(temp_path) # List files inside the TAR to locate the desired files status_code, stdout, stderr = ssh_client.send_ssh_command( f\"tar tvf {tar_path}\", return_strings=False) logger.info(f\"{tar_path}: List files in TAR\") in_tar_file_paths: List[str] = [] in_tar_found_files: List[Any] = [] for file_line in stdout: # Parse each line of tar output to extract the file path split_file_line = file_line.split(\" \") line_file_path = split_file_line[-1].replace(\"\\n\", \"\") # Check if this is one of our requested files found_file_paths = [x for x in file_names if x in line_file_path] if len(found_file_paths) > 0: in_tar_file_paths.append(line_file_path) in_tar_found_files.append(found_file_paths[0]) logger.info( f\"{tar_path}: {len(in_tar_file_paths)}/{len(file_names)}\") if len(in_tar_file_paths) == len(file_names): # Stop early if all target files found logger.info(f\"{tar_path}: All files_found\") break if len(in_tar_file_paths) == 0: # None of the requested files were found in the TAR archive raise Exception(f\"{tar_path}: No files found in TAR\") else: # Log any requested files not found in the archive missing_files = [x for x in file_names if x not in in_tar_found_files] if len(missing_files) > 0: logger.info(f\"{tar_path}: Files not found: {missing_files}\") # Extract files in manageable chunks to avoid command length limits chunked_in_tar_file_paths = [ x for x in divide_chunks(in_tar_file_paths, 500)] for idx, in_tar_file_paths_set in enumerate(chunked_in_tar_file_paths): logger.info( f\"{tar_path}: Extract file chunk {idx}/{len(chunked_in_tar_file_paths)}\") combined_in_tar_file_paths = ( \" \".join([f\"'{x}'\" for x in in_tar_file_paths_set])) status_code, stdout, stderr = ssh_client.send_ssh_command( f\"tar -zxvf {tar_path} -C {temp_path} {combined_in_tar_file_paths}\") logger.info( f\"{tar_path}: Extract file chunk {idx}/{len(chunked_in_tar_file_paths)} {status_code}\") # Connect to SCP for file transfer from archive to local storage ssh_client.connect_to_scp() file_objs_to_update: List[DataFile] = [] all_pks: List[int] = [] for idx, in_tar_file_path in enumerate(in_tar_file_paths): try: full_file_name = os.path.split(in_tar_file_path)[1] file_name = os.path.splitext(full_file_name)[0] # Get the corresponding DataFile object file_obj = file_objs.get(file_name=file_name) # Prepare the local storage directory local_dir = os.path.join(settings.FILE_STORAGE_ROOT, file_obj.path) os.makedirs(local_dir, exist_ok=True) local_file_path = os.path.join(local_dir, full_file_name) temp_file_path = posixjoin(temp_path, in_tar_file_path) if not os.path.exists(local_file_path): # Transfer file from archive temp path to local storage ssh_client.scp_c.get( temp_file_path, local_file_path, preserve_times=True) # Mark file as locally available and update metadata file_obj.modified_on = djtimezone.now() file_obj.local_path = settings.FILE_STORAGE_ROOT file_obj.local_storage = True file_objs_to_update.append(file_obj) all_pks.append(file_obj.pk) except Exception as e: # Log and continue on any per-file errors logger.info(f\"{tar_path}: Error retrieving file: {repr(e)}\") logger.info(f\"{tar_path}: Update database\") DataFile.objects.bulk_update(file_objs_to_update, fields=[ \"local_path\", \"local_storage\", \"modified_on\"]) logger.info(f\"{tar_path}: Clear temporary files\") # Remove temporary extraction files from the archive server status_code, stdout, stderr = ssh_client.send_ssh_command( f\"rm -rf {temp_path}\") ssh_client.close_connection() return all_pks","title":"get_files_from_archived_tar_task"},{"location":"reference/archiving/tasks/#archiving.tasks.post_get_file_from_archive_task","text":"After files are retrieved from archive, process them by sensor model and file format. Parameters: all_file_pks ( List [ List [ int ]] ) \u2013 Nested lists of file PKs. Source code in archiving\\tasks.py @app.task() def post_get_file_from_archive_task(all_file_pks: List[List[int]]) -> None: \"\"\" After files are retrieved from archive, process them by sensor model and file format. Args: all_file_pks (List[List[int]]): Nested lists of file PKs. \"\"\" # Flatten the list of lists of PKs into a single list all_file_pks_flat = [item for items in all_file_pks for item in items] # Query all DataFile objects just downloaded file_objs = DataFile.objects.filter(pk__in=all_file_pks_flat) # Find all unique device models (sensor models) present in these files device_models = file_objs.values_list( \"deployment__device__model\", flat=True).distinct() # For each unique device model, process files by format for device_model in device_models: sensor_model_file_objs = file_objs.filter( deployment__device__model=device_model) # Look up the handler for this model data_handler = settings.DATA_HANDLERS.get_handler( device_model.type.name, device_model.name) # Find all unique file formats for files from this model device_file_formats = sensor_model_file_objs.values_list( \"file_format\", flat=True).distinct() # For each file format, create and dispatch a post-download task if available for device_file_format in device_file_formats: device_model_format_file_objs = sensor_model_file_objs.filter( file_format=device_file_format) device_model_format_file_file_names = list( device_model_format_file_objs.values_list(\"file_name\", flat=True)) task_name = data_handler.get_post_download_task( device_file_format, False) if task_name is not None: new_task = app.signature( task_name, [device_model_format_file_file_names], immutable=True) new_task.apply_async() # Schedule the post-download processing","title":"post_get_file_from_archive_task"},{"location":"reference/camtrap_dp_export/","text":"This module provides functionality for exporting data from the sensor portal to Camtrap DP.","title":"camtrap_dp_export"},{"location":"reference/camtrap_dp_export/metadata_functions/","text":"create_camtrap_dp_metadata(file_qs, uuid='', title='') Create Camtrap-DP metadata and table dataframes for export. Parameters: file_qs ( QuerySet ) \u2013 Queryset of data files to export. uuid ( str , default: '' ) \u2013 Unique identifier for the dataset. Defaults to \"\". title ( str , default: '' ) \u2013 Title for the dataset. Defaults to \"\". Returns: DataFrame \u2013 Tuple[ DataFrame, # file_df: DataFrame of media files DataFrame, # observation_df: DataFrame of observations DataFrame, # deploy_df: DataFrame of deployments DataFrame, # event_df: DataFrame of events/sequences Dict[str, Any] # metadata: Metadata dictionary for Camtrap-DP DataFrame \u2013 ] Source code in camtrap_dp_export\\metadata_functions.py def create_camtrap_dp_metadata( file_qs: QuerySet, uuid: str = \"\", title: str = \"\", ) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame, Dict[str, Any]]: \"\"\" Create Camtrap-DP metadata and table dataframes for export. Args: file_qs (QuerySet): Queryset of data files to export. uuid (str, optional): Unique identifier for the dataset. Defaults to \"\". title (str, optional): Title for the dataset. Defaults to \"\". Returns: Tuple[ DataFrame, # file_df: DataFrame of media files DataFrame, # observation_df: DataFrame of observations DataFrame, # deploy_df: DataFrame of deployments DataFrame, # event_df: DataFrame of events/sequences Dict[str, Any] # metadata: Metadata dictionary for Camtrap-DP ] \"\"\" # get files file_qs = file_qs.distinct() file_qs = get_ctdp_media_qs(file_qs) # get deployments deployment_qs = Deployment.objects.filter(files__in=file_qs).distinct() deployment_qs = get_ctdp_deployment_qs(deployment_qs) # get observations observation_qs = Observation.objects.filter( data_files__in=file_qs).distinct() event_qs = get_ctdp_seq_qs(observation_qs) observation_qs = get_ctdp_obs_qs(observation_qs) project_qs = Project.objects.filter(deployments__in=deployment_qs).exclude( project_ID=settings.GLOBAL_PROJECT_ID ).distinct() principals = list( project_qs.values( \"principal_investigator\", \"principal_investigator_email\", \"organisation\" ) ) for x in principals: x[\"title\"] = x.pop(\"principal_investigator\") x[\"email\"] = x.pop(\"principal_investigator_email\") x.update({\"role\": \"principal_investigator\"}) x[\"organization\"] = x.pop(\"organisation\") contributors = list(project_qs.values( \"contact\", \"contact_email\", \"organisation\")) for x in contributors: x[\"title\"] = x.pop(\"contact\") x[\"email\"] = x.pop(\"contact_email\") x.update({\"role\": \"contributor\"}) x[\"organization\"] = x.pop(\"organisation\") all_contributors = principals + contributors all_contributors_distinct_title: List[Dict[str, Any]] = [] for x in all_contributors: if x[\"title\"] not in [y[\"title\"] for y in all_contributors_distinct_title]: all_contributors_distinct_title.append(x) # Get 4 dicts file_dict = DataFileSerializerCTDP(file_qs, many=True).data observation_dict = ObservationSerializerCTDP( observation_qs, many=True).data deploy_dict = DeploymentSerializerCTDP(deployment_qs, many=True).data event_dict = SequenceSerializer(event_qs, many=True).data file_df = pd.DataFrame.from_dict(file_dict) deploy_df = pd.DataFrame.from_dict(deploy_dict) if len(observation_dict) == 0: observation_dict = {x: [] for x in ObservationSerializerCTDP().get_fields().keys()} if len(event_dict) == 0: event_dict = {x: [] for x in SequenceSerializer().get_fields().keys()} observation_df = pd.DataFrame.from_dict(observation_dict) event_df = ( pd.DataFrame.from_dict(event_dict) .explode(\"mediaID\") .drop_duplicates([\"eventID\", \"mediaID\"]) ) project_dict = { \"title\": project_qs.first().name, \"description\": project_qs.first().objectives, \"samplingDesign\": \"systematic random\", \"captureMethod\": list(file_df.captureMethod.unique()), \"individualAnimals\": any([x is not None for x in observation_df.individualID]), \"observationLevel\": list(file_df.captureMethod.unique()), } points = deployment_qs.filter( point__isnull=False).values_list(\"point\", flat=True) all_points = MultiPoint(*points) hull = all_points.convex_hull spatial_dict = json.loads(hull.geojson) spatial_dict.update({\"bbox\": list(hull.extent)}) taxon_ids = observation_qs.values( \"taxon__species_name\", \"taxon__taxon_code\") taxon_dict: List[Dict[str, Union[str, None]]] = [] for x in taxon_ids: if x[\"taxon__taxon_code\"] != \"\": taxon_ID = f\"https://www.gbif.org/{x['taxon__taxon_code']}\" else: taxon_ID = None taxon_dict.append( {\"scientificName\": x[\"taxon__species_name\"], \"taxonID\": taxon_ID}) metadata: Dict[str, Any] = { \"resources\": [ { \"name\": \"deployments\", \"path\": \"deployments.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/deployments-table-schema.json\", }, { \"name\": \"media\", \"path\": \"media.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/media-table-schema.json\", }, { \"name\": \"observations\", \"path\": \"observations.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/observations-table-schema.json\", }, { \"name\": \"events\", \"path\": \"events.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"description\": \"Table of observation events, listing the media items that make up those events\", }, ], \"profile\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/camtrap-dp-profile.json\", \"name\": title, \"id\": uuid, \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \"title\": \"Camtrap DP dataset\", \"contributors\": all_contributors_distinct_title, \"description\": \"\", \"version\": \"1.0\", \"keywords\": [\"\"], \"image\": \"\", \"homepage\": \"\", \"sources\": [{\"title\": \"ARISE-MDS\"}], \"licenses\": [ {\"name\": \"CC0-1.0\", \"scope\": \"data\"}, {\"path\": \"http://creativecommons.org/licenses/by/4.0/\", \"scope\": \"media\"}, ], \"bibliographicCitation\": \"\", \"project\": project_dict, \"coordinatePrecision\": 0.00001, \"spatial\": spatial_dict, \"temporal\": { \"start\": pd.to_datetime(deploy_df[\"deploymentStart\"], format=\"%Y-%m-%dT%H:%M:%S%z\") .min() .date() .strftime(\"%Y-%m-%d\"), \"end\": pd.to_datetime(deploy_df[\"deploymentEnd\"], format=\"%Y-%m-%dT%H:%M:%S%z\") .max() .date() .strftime(\"%Y-%m-%d\"), }, \"taxonomic\": taxon_dict, \"relatedIdentifiers\": [], } return file_df, observation_df, deploy_df, event_df, metadata","title":"metadata_functions"},{"location":"reference/camtrap_dp_export/metadata_functions/#camtrap_dp_export.metadata_functions.create_camtrap_dp_metadata","text":"Create Camtrap-DP metadata and table dataframes for export. Parameters: file_qs ( QuerySet ) \u2013 Queryset of data files to export. uuid ( str , default: '' ) \u2013 Unique identifier for the dataset. Defaults to \"\". title ( str , default: '' ) \u2013 Title for the dataset. Defaults to \"\". Returns: DataFrame \u2013 Tuple[ DataFrame, # file_df: DataFrame of media files DataFrame, # observation_df: DataFrame of observations DataFrame, # deploy_df: DataFrame of deployments DataFrame, # event_df: DataFrame of events/sequences Dict[str, Any] # metadata: Metadata dictionary for Camtrap-DP DataFrame \u2013 ] Source code in camtrap_dp_export\\metadata_functions.py def create_camtrap_dp_metadata( file_qs: QuerySet, uuid: str = \"\", title: str = \"\", ) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame, Dict[str, Any]]: \"\"\" Create Camtrap-DP metadata and table dataframes for export. Args: file_qs (QuerySet): Queryset of data files to export. uuid (str, optional): Unique identifier for the dataset. Defaults to \"\". title (str, optional): Title for the dataset. Defaults to \"\". Returns: Tuple[ DataFrame, # file_df: DataFrame of media files DataFrame, # observation_df: DataFrame of observations DataFrame, # deploy_df: DataFrame of deployments DataFrame, # event_df: DataFrame of events/sequences Dict[str, Any] # metadata: Metadata dictionary for Camtrap-DP ] \"\"\" # get files file_qs = file_qs.distinct() file_qs = get_ctdp_media_qs(file_qs) # get deployments deployment_qs = Deployment.objects.filter(files__in=file_qs).distinct() deployment_qs = get_ctdp_deployment_qs(deployment_qs) # get observations observation_qs = Observation.objects.filter( data_files__in=file_qs).distinct() event_qs = get_ctdp_seq_qs(observation_qs) observation_qs = get_ctdp_obs_qs(observation_qs) project_qs = Project.objects.filter(deployments__in=deployment_qs).exclude( project_ID=settings.GLOBAL_PROJECT_ID ).distinct() principals = list( project_qs.values( \"principal_investigator\", \"principal_investigator_email\", \"organisation\" ) ) for x in principals: x[\"title\"] = x.pop(\"principal_investigator\") x[\"email\"] = x.pop(\"principal_investigator_email\") x.update({\"role\": \"principal_investigator\"}) x[\"organization\"] = x.pop(\"organisation\") contributors = list(project_qs.values( \"contact\", \"contact_email\", \"organisation\")) for x in contributors: x[\"title\"] = x.pop(\"contact\") x[\"email\"] = x.pop(\"contact_email\") x.update({\"role\": \"contributor\"}) x[\"organization\"] = x.pop(\"organisation\") all_contributors = principals + contributors all_contributors_distinct_title: List[Dict[str, Any]] = [] for x in all_contributors: if x[\"title\"] not in [y[\"title\"] for y in all_contributors_distinct_title]: all_contributors_distinct_title.append(x) # Get 4 dicts file_dict = DataFileSerializerCTDP(file_qs, many=True).data observation_dict = ObservationSerializerCTDP( observation_qs, many=True).data deploy_dict = DeploymentSerializerCTDP(deployment_qs, many=True).data event_dict = SequenceSerializer(event_qs, many=True).data file_df = pd.DataFrame.from_dict(file_dict) deploy_df = pd.DataFrame.from_dict(deploy_dict) if len(observation_dict) == 0: observation_dict = {x: [] for x in ObservationSerializerCTDP().get_fields().keys()} if len(event_dict) == 0: event_dict = {x: [] for x in SequenceSerializer().get_fields().keys()} observation_df = pd.DataFrame.from_dict(observation_dict) event_df = ( pd.DataFrame.from_dict(event_dict) .explode(\"mediaID\") .drop_duplicates([\"eventID\", \"mediaID\"]) ) project_dict = { \"title\": project_qs.first().name, \"description\": project_qs.first().objectives, \"samplingDesign\": \"systematic random\", \"captureMethod\": list(file_df.captureMethod.unique()), \"individualAnimals\": any([x is not None for x in observation_df.individualID]), \"observationLevel\": list(file_df.captureMethod.unique()), } points = deployment_qs.filter( point__isnull=False).values_list(\"point\", flat=True) all_points = MultiPoint(*points) hull = all_points.convex_hull spatial_dict = json.loads(hull.geojson) spatial_dict.update({\"bbox\": list(hull.extent)}) taxon_ids = observation_qs.values( \"taxon__species_name\", \"taxon__taxon_code\") taxon_dict: List[Dict[str, Union[str, None]]] = [] for x in taxon_ids: if x[\"taxon__taxon_code\"] != \"\": taxon_ID = f\"https://www.gbif.org/{x['taxon__taxon_code']}\" else: taxon_ID = None taxon_dict.append( {\"scientificName\": x[\"taxon__species_name\"], \"taxonID\": taxon_ID}) metadata: Dict[str, Any] = { \"resources\": [ { \"name\": \"deployments\", \"path\": \"deployments.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/deployments-table-schema.json\", }, { \"name\": \"media\", \"path\": \"media.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/media-table-schema.json\", }, { \"name\": \"observations\", \"path\": \"observations.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"schema\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/observations-table-schema.json\", }, { \"name\": \"events\", \"path\": \"events.csv\", \"profile\": \"tabular-data-resource\", \"format\": \"csv\", \"mediatype\": \"text/csv\", \"encoding\": \"utf-8\", \"description\": \"Table of observation events, listing the media items that make up those events\", }, ], \"profile\": \"https://raw.githubusercontent.com/tdwg/camtrap-dp/1.0/camtrap-dp-profile.json\", \"name\": title, \"id\": uuid, \"created\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \"title\": \"Camtrap DP dataset\", \"contributors\": all_contributors_distinct_title, \"description\": \"\", \"version\": \"1.0\", \"keywords\": [\"\"], \"image\": \"\", \"homepage\": \"\", \"sources\": [{\"title\": \"ARISE-MDS\"}], \"licenses\": [ {\"name\": \"CC0-1.0\", \"scope\": \"data\"}, {\"path\": \"http://creativecommons.org/licenses/by/4.0/\", \"scope\": \"media\"}, ], \"bibliographicCitation\": \"\", \"project\": project_dict, \"coordinatePrecision\": 0.00001, \"spatial\": spatial_dict, \"temporal\": { \"start\": pd.to_datetime(deploy_df[\"deploymentStart\"], format=\"%Y-%m-%dT%H:%M:%S%z\") .min() .date() .strftime(\"%Y-%m-%d\"), \"end\": pd.to_datetime(deploy_df[\"deploymentEnd\"], format=\"%Y-%m-%dT%H:%M:%S%z\") .max() .date() .strftime(\"%Y-%m-%d\"), }, \"taxonomic\": taxon_dict, \"relatedIdentifiers\": [], } return file_df, observation_df, deploy_df, event_df, metadata","title":"create_camtrap_dp_metadata"},{"location":"reference/camtrap_dp_export/querysets/","text":"get_ctdp_deployment_qs(qs) Annotate a deployment queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Deployment objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_deployment_qs(qs: _QS) -> _QS: \"\"\" Annotate a deployment queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for Deployment objects. Returns: QuerySet: Annotated queryset with additional CTDP fields. \"\"\" qs = qs.annotate( locationID=Case( When(extra_data__deploymentLocationID__isnull=False, then=Cast(KeyTextTransform('deploymentLocationID', 'extra_data'), output_field=CharField())), When(extra_data__locationID__isnull=False, then=Cast(KeyTextTransform('locationID', 'extra_data'), output_field=CharField())), default=F('deployment_ID'), output_field=CharField() ) ) qs = qs.annotate(setupBy=Case(When(owner__isnull=False, then=Concat(F('owner__first_name'), Value( ' '), F('owner__last_name'))), default=Value(''), output_field=CharField())) qs = qs.annotate( coordinateUncertainty=Case( When(extra_data__coordinateUncertainty__isnull=False, then=Cast(KeyTextTransform('coordinateUncertainty', 'extra_data'), output_field=FloatField())), When(extra_data__LatLongInaccuracy_cm__isnull=False, then=Cast(KeyTextTransform('LatLongInaccuracy_cm', 'extra_data'), output_field=FloatField()) / 100), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate(cameraID=F('device__device_ID')) qs = qs.annotate( cameraModel=Concat(F('device__model__manufacturer'), Value('-'), F('device__model__name')) ) qs = qs.annotate( cameraHeight=Case( When(extra_data__cameraHeight__isnull=False, then=Cast(KeyTextTransform('cameraHeight', 'extra_data'), output_field=FloatField())), When(extra_data__height_cm__isnull=False, then=Cast(KeyTextTransform('height_cm', 'extra_data'), output_field=FloatField()) / 100), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( cameraHeading=Case( When(extra_data__cameraHeading__isnull=False, then=Cast(KeyTextTransform('cameraHeading', 'extra_data'), output_field=IntegerField())), When(extra_data__Direction__isnull=False, then=Case( When( extra_data__Direction=\"NE\", then=Value(45)), When( extra_data__Direction=\"E\", then=Value(90)), When( extra_data__Direction=\"SE\", then=Value(135)), When( extra_data__Direction=\"S\", then=Value(180)), When( extra_data__Direction=\"SW\", then=Value(225)), When( extra_data__Direction=\"W\", then=Value(270)), When( extra_data__Direction=\"NW\", then=Value(315)), default=Value(None), output_field=IntegerField() ) ), default=Value(None), output_field=IntegerField() ) ) qs = qs.annotate( baitUse=Case( When(extra_data__baitUse__isnull=False, extra_data__baitUse=\"yes\", then=Value(True)), When(extra_data__Bait__isnull=False, extra_data__Bait=\"yes\", then=Value(True)), default=Value(False), output_field=BooleanField() ) ) qs = qs.annotate( habitatType=Case( When(extra_data__habitatType__isnull=False, then=F('extra_data__habitatType')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentTags=Case( When(extra_data__deploymentTags__isnull=False, then=F('extra_data__deploymentTags')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentGroups=Case( When(extra_data__deploymentGroups__isnull=False, then=F('extra_data__deploymentGroups')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentComments=Case( When(extra_data__deploymentComments__isnull=False, then=F('extra_data__deploymentComments_')), When(extra_data__comments__isnull=False, then=F('extra_data__comments')), When(extra_data__Comment__isnull=False, then=F('extra_data__comments')), default=Value(None), output_field=CharField() ) ) return qs get_ctdp_media_qs(qs) Annotate a media file queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for DataFile or similar media objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP media fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_media_qs(qs: _QS) -> _QS: \"\"\" Annotate a media file queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for DataFile or similar media objects. Returns: QuerySet: Annotated queryset with additional CTDP media fields. \"\"\" qs = qs.annotate( favorite=ExpressionWrapper( Q(favourite_of__isnull=False), output_field=BooleanField() ) ) qs = qs.annotate( captureMethod=Case( When(file_type__name__in=timelapse_types, then=Value(\"timeLapse\")), When(file_type__name=activity_types, then=Value(\"activityDetection\")), default=Value(\"\"), output_field=CharField() ) ) qs = qs.annotate( timestamp=Replace( Concat(F('recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) qs = qs.annotate( deploymentID=F('deployment__deployment_device_ID') ) qs = qs.annotate( filePath=Case( When(local_storage=True, then=Replace( 'file_url', Concat( F('file_name'), F('file_format')), Value('') )), When(local_storage=False, archived=True, then=Concat( Value('ARCHIVE/'), F('tar_file__name'), Value('/'), F('path') ) ), defaul=Value('') ) ) qs = qs.annotate( fileName=Concat(F('file_name'), F('file_format')) ) qs = qs.annotate( mediaComments=Case( When(extra_data__mediaComments__isnull=False, then=Cast(KeyTextTransform('mediaComments', 'extra_data'), output_field=CharField())), When(extra_data__comments__isnull=False, then=Cast(KeyTextTransform('comments', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( filePublic=Value(False) ) return qs get_ctdp_obs_qs(qs) Annotate an observation queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Observation objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP observation fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_obs_qs(qs: _QS) -> _QS: \"\"\" Annotate an observation queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for Observation objects. Returns: QuerySet: Annotated queryset with additional CTDP observation fields. \"\"\" qs = qs.annotate(nfiles=Count('data_files')) qs = qs.annotate( deploymentID=Min(F('data_files__deployment__deployment_device_ID'))) qs = qs.annotate( observationID=F('label')) qs = qs.annotate( mediaID=Case( When(nfiles=1, then=Min('data_files__file_name')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( eventIDA=Case( When(nfiles__gt=1, then=F('label')), default=Value(None), output_field=CharField() )) qs = qs.annotate( eventID=Case( When(nfiles__gt=1, then=Concat(F('eventIDA'), Value('_EVENT'))), default=Value(None), output_field=CharField() )) qs = qs.annotate( observationLevel=Case( When(nfiles__gt=1, then=Value('event')), default=Value('media'), output_field=CharField() )) qs = qs.annotate( observationType=Case( When(taxon__taxon_code=settings.HUMAN_TAXON_CODE, then=Value('human')), When(taxon__species_name__in=[ 'car', 'van', 'vehicle', 'plane'], then=Value('vehicle')), When(taxon__species_name__in=[ 'blank', 'empty', 'None', 'No detection'], then=Value('blank')), When(taxon__species_name__in=[ 'unknown'], then=Value('unknown')), default=Value('animal'), output_field=CharField())) qs = qs.annotate( scientificName=Case( When(taxon__taxon_code=\"\", then=Value(None)), default=F('taxon__species_name'), output_field=CharField() )) qs = qs.annotate( classificationMethod=Case( When(source='human', then=Value('human')), default=Value('machine'), output_field=CharField() )) qs = qs.annotate( individualID=Case( When(extra_data__individualID__isnull=False, then=Cast(KeyTextTransform('individualID', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() )) qs = qs.annotate( bboxX=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__x1'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxY=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__y1'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxX2=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__x2'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxY2=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__y2'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxWidth=Case( When(bounding_box__x1__isnull=False, then=F('bboxX2')-F('bboxX')), default=Value(None), output_field=FloatField() ), bboxHeight=Case( When(bounding_box__x1__isnull=False, then=F('bboxY2')-F('bboxY')), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( classifiedBy=Case( When(owner__isnull=False, then=Concat(F('owner__first_name'), Value( ' '), F('owner__last_name'))), default=F('source'), output_field=CharField() ) ) qs = qs.annotate( classificationProbability=Case( When(confidence__isnull=False, then=F('confidence')), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( observationComments=Case( When(extra_data__observationComments__isnull=False, then=Cast(KeyTextTransform('observationComments', 'extra_data'), output_field=CharField())), When(extra_data__comments__isnull=False, then=Cast(KeyTextTransform('comments', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() )) qs = qs.annotate( eventStart=Case( When(nfiles__gt=1, then=Replace( Concat(Min('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") )), default=Value(None), output_field=CharField() ), eventEnd=Case( When(nfiles__gt=1, then=Replace( Concat(Max('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") )), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( lifeStage=F('lifestage'), count=F('number') ) return qs get_ctdp_seq_qs(qs) Annotate an observation queryset to represent event/sequence records for CTDP export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Observation objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset filtered for sequence/event records with additional event fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_seq_qs(qs: _QS) -> _QS: \"\"\" Annotate an observation queryset to represent event/sequence records for CTDP export. Args: qs (QuerySet): A Django queryset for Observation objects. Returns: QuerySet: Annotated queryset filtered for sequence/event records with additional event fields. \"\"\" qs = qs.distinct() qs = qs.annotate(nfiles=Count('data_files')).filter(nfiles__gt=1) qs = qs.annotate( eventIDA=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=F('label')) ), eventID=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Concat(F('eventIDA'), Value('_EVENT')) ) ), eventStart=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Replace( Concat(Min('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) ), eventEnd=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Replace( Concat(Max('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) ), ) return qs","title":"querysets"},{"location":"reference/camtrap_dp_export/querysets/#camtrap_dp_export.querysets.get_ctdp_deployment_qs","text":"Annotate a deployment queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Deployment objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_deployment_qs(qs: _QS) -> _QS: \"\"\" Annotate a deployment queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for Deployment objects. Returns: QuerySet: Annotated queryset with additional CTDP fields. \"\"\" qs = qs.annotate( locationID=Case( When(extra_data__deploymentLocationID__isnull=False, then=Cast(KeyTextTransform('deploymentLocationID', 'extra_data'), output_field=CharField())), When(extra_data__locationID__isnull=False, then=Cast(KeyTextTransform('locationID', 'extra_data'), output_field=CharField())), default=F('deployment_ID'), output_field=CharField() ) ) qs = qs.annotate(setupBy=Case(When(owner__isnull=False, then=Concat(F('owner__first_name'), Value( ' '), F('owner__last_name'))), default=Value(''), output_field=CharField())) qs = qs.annotate( coordinateUncertainty=Case( When(extra_data__coordinateUncertainty__isnull=False, then=Cast(KeyTextTransform('coordinateUncertainty', 'extra_data'), output_field=FloatField())), When(extra_data__LatLongInaccuracy_cm__isnull=False, then=Cast(KeyTextTransform('LatLongInaccuracy_cm', 'extra_data'), output_field=FloatField()) / 100), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate(cameraID=F('device__device_ID')) qs = qs.annotate( cameraModel=Concat(F('device__model__manufacturer'), Value('-'), F('device__model__name')) ) qs = qs.annotate( cameraHeight=Case( When(extra_data__cameraHeight__isnull=False, then=Cast(KeyTextTransform('cameraHeight', 'extra_data'), output_field=FloatField())), When(extra_data__height_cm__isnull=False, then=Cast(KeyTextTransform('height_cm', 'extra_data'), output_field=FloatField()) / 100), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( cameraHeading=Case( When(extra_data__cameraHeading__isnull=False, then=Cast(KeyTextTransform('cameraHeading', 'extra_data'), output_field=IntegerField())), When(extra_data__Direction__isnull=False, then=Case( When( extra_data__Direction=\"NE\", then=Value(45)), When( extra_data__Direction=\"E\", then=Value(90)), When( extra_data__Direction=\"SE\", then=Value(135)), When( extra_data__Direction=\"S\", then=Value(180)), When( extra_data__Direction=\"SW\", then=Value(225)), When( extra_data__Direction=\"W\", then=Value(270)), When( extra_data__Direction=\"NW\", then=Value(315)), default=Value(None), output_field=IntegerField() ) ), default=Value(None), output_field=IntegerField() ) ) qs = qs.annotate( baitUse=Case( When(extra_data__baitUse__isnull=False, extra_data__baitUse=\"yes\", then=Value(True)), When(extra_data__Bait__isnull=False, extra_data__Bait=\"yes\", then=Value(True)), default=Value(False), output_field=BooleanField() ) ) qs = qs.annotate( habitatType=Case( When(extra_data__habitatType__isnull=False, then=F('extra_data__habitatType')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentTags=Case( When(extra_data__deploymentTags__isnull=False, then=F('extra_data__deploymentTags')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentGroups=Case( When(extra_data__deploymentGroups__isnull=False, then=F('extra_data__deploymentGroups')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( deploymentComments=Case( When(extra_data__deploymentComments__isnull=False, then=F('extra_data__deploymentComments_')), When(extra_data__comments__isnull=False, then=F('extra_data__comments')), When(extra_data__Comment__isnull=False, then=F('extra_data__comments')), default=Value(None), output_field=CharField() ) ) return qs","title":"get_ctdp_deployment_qs"},{"location":"reference/camtrap_dp_export/querysets/#camtrap_dp_export.querysets.get_ctdp_media_qs","text":"Annotate a media file queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for DataFile or similar media objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP media fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_media_qs(qs: _QS) -> _QS: \"\"\" Annotate a media file queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for DataFile or similar media objects. Returns: QuerySet: Annotated queryset with additional CTDP media fields. \"\"\" qs = qs.annotate( favorite=ExpressionWrapper( Q(favourite_of__isnull=False), output_field=BooleanField() ) ) qs = qs.annotate( captureMethod=Case( When(file_type__name__in=timelapse_types, then=Value(\"timeLapse\")), When(file_type__name=activity_types, then=Value(\"activityDetection\")), default=Value(\"\"), output_field=CharField() ) ) qs = qs.annotate( timestamp=Replace( Concat(F('recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) qs = qs.annotate( deploymentID=F('deployment__deployment_device_ID') ) qs = qs.annotate( filePath=Case( When(local_storage=True, then=Replace( 'file_url', Concat( F('file_name'), F('file_format')), Value('') )), When(local_storage=False, archived=True, then=Concat( Value('ARCHIVE/'), F('tar_file__name'), Value('/'), F('path') ) ), defaul=Value('') ) ) qs = qs.annotate( fileName=Concat(F('file_name'), F('file_format')) ) qs = qs.annotate( mediaComments=Case( When(extra_data__mediaComments__isnull=False, then=Cast(KeyTextTransform('mediaComments', 'extra_data'), output_field=CharField())), When(extra_data__comments__isnull=False, then=Cast(KeyTextTransform('comments', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( filePublic=Value(False) ) return qs","title":"get_ctdp_media_qs"},{"location":"reference/camtrap_dp_export/querysets/#camtrap_dp_export.querysets.get_ctdp_obs_qs","text":"Annotate an observation queryset with fields formatted for Camtrap Data Package (CTDP) export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Observation objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset with additional CTDP observation fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_obs_qs(qs: _QS) -> _QS: \"\"\" Annotate an observation queryset with fields formatted for Camtrap Data Package (CTDP) export. Args: qs (QuerySet): A Django queryset for Observation objects. Returns: QuerySet: Annotated queryset with additional CTDP observation fields. \"\"\" qs = qs.annotate(nfiles=Count('data_files')) qs = qs.annotate( deploymentID=Min(F('data_files__deployment__deployment_device_ID'))) qs = qs.annotate( observationID=F('label')) qs = qs.annotate( mediaID=Case( When(nfiles=1, then=Min('data_files__file_name')), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( eventIDA=Case( When(nfiles__gt=1, then=F('label')), default=Value(None), output_field=CharField() )) qs = qs.annotate( eventID=Case( When(nfiles__gt=1, then=Concat(F('eventIDA'), Value('_EVENT'))), default=Value(None), output_field=CharField() )) qs = qs.annotate( observationLevel=Case( When(nfiles__gt=1, then=Value('event')), default=Value('media'), output_field=CharField() )) qs = qs.annotate( observationType=Case( When(taxon__taxon_code=settings.HUMAN_TAXON_CODE, then=Value('human')), When(taxon__species_name__in=[ 'car', 'van', 'vehicle', 'plane'], then=Value('vehicle')), When(taxon__species_name__in=[ 'blank', 'empty', 'None', 'No detection'], then=Value('blank')), When(taxon__species_name__in=[ 'unknown'], then=Value('unknown')), default=Value('animal'), output_field=CharField())) qs = qs.annotate( scientificName=Case( When(taxon__taxon_code=\"\", then=Value(None)), default=F('taxon__species_name'), output_field=CharField() )) qs = qs.annotate( classificationMethod=Case( When(source='human', then=Value('human')), default=Value('machine'), output_field=CharField() )) qs = qs.annotate( individualID=Case( When(extra_data__individualID__isnull=False, then=Cast(KeyTextTransform('individualID', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() )) qs = qs.annotate( bboxX=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__x1'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxY=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__y1'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxX2=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__x2'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxY2=Case( When(bounding_box__x1__isnull=False, then=Cast( Cast(F('bounding_box__y2'), CharField()), FloatField())), default=Value(None), output_field=FloatField() ), bboxWidth=Case( When(bounding_box__x1__isnull=False, then=F('bboxX2')-F('bboxX')), default=Value(None), output_field=FloatField() ), bboxHeight=Case( When(bounding_box__x1__isnull=False, then=F('bboxY2')-F('bboxY')), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( classifiedBy=Case( When(owner__isnull=False, then=Concat(F('owner__first_name'), Value( ' '), F('owner__last_name'))), default=F('source'), output_field=CharField() ) ) qs = qs.annotate( classificationProbability=Case( When(confidence__isnull=False, then=F('confidence')), default=Value(None), output_field=FloatField() ) ) qs = qs.annotate( observationComments=Case( When(extra_data__observationComments__isnull=False, then=Cast(KeyTextTransform('observationComments', 'extra_data'), output_field=CharField())), When(extra_data__comments__isnull=False, then=Cast(KeyTextTransform('comments', 'extra_data'), output_field=CharField())), default=Value(None), output_field=CharField() )) qs = qs.annotate( eventStart=Case( When(nfiles__gt=1, then=Replace( Concat(Min('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") )), default=Value(None), output_field=CharField() ), eventEnd=Case( When(nfiles__gt=1, then=Replace( Concat(Max('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") )), default=Value(None), output_field=CharField() ) ) qs = qs.annotate( lifeStage=F('lifestage'), count=F('number') ) return qs","title":"get_ctdp_obs_qs"},{"location":"reference/camtrap_dp_export/querysets/#camtrap_dp_export.querysets.get_ctdp_seq_qs","text":"Annotate an observation queryset to represent event/sequence records for CTDP export. Parameters: qs ( QuerySet ) \u2013 A Django queryset for Observation objects. Returns: QuerySet ( _QS ) \u2013 Annotated queryset filtered for sequence/event records with additional event fields. Source code in camtrap_dp_export\\querysets.py def get_ctdp_seq_qs(qs: _QS) -> _QS: \"\"\" Annotate an observation queryset to represent event/sequence records for CTDP export. Args: qs (QuerySet): A Django queryset for Observation objects. Returns: QuerySet: Annotated queryset filtered for sequence/event records with additional event fields. \"\"\" qs = qs.distinct() qs = qs.annotate(nfiles=Count('data_files')).filter(nfiles__gt=1) qs = qs.annotate( eventIDA=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=F('label')) ), eventID=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Concat(F('eventIDA'), Value('_EVENT')) ) ), eventStart=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Replace( Concat(Min('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) ), eventEnd=Case( When(nfiles__lte=1, then=Value(None)), When(nfiles__gt=1, then=Replace( Concat(Max('data_files__recording_dt'), Value('Z'), output_field=CharField()), Value(\" \"), Value(\"T\") ) ) ), ) return qs","title":"get_ctdp_seq_qs"},{"location":"reference/camtrap_dp_export/serializers/","text":"DataFileSerializerCTDP Bases: Serializer Serializer for representing media file metadata in the Camtrap DP export format. Maps file fields to the expected Camtrap DP schema, including media identifiers, file paths, MIME types, and additional comments. Source code in camtrap_dp_export\\serializers.py class DataFileSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing media file metadata in the Camtrap DP export format. Maps file fields to the expected Camtrap DP schema, including media identifiers, file paths, MIME types, and additional comments. \"\"\" mediaID = serializers.CharField( source='file_name', help_text=\"Unique identifier for the media file (Camtrap DP: mediaID).\" ) deploymentID = serializers.CharField( help_text=\"Identifier for the deployment associated with this media (Camtrap DP: deploymentID).\" ) captureMethod = serializers.CharField( help_text=\"Method used to capture the media (e.g., motion detection, time lapse) (Camtrap DP: captureMethod).\" ) timestamp = serializers.CharField( help_text=\"Date and time when the media was recorded (Camtrap DP: timestamp, ISO 8601).\" ) filePath = serializers.CharField( help_text=\"File path to the media file (Camtrap DP: filePath).\" ) fileName = serializers.CharField( help_text=\"Name of the media file (Camtrap DP: fileName).\" ) filePublic = serializers.CharField( help_text=\"Indicates if the file is publicly accessible (Camtrap DP: filePublic).\" ) fileMediatype = serializers.SerializerMethodField( help_text=\"MIME type of the media file, e.g., image/jpeg, video/mp4 (Camtrap DP: fileMediatype).\" ) favorite = serializers.BooleanField( help_text=\"Indicates if the file is marked as a favorite (not a standard Camtrap DP field, custom).\" ) mediaComments = serializers.CharField( help_text=\"Comments or notes about the media file (Camtrap DP: mediaComments).\" ) def get_fileMediatype(self, obj): \"\"\" Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. \"\"\" return mimetypes.guess_type(obj.fileName)[0] get_fileMediatype(obj) Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. Source code in camtrap_dp_export\\serializers.py def get_fileMediatype(self, obj): \"\"\" Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. \"\"\" return mimetypes.guess_type(obj.fileName)[0] DeploymentSerializerCTDP Bases: Serializer Serializer for representing deployment metadata in the Camtrap DP export format. Maps deployment fields to the expected Camtrap DP schema, including location, coordinates, camera details, and deployment attributes. Source code in camtrap_dp_export\\serializers.py class DeploymentSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing deployment metadata in the Camtrap DP export format. Maps deployment fields to the expected Camtrap DP schema, including location, coordinates, camera details, and deployment attributes. \"\"\" deploymentID = serializers.CharField( source='deployment_device_ID', help_text=\"Unique identifier for this deployment (required by Camtrap DP: deploymentID).\" ) locationID = serializers.CharField( help_text=\"Identifier for the location where the deployment occurs (required by Camtrap DP: locationID).\" ) latitude = serializers.DecimalField( max_digits=8, decimal_places=6, help_text=\"Latitude of the deployment in decimal degrees (required by Camtrap DP: latitude).\" ) longitude = serializers.DecimalField( max_digits=8, decimal_places=6, help_text=\"Longitude of the deployment in decimal degrees (required by Camtrap DP: longitude).\" ) deploymentStart = serializers.DateTimeField( source=\"deployment_start\", format=\"%Y-%m-%dT%H:%M:%S%z\", help_text=\"Date and time when the deployment started (required by Camtrap DP: deploymentStart, ISO 8601).\" ) deploymentEnd = serializers.DateTimeField( source=\"deployment_start\", format=\"%Y-%m-%dT%H:%M:%S%z\", help_text=\"Date and time when the deployment ended (required by Camtrap DP: deploymentEnd, ISO 8601).\" ) setupBy = serializers.CharField( help_text=\"Person or organization responsible for setting up the deployment (Camtrap DP: setupBy).\" ) cameraID = serializers.CharField( help_text=\"Identifier for the camera trap device used in this deployment (Camtrap DP: cameraID).\" ) cameraModel = serializers.CharField( help_text=\"Manufacturer's model name or number of the camera trap device (Camtrap DP: cameraModel).\" ) coordinateUncertainty = serializers.FloatField( help_text=\"Uncertainty in the spatial coordinates, in meters (Camtrap DP: coordinateUncertainty).\" ) cameraHeight = serializers.FloatField( help_text=\"Height of the camera above ground in meters (Camtrap DP: cameraHeight).\" ) cameraHeading = serializers.IntegerField( help_text=\"Compass direction the camera is facing in degrees (Camtrap DP: cameraHeading).\" ) baitUse = serializers.BooleanField( help_text=\"Whether bait was used at the deployment (Camtrap DP: baitUse).\" ) habitatType = serializers.CharField( help_text=\"Type of habitat at the deployment location (Camtrap DP: habitatType).\" ) deploymentGroups = serializers.CharField( help_text=\"Group names or categories associated with this deployment (Camtrap DP: deploymentGroups).\" ) deploymentTags = serializers.CharField( help_text=\"Tags to annotate this deployment (Camtrap DP: deploymentTags).\" ) deploymentComments = serializers.CharField( help_text=\"Free-form comments about the deployment (Camtrap DP: deploymentComments).\" ) ObservationSerializerCTDP Bases: Serializer Serializer for representing observation metadata in the Camtrap DP export format. Maps observation fields to the expected Camtrap DP schema, including event, taxonomic, and detection details. Source code in camtrap_dp_export\\serializers.py class ObservationSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing observation metadata in the Camtrap DP export format. Maps observation fields to the expected Camtrap DP schema, including event, taxonomic, and detection details. \"\"\" observationID = serializers.CharField( help_text=\"Unique identifier for this observation (Camtrap DP: observationID).\" ) deploymentID = serializers.CharField( help_text=\"Deployment identifier associated with this observation (Camtrap DP: deploymentID).\" ) mediaID = serializers.CharField( allow_null=True, help_text=\"Media identifier related to this observation, if any (Camtrap DP: mediaID).\" ) eventID = serializers.CharField( allow_null=True, help_text=\"Identifier for the event grouping observations (Camtrap DP: eventID).\" ) eventStart = serializers.CharField( allow_null=True, help_text=\"Start time of the event (Camtrap DP: eventStart, ISO 8601).\" ) eventEnd = serializers.CharField( allow_null=True, help_text=\"End time of the event (Camtrap DP: eventEnd, ISO 8601).\" ) observationLevel = serializers.CharField( help_text=\"Level of observation (e.g. image, sequence, deployment) (Camtrap DP: observationLevel).\" ) observationType = serializers.CharField( help_text=\"Type of observation (e.g. species, blank, unclassified) (Camtrap DP: observationType).\" ) scientificName = serializers.CharField( allow_null=True, help_text=\"Scientific name of the observed taxon (Camtrap DP: scientificName).\" ) count = serializers.IntegerField( help_text=\"Number of individuals observed (Camtrap DP: count).\" ) lifeStage = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Life stage of the observed organism, if known (Camtrap DP: lifeStage).\" ) sex = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Sex of the observed organism, if known (Camtrap DP: sex).\" ) behavior = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Observed behavior(s) (Camtrap DP: behavior).\" ) individualID = serializers.CharField( allow_null=True, help_text=\"Identifier for the individual, if known (Camtrap DP: individualID).\" ) bboxX = serializers.FloatField( allow_null=True, help_text=\"Bounding box X coordinate for detection (Camtrap DP: bboxX).\" ) bboxY = serializers.FloatField( allow_null=True, help_text=\"Bounding box Y coordinate for detection (Camtrap DP: bboxY).\" ) bboxWidth = serializers.FloatField( allow_null=True, help_text=\"Bounding box width for detection (Camtrap DP: bboxWidth).\" ) bboxHeight = serializers.FloatField( allow_null=True, help_text=\"Bounding box height for detection (Camtrap DP: bboxHeight).\" ) classificationMethod = serializers.CharField( help_text=\"Method used for classification (Camtrap DP: classificationMethod).\" ) classifiedBy = serializers.CharField( allow_null=True, help_text=\"Person, software, or device that made the classification (Camtrap DP: classifiedBy).\" ) classificationProbability = serializers.FloatField( allow_null=True, help_text=\"Probability/score of the classification, if available (Camtrap DP: classificationProbability).\" ) observationComments = serializers.CharField( allow_null=True, help_text=\"Comments or notes about the observation (Camtrap DP: observationComments).\" ) SequenceSerializer Bases: Serializer Serializer for representing sequence (event) metadata in the Camtrap DP export format. Maps sequence fields to the expected Camtrap DP schema, including event identifiers, media associations, and timing details. Source code in camtrap_dp_export\\serializers.py class SequenceSerializer(serializers.Serializer): \"\"\" Serializer for representing sequence (event) metadata in the Camtrap DP export format. Maps sequence fields to the expected Camtrap DP schema, including event identifiers, media associations, and timing details. \"\"\" eventID = serializers.CharField( help_text=\"Identifier for the sequence event (Camtrap DP: eventID).\" ) mediaID = serializers.SerializerMethodField( help_text=\"List of media identifiers associated with this event (Camtrap DP: mediaID).\" ) mediaCount = serializers.IntegerField( source='nfiles', help_text=\"Total number of media files in this event (Camtrap DP: mediaCount).\" ) eventStart = serializers.CharField( help_text=\"Start time of the event/sequence (Camtrap DP: eventStart, ISO 8601).\" ) eventEnd = serializers.CharField( help_text=\"End time of the event/sequence (Camtrap DP: eventEnd, ISO 8601).\" ) mediaID = serializers.SlugRelatedField( queryset=DataFile.objects.all(), many=True, source=\"data_files\", slug_field='file_name', help_text=\"Media files associated with this sequence (Camtrap DP: mediaID).\" )","title":"serializers"},{"location":"reference/camtrap_dp_export/serializers/#camtrap_dp_export.serializers.DataFileSerializerCTDP","text":"Bases: Serializer Serializer for representing media file metadata in the Camtrap DP export format. Maps file fields to the expected Camtrap DP schema, including media identifiers, file paths, MIME types, and additional comments. Source code in camtrap_dp_export\\serializers.py class DataFileSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing media file metadata in the Camtrap DP export format. Maps file fields to the expected Camtrap DP schema, including media identifiers, file paths, MIME types, and additional comments. \"\"\" mediaID = serializers.CharField( source='file_name', help_text=\"Unique identifier for the media file (Camtrap DP: mediaID).\" ) deploymentID = serializers.CharField( help_text=\"Identifier for the deployment associated with this media (Camtrap DP: deploymentID).\" ) captureMethod = serializers.CharField( help_text=\"Method used to capture the media (e.g., motion detection, time lapse) (Camtrap DP: captureMethod).\" ) timestamp = serializers.CharField( help_text=\"Date and time when the media was recorded (Camtrap DP: timestamp, ISO 8601).\" ) filePath = serializers.CharField( help_text=\"File path to the media file (Camtrap DP: filePath).\" ) fileName = serializers.CharField( help_text=\"Name of the media file (Camtrap DP: fileName).\" ) filePublic = serializers.CharField( help_text=\"Indicates if the file is publicly accessible (Camtrap DP: filePublic).\" ) fileMediatype = serializers.SerializerMethodField( help_text=\"MIME type of the media file, e.g., image/jpeg, video/mp4 (Camtrap DP: fileMediatype).\" ) favorite = serializers.BooleanField( help_text=\"Indicates if the file is marked as a favorite (not a standard Camtrap DP field, custom).\" ) mediaComments = serializers.CharField( help_text=\"Comments or notes about the media file (Camtrap DP: mediaComments).\" ) def get_fileMediatype(self, obj): \"\"\" Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. \"\"\" return mimetypes.guess_type(obj.fileName)[0]","title":"DataFileSerializerCTDP"},{"location":"reference/camtrap_dp_export/serializers/#camtrap_dp_export.serializers.DataFileSerializerCTDP.get_fileMediatype","text":"Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. Source code in camtrap_dp_export\\serializers.py def get_fileMediatype(self, obj): \"\"\" Guess the MIME type of the file based on its name. Args: obj: The object instance being serialized. Returns: str or None: The guessed MIME type, or None if unknown. \"\"\" return mimetypes.guess_type(obj.fileName)[0]","title":"get_fileMediatype"},{"location":"reference/camtrap_dp_export/serializers/#camtrap_dp_export.serializers.DeploymentSerializerCTDP","text":"Bases: Serializer Serializer for representing deployment metadata in the Camtrap DP export format. Maps deployment fields to the expected Camtrap DP schema, including location, coordinates, camera details, and deployment attributes. Source code in camtrap_dp_export\\serializers.py class DeploymentSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing deployment metadata in the Camtrap DP export format. Maps deployment fields to the expected Camtrap DP schema, including location, coordinates, camera details, and deployment attributes. \"\"\" deploymentID = serializers.CharField( source='deployment_device_ID', help_text=\"Unique identifier for this deployment (required by Camtrap DP: deploymentID).\" ) locationID = serializers.CharField( help_text=\"Identifier for the location where the deployment occurs (required by Camtrap DP: locationID).\" ) latitude = serializers.DecimalField( max_digits=8, decimal_places=6, help_text=\"Latitude of the deployment in decimal degrees (required by Camtrap DP: latitude).\" ) longitude = serializers.DecimalField( max_digits=8, decimal_places=6, help_text=\"Longitude of the deployment in decimal degrees (required by Camtrap DP: longitude).\" ) deploymentStart = serializers.DateTimeField( source=\"deployment_start\", format=\"%Y-%m-%dT%H:%M:%S%z\", help_text=\"Date and time when the deployment started (required by Camtrap DP: deploymentStart, ISO 8601).\" ) deploymentEnd = serializers.DateTimeField( source=\"deployment_start\", format=\"%Y-%m-%dT%H:%M:%S%z\", help_text=\"Date and time when the deployment ended (required by Camtrap DP: deploymentEnd, ISO 8601).\" ) setupBy = serializers.CharField( help_text=\"Person or organization responsible for setting up the deployment (Camtrap DP: setupBy).\" ) cameraID = serializers.CharField( help_text=\"Identifier for the camera trap device used in this deployment (Camtrap DP: cameraID).\" ) cameraModel = serializers.CharField( help_text=\"Manufacturer's model name or number of the camera trap device (Camtrap DP: cameraModel).\" ) coordinateUncertainty = serializers.FloatField( help_text=\"Uncertainty in the spatial coordinates, in meters (Camtrap DP: coordinateUncertainty).\" ) cameraHeight = serializers.FloatField( help_text=\"Height of the camera above ground in meters (Camtrap DP: cameraHeight).\" ) cameraHeading = serializers.IntegerField( help_text=\"Compass direction the camera is facing in degrees (Camtrap DP: cameraHeading).\" ) baitUse = serializers.BooleanField( help_text=\"Whether bait was used at the deployment (Camtrap DP: baitUse).\" ) habitatType = serializers.CharField( help_text=\"Type of habitat at the deployment location (Camtrap DP: habitatType).\" ) deploymentGroups = serializers.CharField( help_text=\"Group names or categories associated with this deployment (Camtrap DP: deploymentGroups).\" ) deploymentTags = serializers.CharField( help_text=\"Tags to annotate this deployment (Camtrap DP: deploymentTags).\" ) deploymentComments = serializers.CharField( help_text=\"Free-form comments about the deployment (Camtrap DP: deploymentComments).\" )","title":"DeploymentSerializerCTDP"},{"location":"reference/camtrap_dp_export/serializers/#camtrap_dp_export.serializers.ObservationSerializerCTDP","text":"Bases: Serializer Serializer for representing observation metadata in the Camtrap DP export format. Maps observation fields to the expected Camtrap DP schema, including event, taxonomic, and detection details. Source code in camtrap_dp_export\\serializers.py class ObservationSerializerCTDP(serializers.Serializer): \"\"\" Serializer for representing observation metadata in the Camtrap DP export format. Maps observation fields to the expected Camtrap DP schema, including event, taxonomic, and detection details. \"\"\" observationID = serializers.CharField( help_text=\"Unique identifier for this observation (Camtrap DP: observationID).\" ) deploymentID = serializers.CharField( help_text=\"Deployment identifier associated with this observation (Camtrap DP: deploymentID).\" ) mediaID = serializers.CharField( allow_null=True, help_text=\"Media identifier related to this observation, if any (Camtrap DP: mediaID).\" ) eventID = serializers.CharField( allow_null=True, help_text=\"Identifier for the event grouping observations (Camtrap DP: eventID).\" ) eventStart = serializers.CharField( allow_null=True, help_text=\"Start time of the event (Camtrap DP: eventStart, ISO 8601).\" ) eventEnd = serializers.CharField( allow_null=True, help_text=\"End time of the event (Camtrap DP: eventEnd, ISO 8601).\" ) observationLevel = serializers.CharField( help_text=\"Level of observation (e.g. image, sequence, deployment) (Camtrap DP: observationLevel).\" ) observationType = serializers.CharField( help_text=\"Type of observation (e.g. species, blank, unclassified) (Camtrap DP: observationType).\" ) scientificName = serializers.CharField( allow_null=True, help_text=\"Scientific name of the observed taxon (Camtrap DP: scientificName).\" ) count = serializers.IntegerField( help_text=\"Number of individuals observed (Camtrap DP: count).\" ) lifeStage = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Life stage of the observed organism, if known (Camtrap DP: lifeStage).\" ) sex = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Sex of the observed organism, if known (Camtrap DP: sex).\" ) behavior = serializers.CharField( allow_null=True, allow_blank=True, help_text=\"Observed behavior(s) (Camtrap DP: behavior).\" ) individualID = serializers.CharField( allow_null=True, help_text=\"Identifier for the individual, if known (Camtrap DP: individualID).\" ) bboxX = serializers.FloatField( allow_null=True, help_text=\"Bounding box X coordinate for detection (Camtrap DP: bboxX).\" ) bboxY = serializers.FloatField( allow_null=True, help_text=\"Bounding box Y coordinate for detection (Camtrap DP: bboxY).\" ) bboxWidth = serializers.FloatField( allow_null=True, help_text=\"Bounding box width for detection (Camtrap DP: bboxWidth).\" ) bboxHeight = serializers.FloatField( allow_null=True, help_text=\"Bounding box height for detection (Camtrap DP: bboxHeight).\" ) classificationMethod = serializers.CharField( help_text=\"Method used for classification (Camtrap DP: classificationMethod).\" ) classifiedBy = serializers.CharField( allow_null=True, help_text=\"Person, software, or device that made the classification (Camtrap DP: classifiedBy).\" ) classificationProbability = serializers.FloatField( allow_null=True, help_text=\"Probability/score of the classification, if available (Camtrap DP: classificationProbability).\" ) observationComments = serializers.CharField( allow_null=True, help_text=\"Comments or notes about the observation (Camtrap DP: observationComments).\" )","title":"ObservationSerializerCTDP"},{"location":"reference/camtrap_dp_export/serializers/#camtrap_dp_export.serializers.SequenceSerializer","text":"Bases: Serializer Serializer for representing sequence (event) metadata in the Camtrap DP export format. Maps sequence fields to the expected Camtrap DP schema, including event identifiers, media associations, and timing details. Source code in camtrap_dp_export\\serializers.py class SequenceSerializer(serializers.Serializer): \"\"\" Serializer for representing sequence (event) metadata in the Camtrap DP export format. Maps sequence fields to the expected Camtrap DP schema, including event identifiers, media associations, and timing details. \"\"\" eventID = serializers.CharField( help_text=\"Identifier for the sequence event (Camtrap DP: eventID).\" ) mediaID = serializers.SerializerMethodField( help_text=\"List of media identifiers associated with this event (Camtrap DP: mediaID).\" ) mediaCount = serializers.IntegerField( source='nfiles', help_text=\"Total number of media files in this event (Camtrap DP: mediaCount).\" ) eventStart = serializers.CharField( help_text=\"Start time of the event/sequence (Camtrap DP: eventStart, ISO 8601).\" ) eventEnd = serializers.CharField( help_text=\"End time of the event/sequence (Camtrap DP: eventEnd, ISO 8601).\" ) mediaID = serializers.SlugRelatedField( queryset=DataFile.objects.all(), many=True, source=\"data_files\", slug_field='file_name', help_text=\"Media files associated with this sequence (Camtrap DP: mediaID).\" )","title":"SequenceSerializer"},{"location":"reference/camtrap_dp_export/viewsets/","text":"SequenceViewsetCTDP Bases: OptionalPaginationViewSetMixIn A ViewSet for listing sequences of observations in Camtrap DP format. Source code in camtrap_dp_export\\viewsets.py @extend_schema(summary=\"Sequences of observations\", description=\"An observation can be linked to multiple files.\\ A single observation linked to multiple files is considered a sequence\", tags=[\"Observations\"], methods=[\"get\"], ) @extend_schema_view( retrieve=extend_schema(exclude=True) ) class SequenceViewsetCTDP(OptionalPaginationViewSetMixIn): \"\"\" A ViewSet for listing sequences of observations in Camtrap DP format. \"\"\" http_method_names = ['get', 'head'] queryset = get_ctdp_seq_qs(Observation.objects.all()) serializer_class = SequenceSerializer filterset_class = ObservationFilter","title":"viewsets"},{"location":"reference/camtrap_dp_export/viewsets/#camtrap_dp_export.viewsets.SequenceViewsetCTDP","text":"Bases: OptionalPaginationViewSetMixIn A ViewSet for listing sequences of observations in Camtrap DP format. Source code in camtrap_dp_export\\viewsets.py @extend_schema(summary=\"Sequences of observations\", description=\"An observation can be linked to multiple files.\\ A single observation linked to multiple files is considered a sequence\", tags=[\"Observations\"], methods=[\"get\"], ) @extend_schema_view( retrieve=extend_schema(exclude=True) ) class SequenceViewsetCTDP(OptionalPaginationViewSetMixIn): \"\"\" A ViewSet for listing sequences of observations in Camtrap DP format. \"\"\" http_method_names = ['get', 'head'] queryset = get_ctdp_seq_qs(Observation.objects.all()) serializer_class = SequenceSerializer filterset_class = ObservationFilter","title":"SequenceViewsetCTDP"},{"location":"reference/data_handlers/","text":"This module provides functionality for creating specific data handlers to deal with data from specific sensors. This includes parsing files, converting data formats, and handling post-upload tasks.","title":"data_handlers"},{"location":"reference/data_handlers/base_data_handler_class/","text":"DataTypeHandler Base class for handling different data types and device models. Provides interfaces for file format checking, file handling, and post-download tasks. Source code in data_handlers\\base_data_handler_class.py class DataTypeHandler(): \"\"\" Base class for handling different data types and device models. Provides interfaces for file format checking, file handling, and post-download tasks. \"\"\" data_types = [\"default\"] device_models = [\"default\"] safe_formats = [\"\"] full_name = \"No name provided\" description = \"No description provided\" validity_description = \"No validity description provided\" handling_description = \"No handling description provided\" post_handling_description = \"No post handling description provided\" def format_check(self, file, device_label=None): \"\"\" Check if the file's extension is in the list of safe formats. Args: file: File-like object with a 'name' attribute. device_label (optional): Device label, not used by default. Returns: bool: True if file format is safe, False otherwise. \"\"\" return os.path.splitext(file.name)[1].lower() in self.safe_formats def all_file_format_check(self, files, device_label=None): \"\"\" Check if any file in a list matches a safe file format. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any file matches a safe format, False otherwise. \"\"\" for file in files: if self.format_check(file, device_label): return True return False def get_valid_files(self, files, device_label=None): \"\"\" Return a list of files that match the safe formats. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" return [x for x in files if self.format_check(x, device_label)] def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: \"\"\" Handle a file after download, performing any post-processing or validation. Args: file: File-like object with a 'name' attribute. recording_dt (datetime, optional): Recording datetime. extra_data (dict, optional): Additional data. data_type (str, optional): The type of data. Returns: tuple: (recording_dt, extra_data, data_type, post_download_task) \"\"\" if extra_data is None: extra_data = {} file_format = os.path.splitext(file.name)[1] return recording_dt, extra_data, data_type, self.get_post_download_task(file_format) def get_post_download_task(self, file_extension: str, first_time: bool = True): \"\"\" Get a post-download task for the given file extension. Args: file_extension (str): The file's extension. first_time (bool, optional): Whether this is the first time handling the file. Returns: Any: Task to perform after download. Defaults to None. \"\"\" return None all_file_format_check(files, device_label=None) Check if any file in a list matches a safe file format. Parameters: files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if any file matches a safe format, False otherwise. Source code in data_handlers\\base_data_handler_class.py def all_file_format_check(self, files, device_label=None): \"\"\" Check if any file in a list matches a safe file format. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any file matches a safe format, False otherwise. \"\"\" for file in files: if self.format_check(file, device_label): return True return False format_check(file, device_label=None) Check if the file's extension is in the list of safe formats. Parameters: file \u2013 File-like object with a 'name' attribute. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if file format is safe, False otherwise. Source code in data_handlers\\base_data_handler_class.py def format_check(self, file, device_label=None): \"\"\" Check if the file's extension is in the list of safe formats. Args: file: File-like object with a 'name' attribute. device_label (optional): Device label, not used by default. Returns: bool: True if file format is safe, False otherwise. \"\"\" return os.path.splitext(file.name)[1].lower() in self.safe_formats get_post_download_task(file_extension, first_time=True) Get a post-download task for the given file extension. Parameters: file_extension ( str ) \u2013 The file's extension. first_time ( bool , default: True ) \u2013 Whether this is the first time handling the file. Returns: Any \u2013 Task to perform after download. Defaults to None. Source code in data_handlers\\base_data_handler_class.py def get_post_download_task(self, file_extension: str, first_time: bool = True): \"\"\" Get a post-download task for the given file extension. Args: file_extension (str): The file's extension. first_time (bool, optional): Whether this is the first time handling the file. Returns: Any: Task to perform after download. Defaults to None. \"\"\" return None get_valid_files(files, device_label=None) Return a list of files that match the safe formats. Parameters: files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: list \u2013 List of valid files. Source code in data_handlers\\base_data_handler_class.py def get_valid_files(self, files, device_label=None): \"\"\" Return a list of files that match the safe formats. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" return [x for x in files if self.format_check(x, device_label)] handle_file(file, recording_dt=None, extra_data=None, data_type=None) Handle a file after download, performing any post-processing or validation. Parameters: file \u2013 File-like object with a 'name' attribute. recording_dt ( datetime , default: None ) \u2013 Recording datetime. extra_data ( dict , default: None ) \u2013 Additional data. data_type ( str , default: None ) \u2013 The type of data. Returns: tuple ( Tuple [ datetime , dict , str , str ] ) \u2013 (recording_dt, extra_data, data_type, post_download_task) Source code in data_handlers\\base_data_handler_class.py def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: \"\"\" Handle a file after download, performing any post-processing or validation. Args: file: File-like object with a 'name' attribute. recording_dt (datetime, optional): Recording datetime. extra_data (dict, optional): Additional data. data_type (str, optional): The type of data. Returns: tuple: (recording_dt, extra_data, data_type, post_download_task) \"\"\" if extra_data is None: extra_data = {} file_format = os.path.splitext(file.name)[1] return recording_dt, extra_data, data_type, self.get_post_download_task(file_format) DataTypeHandlerCollection Collection class for managing multiple DataTypeHandler subclasses. Handles dynamic importing and retrieval of handlers based on data type and device model. Source code in data_handlers\\base_data_handler_class.py class DataTypeHandlerCollection(): \"\"\" Collection class for managing multiple DataTypeHandler subclasses. Handles dynamic importing and retrieval of handlers based on data type and device model. \"\"\" data_type_handlers = {} data_handler_list = [] def __init__(self, root_path=\"\") -> None: \"\"\" Initialize the collection by importing all handler modules and instantiating their classes. Args: root_path (str, optional): Root path to the handlers directory. \"\"\" handler_dir = os.path.join( root_path, \"data_handlers\", \"handlers\") handler_files = [os.path.splitext(x)[0] for x in os.listdir( handler_dir) if os.path.splitext(x)[1] == '.py'] for handler_file in handler_files: importlib.import_module( f\"data_handlers.handlers.{handler_file}\") all_handlers = DataTypeHandler.__subclasses__() for idx, handler in enumerate(all_handlers): handler_instance = handler() handler_instance.id = idx self.data_handler_list.append(handler_instance) for data_type in handler.data_types: if not self.data_type_handlers.get(data_type): self.data_type_handlers[data_type] = {} for model in handler.device_models: self.data_type_handlers[data_type][model] = handler_instance def set_default_model(self, data_type, device_model): \"\"\" Ensure the device model exists for the given data type, defaulting to 'default' if not found. Args: data_type (str): The data type. device_model (str): The device model. Returns: str or None: The resolved device model, or None if not available. \"\"\" if data_type not in self.data_type_handlers.keys(): return None if device_model not in self.data_type_handlers[data_type].keys()\\ or self.data_type_handlers.get(data_type) is None: device_model = \"default\" return device_model def get_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Get files valid for a specified data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return [] return self.data_type_handlers[data_type][device_model].get_valid_files(files, device_label) def check_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Check if any files are valid for a given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any valid files exist, False otherwise. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return False return self.data_type_handlers[data_type][device_model].all_file_format_check(files, device_label) def check_handlers(self, data_type, device_model): \"\"\" Check if a handler exists for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or False: The handler instance if found, otherwise False. \"\"\" if self.data_type_handlers.get(data_type) is None: return False device_model = self.set_default_model(data_type, device_model) return self.data_type_handlers[data_type].get(device_model) def get_handler(self, data_type, device_model) -> DataTypeHandler: \"\"\" Retrieve the DataTypeHandler instance for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or None: The handler instance, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type].get(device_model) def get_file_handler(self, data_type, device_model) -> Callable: \"\"\" Retrieve the file handling function for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: Callable or None: The file handler function, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type][device_model].handle_file __init__(root_path='') Initialize the collection by importing all handler modules and instantiating their classes. Parameters: root_path ( str , default: '' ) \u2013 Root path to the handlers directory. Source code in data_handlers\\base_data_handler_class.py def __init__(self, root_path=\"\") -> None: \"\"\" Initialize the collection by importing all handler modules and instantiating their classes. Args: root_path (str, optional): Root path to the handlers directory. \"\"\" handler_dir = os.path.join( root_path, \"data_handlers\", \"handlers\") handler_files = [os.path.splitext(x)[0] for x in os.listdir( handler_dir) if os.path.splitext(x)[1] == '.py'] for handler_file in handler_files: importlib.import_module( f\"data_handlers.handlers.{handler_file}\") all_handlers = DataTypeHandler.__subclasses__() for idx, handler in enumerate(all_handlers): handler_instance = handler() handler_instance.id = idx self.data_handler_list.append(handler_instance) for data_type in handler.data_types: if not self.data_type_handlers.get(data_type): self.data_type_handlers[data_type] = {} for model in handler.device_models: self.data_type_handlers[data_type][model] = handler_instance check_handlers(data_type, device_model) Check if a handler exists for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: \u2013 DataTypeHandler or False: The handler instance if found, otherwise False. Source code in data_handlers\\base_data_handler_class.py def check_handlers(self, data_type, device_model): \"\"\" Check if a handler exists for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or False: The handler instance if found, otherwise False. \"\"\" if self.data_type_handlers.get(data_type) is None: return False device_model = self.set_default_model(data_type, device_model) return self.data_type_handlers[data_type].get(device_model) check_valid_files(data_type, device_model, files, device_label=None) Check if any files are valid for a given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if any valid files exist, False otherwise. Source code in data_handlers\\base_data_handler_class.py def check_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Check if any files are valid for a given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any valid files exist, False otherwise. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return False return self.data_type_handlers[data_type][device_model].all_file_format_check(files, device_label) get_file_handler(data_type, device_model) Retrieve the file handling function for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: Callable \u2013 Callable or None: The file handler function, or None if not found. Source code in data_handlers\\base_data_handler_class.py def get_file_handler(self, data_type, device_model) -> Callable: \"\"\" Retrieve the file handling function for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: Callable or None: The file handler function, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type][device_model].handle_file get_handler(data_type, device_model) Retrieve the DataTypeHandler instance for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: DataTypeHandler \u2013 DataTypeHandler or None: The handler instance, or None if not found. Source code in data_handlers\\base_data_handler_class.py def get_handler(self, data_type, device_model) -> DataTypeHandler: \"\"\" Retrieve the DataTypeHandler instance for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or None: The handler instance, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type].get(device_model) get_valid_files(data_type, device_model, files, device_label=None) Get files valid for a specified data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: list \u2013 List of valid files. Source code in data_handlers\\base_data_handler_class.py def get_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Get files valid for a specified data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return [] return self.data_type_handlers[data_type][device_model].get_valid_files(files, device_label) set_default_model(data_type, device_model) Ensure the device model exists for the given data type, defaulting to 'default' if not found. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: \u2013 str or None: The resolved device model, or None if not available. Source code in data_handlers\\base_data_handler_class.py def set_default_model(self, data_type, device_model): \"\"\" Ensure the device model exists for the given data type, defaulting to 'default' if not found. Args: data_type (str): The data type. device_model (str): The device model. Returns: str or None: The resolved device model, or None if not available. \"\"\" if data_type not in self.data_type_handlers.keys(): return None if device_model not in self.data_type_handlers[data_type].keys()\\ or self.data_type_handlers.get(data_type) is None: device_model = \"default\" return device_model generate_thumbnails(file_pks) Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. Source code in data_handlers\\tasks.py @app.task(name=\"data_handler_generate_thumbnails\") def generate_thumbnails(file_pks: List[int]) -> None: \"\"\" Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Args: file_pks (List[int]): List of primary keys for DataFile objects. \"\"\" from data_models.models import DataFile, Deployment from .functions import generate_thumbnail from .post_upload_task_handler import post_upload_task_handler post_upload_task_handler(file_pks, generate_thumbnail) deployment_pk = DataFile.objects.filter(pk__in=file_pks).values_list( 'deployment__pk', flat=True).distinct() deployment_objs = Deployment.objects.filter(pk__in=deployment_pk) for deployment_obj in deployment_objs: deployment_obj.set_thumb_url() Deployment.objects.bulk_update(deployment_objs, [\"thumb_url\"])","title":"base_data_handler_class"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler","text":"Base class for handling different data types and device models. Provides interfaces for file format checking, file handling, and post-download tasks. Source code in data_handlers\\base_data_handler_class.py class DataTypeHandler(): \"\"\" Base class for handling different data types and device models. Provides interfaces for file format checking, file handling, and post-download tasks. \"\"\" data_types = [\"default\"] device_models = [\"default\"] safe_formats = [\"\"] full_name = \"No name provided\" description = \"No description provided\" validity_description = \"No validity description provided\" handling_description = \"No handling description provided\" post_handling_description = \"No post handling description provided\" def format_check(self, file, device_label=None): \"\"\" Check if the file's extension is in the list of safe formats. Args: file: File-like object with a 'name' attribute. device_label (optional): Device label, not used by default. Returns: bool: True if file format is safe, False otherwise. \"\"\" return os.path.splitext(file.name)[1].lower() in self.safe_formats def all_file_format_check(self, files, device_label=None): \"\"\" Check if any file in a list matches a safe file format. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any file matches a safe format, False otherwise. \"\"\" for file in files: if self.format_check(file, device_label): return True return False def get_valid_files(self, files, device_label=None): \"\"\" Return a list of files that match the safe formats. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" return [x for x in files if self.format_check(x, device_label)] def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: \"\"\" Handle a file after download, performing any post-processing or validation. Args: file: File-like object with a 'name' attribute. recording_dt (datetime, optional): Recording datetime. extra_data (dict, optional): Additional data. data_type (str, optional): The type of data. Returns: tuple: (recording_dt, extra_data, data_type, post_download_task) \"\"\" if extra_data is None: extra_data = {} file_format = os.path.splitext(file.name)[1] return recording_dt, extra_data, data_type, self.get_post_download_task(file_format) def get_post_download_task(self, file_extension: str, first_time: bool = True): \"\"\" Get a post-download task for the given file extension. Args: file_extension (str): The file's extension. first_time (bool, optional): Whether this is the first time handling the file. Returns: Any: Task to perform after download. Defaults to None. \"\"\" return None","title":"DataTypeHandler"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler.all_file_format_check","text":"Check if any file in a list matches a safe file format. Parameters: files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if any file matches a safe format, False otherwise. Source code in data_handlers\\base_data_handler_class.py def all_file_format_check(self, files, device_label=None): \"\"\" Check if any file in a list matches a safe file format. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any file matches a safe format, False otherwise. \"\"\" for file in files: if self.format_check(file, device_label): return True return False","title":"all_file_format_check"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler.format_check","text":"Check if the file's extension is in the list of safe formats. Parameters: file \u2013 File-like object with a 'name' attribute. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if file format is safe, False otherwise. Source code in data_handlers\\base_data_handler_class.py def format_check(self, file, device_label=None): \"\"\" Check if the file's extension is in the list of safe formats. Args: file: File-like object with a 'name' attribute. device_label (optional): Device label, not used by default. Returns: bool: True if file format is safe, False otherwise. \"\"\" return os.path.splitext(file.name)[1].lower() in self.safe_formats","title":"format_check"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler.get_post_download_task","text":"Get a post-download task for the given file extension. Parameters: file_extension ( str ) \u2013 The file's extension. first_time ( bool , default: True ) \u2013 Whether this is the first time handling the file. Returns: Any \u2013 Task to perform after download. Defaults to None. Source code in data_handlers\\base_data_handler_class.py def get_post_download_task(self, file_extension: str, first_time: bool = True): \"\"\" Get a post-download task for the given file extension. Args: file_extension (str): The file's extension. first_time (bool, optional): Whether this is the first time handling the file. Returns: Any: Task to perform after download. Defaults to None. \"\"\" return None","title":"get_post_download_task"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler.get_valid_files","text":"Return a list of files that match the safe formats. Parameters: files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: list \u2013 List of valid files. Source code in data_handlers\\base_data_handler_class.py def get_valid_files(self, files, device_label=None): \"\"\" Return a list of files that match the safe formats. Args: files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" return [x for x in files if self.format_check(x, device_label)]","title":"get_valid_files"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandler.handle_file","text":"Handle a file after download, performing any post-processing or validation. Parameters: file \u2013 File-like object with a 'name' attribute. recording_dt ( datetime , default: None ) \u2013 Recording datetime. extra_data ( dict , default: None ) \u2013 Additional data. data_type ( str , default: None ) \u2013 The type of data. Returns: tuple ( Tuple [ datetime , dict , str , str ] ) \u2013 (recording_dt, extra_data, data_type, post_download_task) Source code in data_handlers\\base_data_handler_class.py def handle_file(self, file, recording_dt: datetime = None, extra_data: dict = None, data_type: str = None) -> Tuple[datetime, dict, str, str]: \"\"\" Handle a file after download, performing any post-processing or validation. Args: file: File-like object with a 'name' attribute. recording_dt (datetime, optional): Recording datetime. extra_data (dict, optional): Additional data. data_type (str, optional): The type of data. Returns: tuple: (recording_dt, extra_data, data_type, post_download_task) \"\"\" if extra_data is None: extra_data = {} file_format = os.path.splitext(file.name)[1] return recording_dt, extra_data, data_type, self.get_post_download_task(file_format)","title":"handle_file"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection","text":"Collection class for managing multiple DataTypeHandler subclasses. Handles dynamic importing and retrieval of handlers based on data type and device model. Source code in data_handlers\\base_data_handler_class.py class DataTypeHandlerCollection(): \"\"\" Collection class for managing multiple DataTypeHandler subclasses. Handles dynamic importing and retrieval of handlers based on data type and device model. \"\"\" data_type_handlers = {} data_handler_list = [] def __init__(self, root_path=\"\") -> None: \"\"\" Initialize the collection by importing all handler modules and instantiating their classes. Args: root_path (str, optional): Root path to the handlers directory. \"\"\" handler_dir = os.path.join( root_path, \"data_handlers\", \"handlers\") handler_files = [os.path.splitext(x)[0] for x in os.listdir( handler_dir) if os.path.splitext(x)[1] == '.py'] for handler_file in handler_files: importlib.import_module( f\"data_handlers.handlers.{handler_file}\") all_handlers = DataTypeHandler.__subclasses__() for idx, handler in enumerate(all_handlers): handler_instance = handler() handler_instance.id = idx self.data_handler_list.append(handler_instance) for data_type in handler.data_types: if not self.data_type_handlers.get(data_type): self.data_type_handlers[data_type] = {} for model in handler.device_models: self.data_type_handlers[data_type][model] = handler_instance def set_default_model(self, data_type, device_model): \"\"\" Ensure the device model exists for the given data type, defaulting to 'default' if not found. Args: data_type (str): The data type. device_model (str): The device model. Returns: str or None: The resolved device model, or None if not available. \"\"\" if data_type not in self.data_type_handlers.keys(): return None if device_model not in self.data_type_handlers[data_type].keys()\\ or self.data_type_handlers.get(data_type) is None: device_model = \"default\" return device_model def get_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Get files valid for a specified data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return [] return self.data_type_handlers[data_type][device_model].get_valid_files(files, device_label) def check_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Check if any files are valid for a given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any valid files exist, False otherwise. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return False return self.data_type_handlers[data_type][device_model].all_file_format_check(files, device_label) def check_handlers(self, data_type, device_model): \"\"\" Check if a handler exists for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or False: The handler instance if found, otherwise False. \"\"\" if self.data_type_handlers.get(data_type) is None: return False device_model = self.set_default_model(data_type, device_model) return self.data_type_handlers[data_type].get(device_model) def get_handler(self, data_type, device_model) -> DataTypeHandler: \"\"\" Retrieve the DataTypeHandler instance for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or None: The handler instance, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type].get(device_model) def get_file_handler(self, data_type, device_model) -> Callable: \"\"\" Retrieve the file handling function for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: Callable or None: The file handler function, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type][device_model].handle_file","title":"DataTypeHandlerCollection"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.__init__","text":"Initialize the collection by importing all handler modules and instantiating their classes. Parameters: root_path ( str , default: '' ) \u2013 Root path to the handlers directory. Source code in data_handlers\\base_data_handler_class.py def __init__(self, root_path=\"\") -> None: \"\"\" Initialize the collection by importing all handler modules and instantiating their classes. Args: root_path (str, optional): Root path to the handlers directory. \"\"\" handler_dir = os.path.join( root_path, \"data_handlers\", \"handlers\") handler_files = [os.path.splitext(x)[0] for x in os.listdir( handler_dir) if os.path.splitext(x)[1] == '.py'] for handler_file in handler_files: importlib.import_module( f\"data_handlers.handlers.{handler_file}\") all_handlers = DataTypeHandler.__subclasses__() for idx, handler in enumerate(all_handlers): handler_instance = handler() handler_instance.id = idx self.data_handler_list.append(handler_instance) for data_type in handler.data_types: if not self.data_type_handlers.get(data_type): self.data_type_handlers[data_type] = {} for model in handler.device_models: self.data_type_handlers[data_type][model] = handler_instance","title":"__init__"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.check_handlers","text":"Check if a handler exists for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: \u2013 DataTypeHandler or False: The handler instance if found, otherwise False. Source code in data_handlers\\base_data_handler_class.py def check_handlers(self, data_type, device_model): \"\"\" Check if a handler exists for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or False: The handler instance if found, otherwise False. \"\"\" if self.data_type_handlers.get(data_type) is None: return False device_model = self.set_default_model(data_type, device_model) return self.data_type_handlers[data_type].get(device_model)","title":"check_handlers"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.check_valid_files","text":"Check if any files are valid for a given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: bool \u2013 True if any valid files exist, False otherwise. Source code in data_handlers\\base_data_handler_class.py def check_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Check if any files are valid for a given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: bool: True if any valid files exist, False otherwise. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return False return self.data_type_handlers[data_type][device_model].all_file_format_check(files, device_label)","title":"check_valid_files"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.get_file_handler","text":"Retrieve the file handling function for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: Callable \u2013 Callable or None: The file handler function, or None if not found. Source code in data_handlers\\base_data_handler_class.py def get_file_handler(self, data_type, device_model) -> Callable: \"\"\" Retrieve the file handling function for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: Callable or None: The file handler function, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type][device_model].handle_file","title":"get_file_handler"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.get_handler","text":"Retrieve the DataTypeHandler instance for the given data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: DataTypeHandler \u2013 DataTypeHandler or None: The handler instance, or None if not found. Source code in data_handlers\\base_data_handler_class.py def get_handler(self, data_type, device_model) -> DataTypeHandler: \"\"\" Retrieve the DataTypeHandler instance for the given data type and device model. Args: data_type (str): The data type. device_model (str): The device model. Returns: DataTypeHandler or None: The handler instance, or None if not found. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return None logger.info(f\"Got data handler {data_type} {device_model}\") return self.data_type_handlers[data_type].get(device_model)","title":"get_handler"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.get_valid_files","text":"Get files valid for a specified data type and device model. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. files ( list ) \u2013 List of file-like objects. device_label ( optional , default: None ) \u2013 Device label, not used by default. Returns: list \u2013 List of valid files. Source code in data_handlers\\base_data_handler_class.py def get_valid_files(self, data_type, device_model, files, device_label=None): \"\"\" Get files valid for a specified data type and device model. Args: data_type (str): The data type. device_model (str): The device model. files (list): List of file-like objects. device_label (optional): Device label, not used by default. Returns: list: List of valid files. \"\"\" device_model = self.set_default_model(data_type, device_model) if device_model is None: return [] return self.data_type_handlers[data_type][device_model].get_valid_files(files, device_label)","title":"get_valid_files"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.DataTypeHandlerCollection.set_default_model","text":"Ensure the device model exists for the given data type, defaulting to 'default' if not found. Parameters: data_type ( str ) \u2013 The data type. device_model ( str ) \u2013 The device model. Returns: \u2013 str or None: The resolved device model, or None if not available. Source code in data_handlers\\base_data_handler_class.py def set_default_model(self, data_type, device_model): \"\"\" Ensure the device model exists for the given data type, defaulting to 'default' if not found. Args: data_type (str): The data type. device_model (str): The device model. Returns: str or None: The resolved device model, or None if not available. \"\"\" if data_type not in self.data_type_handlers.keys(): return None if device_model not in self.data_type_handlers[data_type].keys()\\ or self.data_type_handlers.get(data_type) is None: device_model = \"default\" return device_model","title":"set_default_model"},{"location":"reference/data_handlers/base_data_handler_class/#data_handlers.base_data_handler_class.generate_thumbnails","text":"Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. Source code in data_handlers\\tasks.py @app.task(name=\"data_handler_generate_thumbnails\") def generate_thumbnails(file_pks: List[int]) -> None: \"\"\" Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Args: file_pks (List[int]): List of primary keys for DataFile objects. \"\"\" from data_models.models import DataFile, Deployment from .functions import generate_thumbnail from .post_upload_task_handler import post_upload_task_handler post_upload_task_handler(file_pks, generate_thumbnail) deployment_pk = DataFile.objects.filter(pk__in=file_pks).values_list( 'deployment__pk', flat=True).distinct() deployment_objs = Deployment.objects.filter(pk__in=deployment_pk) for deployment_obj in deployment_objs: deployment_obj.set_thumb_url() Deployment.objects.bulk_update(deployment_objs, [\"thumb_url\"])","title":"generate_thumbnails"},{"location":"reference/data_handlers/functions/","text":"check_exif_keys(image_exif, exif_keys, round_val=2) Check and extract specified EXIF keys from the image's EXIF data, rounding float values. Parameters: image_exif ( Dict [ str , Any ] ) \u2013 EXIF data as a dictionary. exif_keys ( List [ str ] ) \u2013 List of keys to extract from the EXIF data. round_val ( int , default: 2 ) \u2013 Number of decimal places to round float values to. Returns: Dict [ str , Any ] \u2013 A dictionary of the requested EXIF key-value pairs, with floats rounded. Source code in data_handlers\\functions.py def check_exif_keys( image_exif: Dict[str, Any], exif_keys: List[str], round_val: int = 2 ) -> Dict[str, Any]: \"\"\" Check and extract specified EXIF keys from the image's EXIF data, rounding float values. Args: image_exif: EXIF data as a dictionary. exif_keys: List of keys to extract from the EXIF data. round_val: Number of decimal places to round float values to. Returns: A dictionary of the requested EXIF key-value pairs, with floats rounded. \"\"\" new_data = {} for exif_key in exif_keys: val = image_exif.get(exif_key) if val is not None: if type(val) is TiffImagePlugin.IFDRational: val = float(val) if type(val) is float: val = round(val, round_val) new_data[exif_key] = val return new_data generate_thumbnail(data_file, max_width=250, max_height=250) Generate a thumbnail for the given image file. Parameters: data_file ( DataFile ) \u2013 DataFile object. max_width ( int , default: 250 ) \u2013 The maximum width of the thumbnail. max_height ( int , default: 250 ) \u2013 The maximum height of the thumbnail. Returns: Tuple [ Any , List [ str ]] \u2013 A tuple containing: - the modified data_file object (with updated thumbnail URL) - a list of modified attributes (['thumb_url']) Source code in data_handlers\\functions.py def generate_thumbnail( data_file: 'DataFile', max_width: int = 250, max_height: int = 250 ) -> Tuple[Any, List[str]]: \"\"\" Generate a thumbnail for the given image file. Args: data_file: DataFile object. max_width: The maximum width of the thumbnail. max_height: The maximum height of the thumbnail. Returns: A tuple containing: - the modified data_file object (with updated thumbnail URL) - a list of modified attributes (['thumb_url']) \"\"\" file_path = data_file.full_path() thumb_path = os.path.join(os.path.split( file_path)[0], data_file.file_name+\"_THUMB.jpg\") # open image file image = Image.open(file_path) image.thumbnail((max_width, max_height)) # creating thumbnail image.save(thumb_path) data_file.set_thumb_url() return data_file, [\"thumb_url\"] get_image_recording_dt(image_exif) Get the recording datetime from image EXIF data. Parameters: image_exif ( Dict [ str , Any ] ) \u2013 EXIF data as a dictionary. Returns: Optional [ datetime ] \u2013 A datetime object representing when the image was recorded, or None if not available. Source code in data_handlers\\functions.py def get_image_recording_dt(image_exif: Dict[str, Any]) -> Optional[dt]: \"\"\" Get the recording datetime from image EXIF data. Args: image_exif: EXIF data as a dictionary. Returns: A datetime object representing when the image was recorded, or None if not available. \"\"\" recording_dt = image_exif.get('DateTimeOriginal') if recording_dt is None: recording_dt = image_exif.get('DateTime') if recording_dt is None: return None return dt.strptime(recording_dt, '%Y:%m:%d %H:%M:%S') open_exif(uploaded_file) Open an uploaded image file and extract its EXIF metadata. Parameters: uploaded_file ( Any ) \u2013 Uploaded file, expected to have a .file attribute. Returns: Dict [ str , Any ] \u2013 A dictionary mapping EXIF tag names to their corresponding values. Dict [ str , Any ] \u2013 Returns an empty dictionary if the EXIF data cannot be read. Source code in data_handlers\\functions.py def open_exif(uploaded_file: Any) -> Dict[str, Any]: \"\"\" Open an uploaded image file and extract its EXIF metadata. Args: uploaded_file: Uploaded file, expected to have a `.file` attribute. Returns: A dictionary mapping EXIF tag names to their corresponding values. Returns an empty dictionary if the EXIF data cannot be read. \"\"\" try: si = uploaded_file.file image = Image.open(si) image_exif = {ExifTags.TAGS[k]: v for k, v in image.getexif().items() if k in ExifTags.TAGS} return image_exif except OSError: logger.error(\"Unable to open exif\") return {} except UnidentifiedImageError: logger.error(\"Unable to open exif\") return {}","title":"functions"},{"location":"reference/data_handlers/functions/#data_handlers.functions.check_exif_keys","text":"Check and extract specified EXIF keys from the image's EXIF data, rounding float values. Parameters: image_exif ( Dict [ str , Any ] ) \u2013 EXIF data as a dictionary. exif_keys ( List [ str ] ) \u2013 List of keys to extract from the EXIF data. round_val ( int , default: 2 ) \u2013 Number of decimal places to round float values to. Returns: Dict [ str , Any ] \u2013 A dictionary of the requested EXIF key-value pairs, with floats rounded. Source code in data_handlers\\functions.py def check_exif_keys( image_exif: Dict[str, Any], exif_keys: List[str], round_val: int = 2 ) -> Dict[str, Any]: \"\"\" Check and extract specified EXIF keys from the image's EXIF data, rounding float values. Args: image_exif: EXIF data as a dictionary. exif_keys: List of keys to extract from the EXIF data. round_val: Number of decimal places to round float values to. Returns: A dictionary of the requested EXIF key-value pairs, with floats rounded. \"\"\" new_data = {} for exif_key in exif_keys: val = image_exif.get(exif_key) if val is not None: if type(val) is TiffImagePlugin.IFDRational: val = float(val) if type(val) is float: val = round(val, round_val) new_data[exif_key] = val return new_data","title":"check_exif_keys"},{"location":"reference/data_handlers/functions/#data_handlers.functions.generate_thumbnail","text":"Generate a thumbnail for the given image file. Parameters: data_file ( DataFile ) \u2013 DataFile object. max_width ( int , default: 250 ) \u2013 The maximum width of the thumbnail. max_height ( int , default: 250 ) \u2013 The maximum height of the thumbnail. Returns: Tuple [ Any , List [ str ]] \u2013 A tuple containing: - the modified data_file object (with updated thumbnail URL) - a list of modified attributes (['thumb_url']) Source code in data_handlers\\functions.py def generate_thumbnail( data_file: 'DataFile', max_width: int = 250, max_height: int = 250 ) -> Tuple[Any, List[str]]: \"\"\" Generate a thumbnail for the given image file. Args: data_file: DataFile object. max_width: The maximum width of the thumbnail. max_height: The maximum height of the thumbnail. Returns: A tuple containing: - the modified data_file object (with updated thumbnail URL) - a list of modified attributes (['thumb_url']) \"\"\" file_path = data_file.full_path() thumb_path = os.path.join(os.path.split( file_path)[0], data_file.file_name+\"_THUMB.jpg\") # open image file image = Image.open(file_path) image.thumbnail((max_width, max_height)) # creating thumbnail image.save(thumb_path) data_file.set_thumb_url() return data_file, [\"thumb_url\"]","title":"generate_thumbnail"},{"location":"reference/data_handlers/functions/#data_handlers.functions.get_image_recording_dt","text":"Get the recording datetime from image EXIF data. Parameters: image_exif ( Dict [ str , Any ] ) \u2013 EXIF data as a dictionary. Returns: Optional [ datetime ] \u2013 A datetime object representing when the image was recorded, or None if not available. Source code in data_handlers\\functions.py def get_image_recording_dt(image_exif: Dict[str, Any]) -> Optional[dt]: \"\"\" Get the recording datetime from image EXIF data. Args: image_exif: EXIF data as a dictionary. Returns: A datetime object representing when the image was recorded, or None if not available. \"\"\" recording_dt = image_exif.get('DateTimeOriginal') if recording_dt is None: recording_dt = image_exif.get('DateTime') if recording_dt is None: return None return dt.strptime(recording_dt, '%Y:%m:%d %H:%M:%S')","title":"get_image_recording_dt"},{"location":"reference/data_handlers/functions/#data_handlers.functions.open_exif","text":"Open an uploaded image file and extract its EXIF metadata. Parameters: uploaded_file ( Any ) \u2013 Uploaded file, expected to have a .file attribute. Returns: Dict [ str , Any ] \u2013 A dictionary mapping EXIF tag names to their corresponding values. Dict [ str , Any ] \u2013 Returns an empty dictionary if the EXIF data cannot be read. Source code in data_handlers\\functions.py def open_exif(uploaded_file: Any) -> Dict[str, Any]: \"\"\" Open an uploaded image file and extract its EXIF metadata. Args: uploaded_file: Uploaded file, expected to have a `.file` attribute. Returns: A dictionary mapping EXIF tag names to their corresponding values. Returns an empty dictionary if the EXIF data cannot be read. \"\"\" try: si = uploaded_file.file image = Image.open(si) image_exif = {ExifTags.TAGS[k]: v for k, v in image.getexif().items() if k in ExifTags.TAGS} return image_exif except OSError: logger.error(\"Unable to open exif\") return {} except UnidentifiedImageError: logger.error(\"Unable to open exif\") return {}","title":"open_exif"},{"location":"reference/data_handlers/post_upload_task_handler/","text":"post_upload_task_handler(file_pks, task_function) Executes a post-upload task function on a list of DataFile objects, ensuring each file is locked during processing and restored to its initial lock state afterward. This function retrieves DataFile objects by their primary keys, temporarily locks them (by setting do_not_remove=True ), applies the provided task_function to each file, and then restores each file's original do_not_remove state. It ensures that one file's failure does not interrupt processing for others. Parameters: file_pks ( List [ int ] ) \u2013 A list of primary keys representing DataFile objects to process. task_function ( Callable [[ DataFile ], Tuple [ DataFile | None, List [ str ] | None]] ) \u2013 A function to execute on each DataFile. It should accept a DataFile object and return a tuple: (possibly modified DataFile, list of modified field names). If the function fails, the original DataFile object is left unchanged. Notes Each DataFile is locked before processing by setting do_not_remove=True , and unlocked (restored to its original state) after processing. Any exceptions in processing a file are logged, but will not halt the processing of other files. All modified DataFile objects are batch updated at the end, including the do_not_remove field if it was changed. Returns: None \u2013 None Source code in data_handlers\\post_upload_task_handler.py def post_upload_task_handler( file_pks: List[int], task_function: Callable[[DataFile], Tuple[DataFile | None, List[str] | None]] ) -> None: \"\"\" Executes a post-upload task function on a list of DataFile objects, ensuring each file is locked during processing and restored to its initial lock state afterward. This function retrieves DataFile objects by their primary keys, temporarily locks them (by setting `do_not_remove=True`), applies the provided `task_function` to each file, and then restores each file's original `do_not_remove` state. It ensures that one file's failure does not interrupt processing for others. Args: file_pks (List[int]): A list of primary keys representing DataFile objects to process. task_function (Callable[[DataFile], Tuple[DataFile | None, List[str] | None]]): A function to execute on each DataFile. It should accept a DataFile object and return a tuple: (possibly modified DataFile, list of modified field names). If the function fails, the original DataFile object is left unchanged. Notes: - Each DataFile is locked before processing by setting `do_not_remove=True`, and unlocked (restored to its original state) after processing. - Any exceptions in processing a file are logged, but will not halt the processing of other files. - All modified DataFile objects are batch updated at the end, including the `do_not_remove` field if it was changed. Returns: None \"\"\" # get datafiles data_file_objs = DataFile.objects.filter( pk__in=file_pks).order_by(\"created_on\") # save initial do_not_delete state of files logger.info( f\"Running job {str(task_function)} on {data_file_objs.count()} files\") do_not_remove_initial = list(data_file_objs.values_list( \"do_not_remove\", flat=True)) # lock datafiles data_file_objs.update(do_not_remove=True) updated_data_objs = [] modified_fields = [] # loop through datafiles for i, data_file in enumerate(data_file_objs): file_do_not_remove = do_not_remove_initial[i] try: data_file, modified_fields = task_function(data_file) except Exception as e: # One file failing shouldn't lead to the whole job failing logger.error(repr(e)) logger.error(traceback.format_exc()) data_file.do_not_remove = file_do_not_remove updated_data_objs.append(data_file) if \"do_not_remove\" not in modified_fields: modified_fields.append(\"do_not_remove\") # update objects update = DataFile.objects.bulk_update(updated_data_objs, modified_fields) logger.info(f\"Running job {str(task_function)} updated {update} files\")","title":"post_upload_task_handler"},{"location":"reference/data_handlers/post_upload_task_handler/#data_handlers.post_upload_task_handler.post_upload_task_handler","text":"Executes a post-upload task function on a list of DataFile objects, ensuring each file is locked during processing and restored to its initial lock state afterward. This function retrieves DataFile objects by their primary keys, temporarily locks them (by setting do_not_remove=True ), applies the provided task_function to each file, and then restores each file's original do_not_remove state. It ensures that one file's failure does not interrupt processing for others. Parameters: file_pks ( List [ int ] ) \u2013 A list of primary keys representing DataFile objects to process. task_function ( Callable [[ DataFile ], Tuple [ DataFile | None, List [ str ] | None]] ) \u2013 A function to execute on each DataFile. It should accept a DataFile object and return a tuple: (possibly modified DataFile, list of modified field names). If the function fails, the original DataFile object is left unchanged. Notes Each DataFile is locked before processing by setting do_not_remove=True , and unlocked (restored to its original state) after processing. Any exceptions in processing a file are logged, but will not halt the processing of other files. All modified DataFile objects are batch updated at the end, including the do_not_remove field if it was changed. Returns: None \u2013 None Source code in data_handlers\\post_upload_task_handler.py def post_upload_task_handler( file_pks: List[int], task_function: Callable[[DataFile], Tuple[DataFile | None, List[str] | None]] ) -> None: \"\"\" Executes a post-upload task function on a list of DataFile objects, ensuring each file is locked during processing and restored to its initial lock state afterward. This function retrieves DataFile objects by their primary keys, temporarily locks them (by setting `do_not_remove=True`), applies the provided `task_function` to each file, and then restores each file's original `do_not_remove` state. It ensures that one file's failure does not interrupt processing for others. Args: file_pks (List[int]): A list of primary keys representing DataFile objects to process. task_function (Callable[[DataFile], Tuple[DataFile | None, List[str] | None]]): A function to execute on each DataFile. It should accept a DataFile object and return a tuple: (possibly modified DataFile, list of modified field names). If the function fails, the original DataFile object is left unchanged. Notes: - Each DataFile is locked before processing by setting `do_not_remove=True`, and unlocked (restored to its original state) after processing. - Any exceptions in processing a file are logged, but will not halt the processing of other files. - All modified DataFile objects are batch updated at the end, including the `do_not_remove` field if it was changed. Returns: None \"\"\" # get datafiles data_file_objs = DataFile.objects.filter( pk__in=file_pks).order_by(\"created_on\") # save initial do_not_delete state of files logger.info( f\"Running job {str(task_function)} on {data_file_objs.count()} files\") do_not_remove_initial = list(data_file_objs.values_list( \"do_not_remove\", flat=True)) # lock datafiles data_file_objs.update(do_not_remove=True) updated_data_objs = [] modified_fields = [] # loop through datafiles for i, data_file in enumerate(data_file_objs): file_do_not_remove = do_not_remove_initial[i] try: data_file, modified_fields = task_function(data_file) except Exception as e: # One file failing shouldn't lead to the whole job failing logger.error(repr(e)) logger.error(traceback.format_exc()) data_file.do_not_remove = file_do_not_remove updated_data_objs.append(data_file) if \"do_not_remove\" not in modified_fields: modified_fields.append(\"do_not_remove\") # update objects update = DataFile.objects.bulk_update(updated_data_objs, modified_fields) logger.info(f\"Running job {str(task_function)} updated {update} files\")","title":"post_upload_task_handler"},{"location":"reference/data_handlers/serializers/","text":"DataHandlerSerializer Bases: Serializer Serializer for data handler objects in the sensor portal. Captures supported data types, device models, and various descriptions relevant to data handling and post-processing. Source code in data_handlers\\serializers.py class DataHandlerSerializer(serializers.Serializer): \"\"\" Serializer for data handler objects in the sensor portal. Captures supported data types, device models, and various descriptions relevant to data handling and post-processing. \"\"\" id = serializers.IntegerField( read_only=True, help_text=\"Unique identifier for the data handler.\" ) data_types = serializers.ListField( child=serializers.CharField(max_length=100), allow_empty=True, help_text=\"List of supported data types for this handler.\" ) device_models = serializers.ListField( child=serializers.CharField(max_length=100), allow_empty=True, help_text=\"List of compatible device models for this handler.\" ) safe_formats = serializers.ListField( child=serializers.CharField(max_length=10), allow_empty=True, help_text=\"List of safe output data formats.\" ) full_name = serializers.CharField( max_length=100, help_text=\"Full name of the data handler.\" ) description = serializers.CharField( max_length=100, help_text=\"Short description of the data handler.\" ) validity_description = serializers.CharField( max_length=500, help_text=\"Description of data validity checks performed.\" ) handling_description = serializers.CharField( max_length=500, help_text=\"Description of how the data is handled.\" ) post_handling_description = serializers.CharField( max_length=500, help_text=\"Description of post-handling steps taken after processing.\" )","title":"serializers"},{"location":"reference/data_handlers/serializers/#data_handlers.serializers.DataHandlerSerializer","text":"Bases: Serializer Serializer for data handler objects in the sensor portal. Captures supported data types, device models, and various descriptions relevant to data handling and post-processing. Source code in data_handlers\\serializers.py class DataHandlerSerializer(serializers.Serializer): \"\"\" Serializer for data handler objects in the sensor portal. Captures supported data types, device models, and various descriptions relevant to data handling and post-processing. \"\"\" id = serializers.IntegerField( read_only=True, help_text=\"Unique identifier for the data handler.\" ) data_types = serializers.ListField( child=serializers.CharField(max_length=100), allow_empty=True, help_text=\"List of supported data types for this handler.\" ) device_models = serializers.ListField( child=serializers.CharField(max_length=100), allow_empty=True, help_text=\"List of compatible device models for this handler.\" ) safe_formats = serializers.ListField( child=serializers.CharField(max_length=10), allow_empty=True, help_text=\"List of safe output data formats.\" ) full_name = serializers.CharField( max_length=100, help_text=\"Full name of the data handler.\" ) description = serializers.CharField( max_length=100, help_text=\"Short description of the data handler.\" ) validity_description = serializers.CharField( max_length=500, help_text=\"Description of data validity checks performed.\" ) handling_description = serializers.CharField( max_length=500, help_text=\"Description of how the data is handled.\" ) post_handling_description = serializers.CharField( max_length=500, help_text=\"Description of post-handling steps taken after processing.\" )","title":"DataHandlerSerializer"},{"location":"reference/data_handlers/tasks/","text":"generate_thumbnails(file_pks) Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. Source code in data_handlers\\tasks.py @app.task(name=\"data_handler_generate_thumbnails\") def generate_thumbnails(file_pks: List[int]) -> None: \"\"\" Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Args: file_pks (List[int]): List of primary keys for DataFile objects. \"\"\" from data_models.models import DataFile, Deployment from .functions import generate_thumbnail from .post_upload_task_handler import post_upload_task_handler post_upload_task_handler(file_pks, generate_thumbnail) deployment_pk = DataFile.objects.filter(pk__in=file_pks).values_list( 'deployment__pk', flat=True).distinct() deployment_objs = Deployment.objects.filter(pk__in=deployment_pk) for deployment_obj in deployment_objs: deployment_obj.set_thumb_url() Deployment.objects.bulk_update(deployment_objs, [\"thumb_url\"])","title":"tasks"},{"location":"reference/data_handlers/tasks/#data_handlers.tasks.generate_thumbnails","text":"Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Parameters: file_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. Source code in data_handlers\\tasks.py @app.task(name=\"data_handler_generate_thumbnails\") def generate_thumbnails(file_pks: List[int]) -> None: \"\"\" Celery task to generate thumbnails for a list of DataFile primary keys. This task triggers the thumbnail generation function for each file, updates the deployment's thumbnail URL for affected deployments, and performs a bulk update to save the new thumbnail URLs. Args: file_pks (List[int]): List of primary keys for DataFile objects. \"\"\" from data_models.models import DataFile, Deployment from .functions import generate_thumbnail from .post_upload_task_handler import post_upload_task_handler post_upload_task_handler(file_pks, generate_thumbnail) deployment_pk = DataFile.objects.filter(pk__in=file_pks).values_list( 'deployment__pk', flat=True).distinct() deployment_objs = Deployment.objects.filter(pk__in=deployment_pk) for deployment_obj in deployment_objs: deployment_obj.set_thumb_url() Deployment.objects.bulk_update(deployment_objs, [\"thumb_url\"])","title":"generate_thumbnails"},{"location":"reference/data_handlers/viewsets/","text":"DataHandlerViewSet Bases: ViewSet A ViewSet for listing and retrieving data handlers. List: Returns all data handlers available in the settings. Retrieve: Returns a specific data handler by its index (primary key). Source code in data_handlers\\viewsets.py @extend_schema( exclude=True ) class DataHandlerViewSet(viewsets.ViewSet): \"\"\" A ViewSet for listing and retrieving data handlers. List: Returns all data handlers available in the settings. Retrieve: Returns a specific data handler by its index (primary key). \"\"\" serializer_class = DataHandlerSerializer permission_classes = [IsAuthenticated] @method_decorator(cache_page(60 * 60 * 2)) def list(self, request: Request) -> Response: \"\"\" List all data handlers. Args: request (Request): The HTTP request instance. Returns: Response: A DRF Response containing serialized data handlers. \"\"\" serializer = self.serializer_class( instance=settings.DATA_HANDLERS.data_handler_list, many=True) return Response(serializer.data) @method_decorator(cache_page(60 * 60 * 2)) def retrieve(self, request: Request, pk: str = None) -> Response: \"\"\" Retrieve a single data handler by its index. Args: request (Request): The HTTP request instance. pk (str, optional): The index of the data handler in the list. Returns: Response: A DRF Response containing the serialized data handler, or 404 if not found or invalid index. \"\"\" try: data_handler = settings.DATA_HANDLERS.data_handler_list[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(data_handler) return Response(serializer.data) list(request) List all data handlers. Parameters: request ( Request ) \u2013 The HTTP request instance. Returns: Response ( Response ) \u2013 A DRF Response containing serialized data handlers. Source code in data_handlers\\viewsets.py @method_decorator(cache_page(60 * 60 * 2)) def list(self, request: Request) -> Response: \"\"\" List all data handlers. Args: request (Request): The HTTP request instance. Returns: Response: A DRF Response containing serialized data handlers. \"\"\" serializer = self.serializer_class( instance=settings.DATA_HANDLERS.data_handler_list, many=True) return Response(serializer.data) retrieve(request, pk=None) Retrieve a single data handler by its index. Parameters: request ( Request ) \u2013 The HTTP request instance. pk ( str , default: None ) \u2013 The index of the data handler in the list. Returns: Response ( Response ) \u2013 A DRF Response containing the serialized data handler, or 404 if not found or invalid index. Source code in data_handlers\\viewsets.py @method_decorator(cache_page(60 * 60 * 2)) def retrieve(self, request: Request, pk: str = None) -> Response: \"\"\" Retrieve a single data handler by its index. Args: request (Request): The HTTP request instance. pk (str, optional): The index of the data handler in the list. Returns: Response: A DRF Response containing the serialized data handler, or 404 if not found or invalid index. \"\"\" try: data_handler = settings.DATA_HANDLERS.data_handler_list[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(data_handler) return Response(serializer.data)","title":"viewsets"},{"location":"reference/data_handlers/viewsets/#data_handlers.viewsets.DataHandlerViewSet","text":"Bases: ViewSet A ViewSet for listing and retrieving data handlers. List: Returns all data handlers available in the settings. Retrieve: Returns a specific data handler by its index (primary key). Source code in data_handlers\\viewsets.py @extend_schema( exclude=True ) class DataHandlerViewSet(viewsets.ViewSet): \"\"\" A ViewSet for listing and retrieving data handlers. List: Returns all data handlers available in the settings. Retrieve: Returns a specific data handler by its index (primary key). \"\"\" serializer_class = DataHandlerSerializer permission_classes = [IsAuthenticated] @method_decorator(cache_page(60 * 60 * 2)) def list(self, request: Request) -> Response: \"\"\" List all data handlers. Args: request (Request): The HTTP request instance. Returns: Response: A DRF Response containing serialized data handlers. \"\"\" serializer = self.serializer_class( instance=settings.DATA_HANDLERS.data_handler_list, many=True) return Response(serializer.data) @method_decorator(cache_page(60 * 60 * 2)) def retrieve(self, request: Request, pk: str = None) -> Response: \"\"\" Retrieve a single data handler by its index. Args: request (Request): The HTTP request instance. pk (str, optional): The index of the data handler in the list. Returns: Response: A DRF Response containing the serialized data handler, or 404 if not found or invalid index. \"\"\" try: data_handler = settings.DATA_HANDLERS.data_handler_list[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(data_handler) return Response(serializer.data)","title":"DataHandlerViewSet"},{"location":"reference/data_handlers/viewsets/#data_handlers.viewsets.DataHandlerViewSet.list","text":"List all data handlers. Parameters: request ( Request ) \u2013 The HTTP request instance. Returns: Response ( Response ) \u2013 A DRF Response containing serialized data handlers. Source code in data_handlers\\viewsets.py @method_decorator(cache_page(60 * 60 * 2)) def list(self, request: Request) -> Response: \"\"\" List all data handlers. Args: request (Request): The HTTP request instance. Returns: Response: A DRF Response containing serialized data handlers. \"\"\" serializer = self.serializer_class( instance=settings.DATA_HANDLERS.data_handler_list, many=True) return Response(serializer.data)","title":"list"},{"location":"reference/data_handlers/viewsets/#data_handlers.viewsets.DataHandlerViewSet.retrieve","text":"Retrieve a single data handler by its index. Parameters: request ( Request ) \u2013 The HTTP request instance. pk ( str , default: None ) \u2013 The index of the data handler in the list. Returns: Response ( Response ) \u2013 A DRF Response containing the serialized data handler, or 404 if not found or invalid index. Source code in data_handlers\\viewsets.py @method_decorator(cache_page(60 * 60 * 2)) def retrieve(self, request: Request, pk: str = None) -> Response: \"\"\" Retrieve a single data handler by its index. Args: request (Request): The HTTP request instance. pk (str, optional): The index of the data handler in the list. Returns: Response: A DRF Response containing the serialized data handler, or 404 if not found or invalid index. \"\"\" try: data_handler = settings.DATA_HANDLERS.data_handler_list[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(data_handler) return Response(serializer.data)","title":"retrieve"},{"location":"reference/data_models/","text":"This module deals with much of the core functionality of the sensor portal, including data models, data handlers, and tasks related to data processing.","title":"data_models"},{"location":"reference/data_models/file_handling_functions/","text":"create_file_objects(files, check_filename=False, recording_dt=None, extra_data=None, deployment_object=None, device_object=None, data_types=None, request_user=None, multipart=False, verbose=True) Create file objects, handle uploads, validations, and database record creation. Parameters: files ( List [ Union [ object , UploadedFile ]] ) \u2013 List of file objects to process. check_filename ( bool , default: False ) \u2013 If True, check for duplicate filenames in the database. Defaults to False. recording_dt ( Optional [ List [ Optional [ datetime ]]] , default: None ) \u2013 List of recording datetimes for the files. Defaults to None. extra_data ( Optional [ List [ Dict [ str , Union [ str , int , float , bool , None]]]] , default: None ) \u2013 List of additional metadata for the files. Defaults to None. deployment_object ( Optional [ Deployment ] , default: None ) \u2013 Deployment object associated with the files. Defaults to None. device_object ( Optional [ Device ] , default: None ) \u2013 Device object associated with the files. Defaults to None. data_types ( Optional [ List [ str ]] , default: None ) \u2013 List of data types for the files. Defaults to None. request_user ( Optional [ User ] , default: None ) \u2013 User object for permission checks. Defaults to None. multipart ( bool , default: False ) \u2013 If True, handle multipart file uploads. Defaults to False. verbose ( bool , default: True ) \u2013 If True, enable verbose logging. Defaults to False. Returns: List [ DataFile ] \u2013 Tuple[ List[DataFile], # Successfully uploaded file objects Invalid files with errors List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], # Files already in DB int # HTTP status code List [ Dict [ str , Dict [ str , Union [ str , int ]]]] \u2013 ] Raises: ValidationError \u2013 If there is an error validating the database records. Exception \u2013 For any other errors during file handling or database operations. Notes Handles duplicate filename checks and multipart uploads. Validates file types based on the device model. Checks permissions for attaching files to deployments. Filters files based on recording datetime and deployment validity. Creates database records and saves valid files. Supports automated tasks and checksum validation. Source code in data_models\\file_handling_functions.py def create_file_objects( files: List[Union[object, UploadedFile]], check_filename: bool = False, recording_dt: Optional[List[Optional[dt]]] = None, extra_data: Optional[List[Dict[str, Union[str, int, float, bool, None]]]] = None, deployment_object: Optional[\"Deployment\"] = None, device_object: Optional[\"Device\"] = None, data_types: Optional[List[str]] = None, request_user: Optional[\"User\"] = None, multipart: bool = False, verbose: bool = True ) -> Tuple[ List[\"DataFile\"], List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], int ]: \"\"\" Create file objects, handle uploads, validations, and database record creation. Args: files (List[Union[object, UploadedFile]]): List of file objects to process. check_filename (bool, optional): If True, check for duplicate filenames in the database. Defaults to False. recording_dt (Optional[List[Optional[datetime]]], optional): List of recording datetimes for the files. Defaults to None. extra_data (Optional[List[Dict[str, Union[str, int, float, bool, None]]]], optional): List of additional metadata for the files. Defaults to None. deployment_object (Optional[Deployment], optional): Deployment object associated with the files. Defaults to None. device_object (Optional[Device], optional): Device object associated with the files. Defaults to None. data_types (Optional[List[str]], optional): List of data types for the files. Defaults to None. request_user (Optional[User], optional): User object for permission checks. Defaults to None. multipart (bool, optional): If True, handle multipart file uploads. Defaults to False. verbose (bool, optional): If True, enable verbose logging. Defaults to False. Returns: Tuple[ List[DataFile], # Successfully uploaded file objects # Invalid files with errors List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], # Files already in DB int # HTTP status code ] Raises: ValidationError: If there is an error validating the database records. Exception: For any other errors during file handling or database operations. Notes: - Handles duplicate filename checks and multipart uploads. - Validates file types based on the device model. - Checks permissions for attaching files to deployments. - Filters files based on recording datetime and deployment validity. - Creates database records and saves valid files. - Supports automated tasks and checksum validation. \"\"\" from data_models.models import DataFile, DataType, ProjectJob invalid_files = [] existing_files = [] uploaded_files = [] if extra_data is None: extra_data = [{}] if verbose: logger.info(\"Initial files:\", files) logger.info(\"Device object:\", device_object) logger.info(\"Deployment object:\", deployment_object) logger.info(\"Recording datetime:\", recording_dt) logger.info(\"Extra data:\", extra_data) logger.info(\"Data types:\", data_types) logger.info(\"Request user:\", request_user) # Get the current upload datetime upload_dt = djtimezone.now() # Check for duplicate filenames or handle multipart uploads if check_filename or multipart: if verbose: logger.info(\"Checking filenames or handling multipart upload...\") # Extract filenames from the provided file objects filenames = [x.name for x in files] if multipart: if verbose: logger.info(\"Handling multipart upload...\") # Ensure only one file chunk is provided for multipart uploads if len(filenames) > 1: invalid_files += [{x: {\"message\": \"Multipart file upload expects a single file chunk\", \"status\": 400}} for x, in filenames] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Initialize multipart-related variables multipart_obj = None multipart_checksum = None # Query the database for existing multipart files with matching filenames existing_multipart_files = DataFile.objects.filter( original_name__in=filenames, extra_data__md5_checksum__isnull=True) if device_object: # Filter by device if a device object is provided existing_multipart_files = existing_multipart_files.filter( deployment__device=device_object) if deployment_object: # Filter by deployment if a deployment object is provided existing_multipart_files = existing_multipart_files.filter( deployment=deployment_object) if existing_multipart_files.exists(): if verbose: logger.info( \"Found existing multipart object in the database.\") # Retrieve the first matching multipart object multipart_obj = existing_multipart_files.first() # Update recording datetime, deployment, and device based on the multipart object recording_dt = [multipart_obj.recording_dt] deployment_object = multipart_obj.deployment device_object = deployment_object.device # Extract checksum from extra data multipart_checksum = extra_data[0].get(\"md5_checksum\") else: if verbose: logger.info( \"Checking for duplicate filenames in the database...\") # Query the database for filenames that already exist db_filenames = list( DataFile.objects.filter(original_name__in=filenames).values_list('original_name', flat=True)) # Identify files that are not duplicated not_duplicated = [x not in db_filenames for x in filenames] files = [x for x, y in zip(files, not_duplicated) if y] existing_files += [{x: {\"message\": \"Already in database\", \"status\": 200}} for x, y in zip(filenames, not_duplicated) if not y] # If all files are duplicates, return early with a success status if len(files) == 0: if verbose: logger.info(\"All files are already in the database.\") return (uploaded_files, invalid_files, existing_files, status.HTTP_200_OK) # Filter recording datetime values based on non-duplicated files if recording_dt and len(recording_dt) > 1: recording_dt = [x for x, y in zip( recording_dt, not_duplicated) if y] # Filter extra data based on non-duplicated files if len(extra_data) > 1: extra_data = [x for x, y in zip( extra_data, not_duplicated) if y] # Filter data types based on non-duplicated files if data_types is not None: if len(data_types) > 1: data_types = [x for x, y in zip( data_types, not_duplicated) if y] # If no device_object is provided but a deployment_object exists, set the device_object from the deployment_object if device_object is None and deployment_object: if verbose: logger.info(\"Setting device_object from deployment_object...\") device_object = deployment_object.device # Initialize handler_tasks to None handler_tasks = None # If a device_object is available, process the files using the associated device model if device_object: if verbose: logger.info(\"Processing files with device_object...\") # Retrieve the device model object and data handlers from settings device_model_object = device_object.model data_handlers = settings.DATA_HANDLERS # Get the appropriate data handler for the device model type and name data_handler = data_handlers.get_handler( device_model_object.type.name, device_model_object.name) # If a data handler is found, validate and process the files if data_handler is not None: if verbose: logger.info( f\"Using data handler for {device_model_object.name}...\") # Validate the files using the data handler valid_files = data_handler.get_valid_files(files) # If no valid files are found, return an error response if len(valid_files) == 0: if verbose: logger.info(\"No valid files found for the device model.\") invalid_files += [{x.name: {\"message\": f\"Invalid file type for {device_model_object.name}\", \"status\": 400}} for x in files] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) else: # Identify invalid files and update the invalid_files list valid_files_bool = [x in valid_files for x in files] invalid_files += [{x.name: {\"message\": f\"Invalid file type for {device_model_object.name}\", \"status\": 400}} for x, y in zip(files, valid_files_bool) if not y] # Filter out invalid files from the files list files = valid_files # Update recording_dt, extra_data, and other metadata based on valid files if recording_dt is not None and len(recording_dt) > 1: recording_dt = [x for x, y in zip( recording_dt, valid_files_bool) if y] if extra_data and len(extra_data) > 1: extra_data = [x for x, y in zip( extra_data, valid_files_bool) if y] # Initialize list to store updated metadata for valid files handler_return_list = [] # Process each valid file using the data handler for i in range(len(files)): # Retrieve extra_data for the current file if len(extra_data) > 1: file_extra_data = extra_data[i] else: file_extra_data = extra_data[0] # Retrieve recording_dt for the current file if recording_dt is None: file_recording_dt = recording_dt elif len(recording_dt) > 1: file_recording_dt = recording_dt[i] else: file_recording_dt = recording_dt[0] file = files[i] if verbose: logger.info( f\"Handling file {file.name} with data handler...\") # Use the data handler to process the file and extract updated metadata new_file_recording_dt, new_file_extra_data, new_file_data_type, new_file_task = \\ data_handler.handle_file( file, file_recording_dt, file_extra_data, device_model_object.type.name ) # Append the updated metadata to the lists handler_return_list.append({\"file\": file, \"file_name\": file.name, \"recording_dt\": new_file_recording_dt, \"extra_data\": new_file_extra_data, \"data_type\": new_file_data_type, \"task\": new_file_task}) else: # If no device_object is linked to the files, return an error response if verbose: logger.info(\"No linked device found for the files.\") invalid_files += [{x.name: {\"message\": \"No linked device\", \"status\": 400}} for x in files] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Check if all recording dates are None, indicating an inability to extract recording date times if all([x.get(\"recording_dt\") is None for x in handler_return_list]): # Add these files to the invalid_files list with an appropriate error message invalid_files += [{x.get(\"file_name\"): {\"message\": \"Unable to extract recording date time\", \"status\": 400}} for x in handler_return_list] # Return early with an HTTP 400 Bad Request status return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # If a deployment object is provided, validate permissions and recording dates if deployment_object: if verbose: logger.info(\"Checking permissions for deployment_object...\") if request_user: # Check if the user has permission to attach files to the deployment object if not request_user.has_perm('data_models.change_deployment', deployment_object): if verbose: logger.info( f\"User does not have permission to attach files to {deployment_object.deployment_device_ID}.\") # Add these files to the invalid_files list with a permission error message invalid_files += [ {x.get(\"file_name\"): { \"message\": f\"Not allowed to attach files to {deployment_object.deployment_device_ID}\", \"status\": 403}} for x in handler_return_list] # Return early with an HTTP 403 Forbidden status return (uploaded_files, invalid_files, existing_files, status.HTTP_403_FORBIDDEN) if verbose: logger.info(\"Validating recording dates for deployment_object...\") # Validate the recording dates against the deployment object valid_date_bool = deployment_object.check_dates( [x.get(\"recording_dt\") for x in handler_return_list]) # Set deployment_objects to a list containing the deployment_object deployment_objects = [deployment_object] # If no deployment object is provided, determine deployments from the device object and recording dates elif device_object: if verbose: logger.info( \"Determining deployments from device_object and recording dates...\") # Use the device object to find deployments based on recording dates deployment_objects = [device_object.deployment_from_date( x.get(\"recording_dt\")) for x in handler_return_list] # Check which deployments are valid (not None) valid_deployment_bool = [x is not None for x in deployment_objects] # Filter out None values from deployment_objects deployment_objects = [ x for x in deployment_objects if x is not None] if verbose: logger.info( \"Filtering invalid files based on deployment and recording dates...\") # Add invalid files to the invalid_files list with appropriate error messages if deployment_object: invalid_files += [{x.get(\"file_name\"): {\"message\": f\"Recording date time {z} does not exist in {deployment_object}\", \"status\": 400}} for x, y, z in zip(handler_return_list, valid_date_bool, recording_dt) if not y] # Filter out invalid files from the files list handler_return_list = [x for x, y in zip( handler_return_list, valid_date_bool) if y] else: invalid_files += [{x.get(\"file_name\"): {\"message\": f\"no suitable deployment of {device_object} found for recording date time {x.get('recording_dt')}\", \"status\": 400}} for x, y in zip(handler_return_list, valid_deployment_bool) if not y] # Filter out invalid files from the files list handler_return_list = [x for x, y in zip( handler_return_list, valid_deployment_bool) if y] # If no valid files remain after filtering, return an error response if len(handler_return_list) == 0: if verbose: logger.info(\"No valid files remain after filtering.\") return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Initialize lists to store project task primary keys, new DataFile objects, and handler tasks project_task_pks = [] all_new_objects = [] all_handler_tasks = [] # Process each valid file for i in range(len(handler_return_list)): handler_return = handler_return_list[i] file = handler_return.get(\"file\") filename = handler_return.get(\"file_name\") # Determine the deployment object for the current file if len(deployment_objects) > 1: file_deployment = deployment_objects[i] else: file_deployment = deployment_objects[0] if verbose: logger.info( f\"Processing file: {filename} for deployment: {file_deployment.deployment_device_ID}\") # Check if the user has permission to attach the file to the deployment if request_user: if not request_user.has_perm('data_models.change_deployment', file_deployment): if verbose: logger.info( f\"User does not have permission to attach file {filename} to {file_deployment.deployment_device_ID}.\") invalid_files.append( {filename: {\"message\": f\"Not allowed to attach files to {file_deployment.deployment_device_ID}\", \"status\": 403}}) continue # Determine the recording datetime for the current file file_recording_dt = handler_return.get(\"recording_dt\") # Retrieve the handler task for the current file, if available file_handler_task = handler_return.get(\"task\") if verbose: logger.info( f\"Localizing recording date time for file: {filename}...\") # Localize the recording datetime based on the deployment's timezone file_recording_dt = check_dt( file_recording_dt, file_deployment.time_zone) file_extra_data = handler_return.get(\"extra_data\") file_data_type_name = handler_return.get(\"data_type\") # Determine the data type for the current file file_data_type, created = DataType.objects.get_or_create( name=file_data_type_name) if verbose: logger.info(f\"Setting local path for file: {filename}...\") # Set the local path for the file based on the storage root file_local_path = os.path.join(settings.FILE_STORAGE_ROOT) # Check if the file is not part of a multipart upload or if it's a new multipart object if not multipart or (multipart and multipart_obj is None): # Log the process of setting the path for the file if verbose: logger.info(f\"Setting path for file: {filename}...\") # Construct the file path using the data type name, deployment device ID, and upload date file_path = os.path.join(file_data_type.name, file_deployment.deployment_device_ID, str(upload_dt.date())) # Extract the file extension from the original filename file_extension = os.path.splitext(filename)[1] # Generate a new unique name for the file based on deployment, recording datetime, and file count new_file_name = get_new_name(file_deployment, file_recording_dt, file_local_path, file_path ) # Get the size of the file file_size = file.size # Construct the full path where the file will be stored locally file_fullpath = os.path.join( file_local_path, file_path, f\"{new_file_name}{file_extension}\") # Log the creation of the database object for the file if verbose: logger.info(f\"Creating database object for: {filename}...\") # If the file is part of a multipart upload, mark it as incomplete in the extra data if multipart: file_extra_data[\"multipart_complete\"] = False # Create a new DataFile object with all the relevant metadata new_datafile_obj = DataFile( deployment=file_deployment, # Associated deployment file_type=file_data_type, # Type of the file file_name=new_file_name, # Generated unique name for the file original_name=filename, # Original name of the file file_format=file_extension, # File extension upload_dt=upload_dt, # Upload datetime recording_dt=file_recording_dt, # Recording datetime path=file_path, # Relative path for the file local_path=file_local_path, # Local storage path file_size=file_size, # Size of the file extra_data=file_extra_data # Additional metadata ) try: # Validate the new DataFile object to ensure all fields meet the model's constraints new_datafile_obj.full_clean() except ValidationError as e: # Handle validation errors specific to the DataFile model if verbose: logger.info( f\"Error creating database objects for: {filename}...\") # Add the file to the invalid_files list with a detailed error message invalid_files.append( {filename: {\"message\": f\"Error creating database records {repr(e)}\", \"status\": 400}}) # Skip further processing for this file continue except Exception as e: # Handle any other unexpected exceptions during validation invalid_files.append( {filename: {\"message\": repr(e), \"status\": 400}}) # Skip further processing for this file continue else: # Retrieve the full path for the multipart object file_fullpath = multipart_obj.full_path() try: if verbose: logger.info(f\"Saving file to path: {file_fullpath}...\") # Try to save the file handle_uploaded_file(file, file_fullpath, multipart, verbose) except Exception as e: if verbose: logger.info( f\"Error handling uploaded file for: {filename} - {repr(e)}\") invalid_files.append( {filename: {\"message\": repr(e), \"status\": 400}}) if multipart: # This is a complete failure when multipart return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) continue if not multipart or (multipart and multipart_obj is None): # Set the file URL when first registered in the database if verbose: logger.info(f\"Setting file URL for: {filename}...\") new_datafile_obj.set_file_url() all_new_objects.append(new_datafile_obj) # If a single file or a completing (checksum received) multipart if not multipart or (multipart and multipart_checksum is not None): # Flag to append tasks for processing append_tasks = True if multipart: # Perform MD5 checksum validation for multipart file uploads if verbose: logger.info( f\"Performing MD5 checksum validation for multipart file: {multipart_obj.original_name}...\") # Calculate the server-side checksum of the uploaded file server_checksum = get_md5(multipart_obj.full_path()) if verbose: logger.info( f\"Server checksum: {server_checksum}, Client checksum: {multipart_checksum}\") # Compare the server checksum with the client-provided checksum if not multipart_checksum == server_checksum: # If the checksums do not match, log the mismatch and add an error to invalid_files if verbose: logger.info( f\"Checksum mismatch for multipart file: {multipart_obj.original_name}\") invalid_files += [{multipart_obj.original_name: { \"message\": \"Multipart file upload checksum mismatch\", \"status\": 400}}] # Return early with an HTTP 400 Bad Request status return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) else: # If the checksums match, log the success and update the multipart file metadata if verbose: logger.info( f\"Checksum validation passed for multipart file: {multipart_obj.original_name}\") # Update the extra_data field with the validated checksum multipart_extra_data = multipart_obj.extra_data multipart_extra_data['md5_checksum'] = server_checksum # Remove the multipart_complete flag from the metadata multipart_extra_data.pop(\"multipart_complete\") # Save the updated metadata to the database multipart_obj.extra_data = multipart_extra_data multipart_obj.save() else: # Do not perform post upload tasks append_tasks = False # If post upload tasks are to be performed if append_tasks: # Fetch deployment tasks associated with the current file's deployment if verbose: logger.info( f\"Fetching deployment tasks for file: {filename}...\") file_deployment_tasks = list(file_deployment.project.all().values_list( 'automated_tasks__pk', flat=True)) # Retrieve primary keys of automated tasks linked to the deployment file_deployment_tasks = [ x for x in file_deployment_tasks if x is not None] # Filter out None values from the task list if verbose: logger.info( f\"Deployment tasks for file {filename}: {file_deployment_tasks}\") # Append the handler task for the current file to the list of all handler tasks all_handler_tasks.append( {\"original_name\": filename, \"task\": file_handler_task}) if verbose: logger.info( f\"Handler task for file {filename}: {file_handler_task}\") # Append the deployment tasks for the current file to the list of project task primary keys project_task_pks.append( {\"original_name\": filename, \"tasks\": file_deployment_tasks}) final_status = status.HTTP_200_OK if len(all_new_objects) > 0 or multipart: # If new objects are to be created if len(all_new_objects) > 0: if verbose: logger.info( f\"Bulk creating {len(all_new_objects)} new DataFile objects...\") uploaded_files = DataFile.objects.bulk_create(all_new_objects) uploaded_files_name_pks = [ {\"original_name\": x.original_name, \"pk\": x.pk} for x in uploaded_files] if verbose: logger.info( f\"Created DataFile objects with primary keys: {uploaded_files_name_pks}\") final_status = status.HTTP_201_CREATED # Otherwise if this part of a multipart upload elif multipart: uploaded_files = [multipart_obj] uploaded_files_name_pks = [ {\"original_name\": multipart_obj.original_name, \"pk\": multipart_obj.pk}] if verbose: logger.info( f\"Using existing multipart object with primary key: {multipart_obj.pk}\") # Is multipart completing if multipart_checksum is not None: # Multipart done final_status = status.HTTP_200_OK else: # Multipart continues final_status = status.HTTP_100_CONTINUE # Get all tasks all_tasks = [] # For unique data handler tasks, fire off jobs to perform them all_task_names = [x.get(\"task\") for x in all_handler_tasks] all_task_names_filtered = [x for x in all_task_names if x is not None] unique_tasks = list( set(all_task_names_filtered)) if verbose: logger.info( f\"Found unique tasks: {unique_tasks}\") if len(unique_tasks) > 0: for task_name in unique_tasks: # get original_name associated with this task task_original_names = [ x.get(\"original_name\") for x in all_handler_tasks if x.get(\"task\") == task_name] # get pks for this task task_file_pks = [x.get(\"pk\") for x in uploaded_files_name_pks if x.get( \"original_name\") in task_original_names] if verbose: logger.info( f\"Task {task_name}: {task_file_pks}\") if len(task_file_pks) > 0: new_task = app.signature( task_name, [task_file_pks], immutable=True) all_tasks.append(new_task) if verbose: logger.info( f\"All tasks:{all_tasks}\") # For unique project tasks, fire off jobs to perform them flat_project_task_pks = [ x for internal_list in project_task_pks for x in internal_list.get(\"tasks\")] unique_project_task_pks = list(set(flat_project_task_pks)) if len(unique_project_task_pks) > 0: for project_task_pk in unique_project_task_pks: project_task_original_names = [ x.get(\"original_name\") for x in project_task_pks if project_task_pk in x.get(\"task\")] project_task_file_pks = [x.get(\"pk\") for x in uploaded_files_name_pks if x.get( \"original_name\") in project_task_original_names] if len(task_file_pks) > 0: # get signature from the project job db object task_obj = ProjectJob.objects.get(pk=project_task_pk) new_task = task_obj.get_job_signature( project_task_file_pks) all_tasks.append(new_task) if len(all_tasks) > 0: task_chain = chain(all_tasks) task_chain.apply_async() else: if verbose: logger.info(\"Determining final status based on invalid files...\") final_status = status.HTTP_400_BAD_REQUEST if all([[y[x].get('status') == 403 for x in y.keys()][0] for y in invalid_files]): if verbose: logger.info( \"All invalid files have a status of 403. Setting final status to HTTP_403_FORBIDDEN.\") final_status = status.HTTP_403_FORBIDDEN return (uploaded_files, invalid_files, existing_files, final_status) get_n_files(dir_path) Count the number of files in a directory that have an extension. Parameters: dir_path ( str ) \u2013 Directory path. Returns: int ( int ) \u2013 Number of files with an extension, or 0 if directory does not exist. Source code in data_models\\file_handling_functions.py def get_n_files(dir_path: str) -> int: \"\"\" Count the number of files in a directory that have an extension. Args: dir_path (str): Directory path. Returns: int: Number of files with an extension, or 0 if directory does not exist. \"\"\" if os.path.exists(dir_path): all_files = os.listdir(dir_path) # Filter files that have an extension all_files = [x for x in all_files if '.' in x] n_files = len(all_files) else: n_files = 0 return n_files get_new_name(deployment, recording_dt, file_local_path, file_path, file_n=None) Generate a unique file name based on deployment, recording datetime, and file count. Parameters: deployment ( Deployment ) \u2013 Associated deployment object. recording_dt ( datetime ) \u2013 File recording datetime. file_local_path ( str ) \u2013 Root local path for storage. file_path ( str ) \u2013 Relative file path within storage root. file_n ( Optional [ int ] , default: None ) \u2013 File count for uniqueness. Defaults to None. Returns: str ( str ) \u2013 Unique file name in the format \"{deployment_device_ID} {YYYY-MM-DD_HH-MM-SS} ({file_n})\" Source code in data_models\\file_handling_functions.py def get_new_name( deployment: \"Deployment\", recording_dt: dt, file_local_path: str, file_path: str, file_n: Optional[int] = None ) -> str: \"\"\" Generate a unique file name based on deployment, recording datetime, and file count. Args: deployment (Deployment): Associated deployment object. recording_dt (datetime): File recording datetime. file_local_path (str): Root local path for storage. file_path (str): Relative file path within storage root. file_n (Optional[int], optional): File count for uniqueness. Defaults to None. Returns: str: Unique file name in the format \"{deployment_device_ID}_{YYYY-MM-DD_HH-MM-SS}_({file_n})\" \"\"\" if file_n is None: file_n = get_n_files(os.path.join(file_local_path, file_path)) + 1 newname = f\"{deployment.deployment_device_ID}_{dt.strftime(recording_dt, '%Y-%m-%d_%H-%M-%S')}_({file_n})\" return newname group_files_by_size(file_objs, max_size=settings.MAX_ARCHIVE_SIZE_GB) Group files into batches by size, ensuring each batch does not exceed max_size (GB). Parameters: file_objs ( QuerySet ) \u2013 Django QuerySet with 'pk' and 'file_size' attributes. max_size ( float , default: MAX_ARCHIVE_SIZE_GB ) \u2013 Maximum group size in GB. Defaults to settings.MAX_ARCHIVE_SIZE_GB. Returns: list [ dict [ str , float | list [ int ]]] \u2013 list[dict[str, float | list[int]]]: List of groups, where each dict contains: - \"file_pks\": List[int] - Primary keys of files in the group. - \"total_size_gb\": float - Total size of the group in GB. Notes Groups files in order of their 'recording_dt' attribute. Source code in data_models\\file_handling_functions.py def group_files_by_size( file_objs: QuerySet, max_size: float = settings.MAX_ARCHIVE_SIZE_GB ) -> list[dict[str, float | list[int]]]: \"\"\" Group files into batches by size, ensuring each batch does not exceed max_size (GB). Args: file_objs (QuerySet): Django QuerySet with 'pk' and 'file_size' attributes. max_size (float, optional): Maximum group size in GB. Defaults to settings.MAX_ARCHIVE_SIZE_GB. Returns: list[dict[str, float | list[int]]]: List of groups, where each dict contains: - \"file_pks\": List[int] - Primary keys of files in the group. - \"total_size_gb\": float - Total size of the group in GB. Notes: - Groups files in order of their 'recording_dt' attribute. \"\"\" # Initialize variables to track the current group key, total size, and file information curr_key = 0 curr_total = 0 file_info = [] # Order the file objects by their recording datetime to ensure logical grouping file_objs = file_objs.order_by('recording_dt') # Extract primary key and file size values from the file objects file_values = file_objs.values('pk', 'file_size') for file_value in file_values: # Convert the file size to GB for comparison file_size = convert_unit(file_value['file_size'], \"GB\") # Check if adding the current file would exceed the maximum allowed size for a group if (curr_total + file_size) > max_size: # If the new file would push over the max size, start a new group curr_total = file_size # Reset the current total size to the new file's size curr_key += 1 # Increment the group key to start a new group else: # Otherwise, add the file's size to the current group's total size curr_total += file_size # Append the file's information along with its assigned group key file_info.append( {\"pk\": file_value['pk'], \"file_size\": file_size, \"key\": curr_key}) # Initialize an empty list to store the grouped file information groups = [] # Use itertools.groupby to group files by their assigned group key for k, g in itertools.groupby(file_info, lambda x: x.get(\"key\")): # Convert the group iterator into a list for processing files = list(g) # Calculate the total size of the files in the current group total_size_gb = sum([x.get('file_size') for x in files]) # Extract the primary keys of the files in the current group file_pks = [x.get('pk') for x in files] # Append the group information (file primary keys and total size) to the groups list groups.append({\"file_pks\": file_pks, \"total_size_gb\": total_size_gb}) return groups handle_uploaded_file(file, filepath, multipart=False, verbose=False) Upload and save a file to the specified filepath. Parameters: file ( Union [ object , UploadedFile ] ) \u2013 The file object to save. Must provide a chunks() method. filepath ( str ) \u2013 The destination path. multipart ( bool , default: False ) \u2013 If True, append to an existing file. Defaults to False. verbose ( bool , default: False ) \u2013 If True, log debug info. Defaults to False. Raises: OSError \u2013 If creating directories or writing to the file fails. Example handle_uploaded_file( uploaded_file, '/path/to/save/file.txt', multipart=True, verbose=True) Source code in data_models\\file_handling_functions.py def handle_uploaded_file( file: Union[object, UploadedFile], filepath: str, multipart: bool = False, verbose: bool = False ) -> None: \"\"\" Upload and save a file to the specified filepath. Args: file (Union[object, UploadedFile]): The file object to save. Must provide a `chunks()` method. filepath (str): The destination path. multipart (bool, optional): If True, append to an existing file. Defaults to False. verbose (bool, optional): If True, log debug info. Defaults to False. Raises: OSError: If creating directories or writing to the file fails. Example: handle_uploaded_file( uploaded_file, '/path/to/save/file.txt', multipart=True, verbose=True) \"\"\" os.makedirs(os.path.split(filepath)[0], exist_ok=True) if multipart and os.path.exists(filepath): if verbose: logger.info(f\"Appending to {filepath}\") with open(filepath, 'ab+') as destination: for chunk in file.chunks(): destination.write(chunk) else: if verbose: logger.info(f\"Writing to {filepath}\") with open(filepath, 'wb+') as destination: for chunk in file.chunks(): destination.write(chunk)","title":"file_handling_functions"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.create_file_objects","text":"Create file objects, handle uploads, validations, and database record creation. Parameters: files ( List [ Union [ object , UploadedFile ]] ) \u2013 List of file objects to process. check_filename ( bool , default: False ) \u2013 If True, check for duplicate filenames in the database. Defaults to False. recording_dt ( Optional [ List [ Optional [ datetime ]]] , default: None ) \u2013 List of recording datetimes for the files. Defaults to None. extra_data ( Optional [ List [ Dict [ str , Union [ str , int , float , bool , None]]]] , default: None ) \u2013 List of additional metadata for the files. Defaults to None. deployment_object ( Optional [ Deployment ] , default: None ) \u2013 Deployment object associated with the files. Defaults to None. device_object ( Optional [ Device ] , default: None ) \u2013 Device object associated with the files. Defaults to None. data_types ( Optional [ List [ str ]] , default: None ) \u2013 List of data types for the files. Defaults to None. request_user ( Optional [ User ] , default: None ) \u2013 User object for permission checks. Defaults to None. multipart ( bool , default: False ) \u2013 If True, handle multipart file uploads. Defaults to False. verbose ( bool , default: True ) \u2013 If True, enable verbose logging. Defaults to False. Returns: List [ DataFile ] \u2013 Tuple[ List[DataFile], # Successfully uploaded file objects","title":"create_file_objects"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.create_file_objects--invalid-files-with-errors","text":"List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], # Files already in DB int # HTTP status code List [ Dict [ str , Dict [ str , Union [ str , int ]]]] \u2013 ] Raises: ValidationError \u2013 If there is an error validating the database records. Exception \u2013 For any other errors during file handling or database operations. Notes Handles duplicate filename checks and multipart uploads. Validates file types based on the device model. Checks permissions for attaching files to deployments. Filters files based on recording datetime and deployment validity. Creates database records and saves valid files. Supports automated tasks and checksum validation. Source code in data_models\\file_handling_functions.py def create_file_objects( files: List[Union[object, UploadedFile]], check_filename: bool = False, recording_dt: Optional[List[Optional[dt]]] = None, extra_data: Optional[List[Dict[str, Union[str, int, float, bool, None]]]] = None, deployment_object: Optional[\"Deployment\"] = None, device_object: Optional[\"Device\"] = None, data_types: Optional[List[str]] = None, request_user: Optional[\"User\"] = None, multipart: bool = False, verbose: bool = True ) -> Tuple[ List[\"DataFile\"], List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], int ]: \"\"\" Create file objects, handle uploads, validations, and database record creation. Args: files (List[Union[object, UploadedFile]]): List of file objects to process. check_filename (bool, optional): If True, check for duplicate filenames in the database. Defaults to False. recording_dt (Optional[List[Optional[datetime]]], optional): List of recording datetimes for the files. Defaults to None. extra_data (Optional[List[Dict[str, Union[str, int, float, bool, None]]]], optional): List of additional metadata for the files. Defaults to None. deployment_object (Optional[Deployment], optional): Deployment object associated with the files. Defaults to None. device_object (Optional[Device], optional): Device object associated with the files. Defaults to None. data_types (Optional[List[str]], optional): List of data types for the files. Defaults to None. request_user (Optional[User], optional): User object for permission checks. Defaults to None. multipart (bool, optional): If True, handle multipart file uploads. Defaults to False. verbose (bool, optional): If True, enable verbose logging. Defaults to False. Returns: Tuple[ List[DataFile], # Successfully uploaded file objects # Invalid files with errors List[Dict[str, Dict[str, Union[str, int]]]], List[Dict[str, Dict[str, Union[str, int]]]], # Files already in DB int # HTTP status code ] Raises: ValidationError: If there is an error validating the database records. Exception: For any other errors during file handling or database operations. Notes: - Handles duplicate filename checks and multipart uploads. - Validates file types based on the device model. - Checks permissions for attaching files to deployments. - Filters files based on recording datetime and deployment validity. - Creates database records and saves valid files. - Supports automated tasks and checksum validation. \"\"\" from data_models.models import DataFile, DataType, ProjectJob invalid_files = [] existing_files = [] uploaded_files = [] if extra_data is None: extra_data = [{}] if verbose: logger.info(\"Initial files:\", files) logger.info(\"Device object:\", device_object) logger.info(\"Deployment object:\", deployment_object) logger.info(\"Recording datetime:\", recording_dt) logger.info(\"Extra data:\", extra_data) logger.info(\"Data types:\", data_types) logger.info(\"Request user:\", request_user) # Get the current upload datetime upload_dt = djtimezone.now() # Check for duplicate filenames or handle multipart uploads if check_filename or multipart: if verbose: logger.info(\"Checking filenames or handling multipart upload...\") # Extract filenames from the provided file objects filenames = [x.name for x in files] if multipart: if verbose: logger.info(\"Handling multipart upload...\") # Ensure only one file chunk is provided for multipart uploads if len(filenames) > 1: invalid_files += [{x: {\"message\": \"Multipart file upload expects a single file chunk\", \"status\": 400}} for x, in filenames] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Initialize multipart-related variables multipart_obj = None multipart_checksum = None # Query the database for existing multipart files with matching filenames existing_multipart_files = DataFile.objects.filter( original_name__in=filenames, extra_data__md5_checksum__isnull=True) if device_object: # Filter by device if a device object is provided existing_multipart_files = existing_multipart_files.filter( deployment__device=device_object) if deployment_object: # Filter by deployment if a deployment object is provided existing_multipart_files = existing_multipart_files.filter( deployment=deployment_object) if existing_multipart_files.exists(): if verbose: logger.info( \"Found existing multipart object in the database.\") # Retrieve the first matching multipart object multipart_obj = existing_multipart_files.first() # Update recording datetime, deployment, and device based on the multipart object recording_dt = [multipart_obj.recording_dt] deployment_object = multipart_obj.deployment device_object = deployment_object.device # Extract checksum from extra data multipart_checksum = extra_data[0].get(\"md5_checksum\") else: if verbose: logger.info( \"Checking for duplicate filenames in the database...\") # Query the database for filenames that already exist db_filenames = list( DataFile.objects.filter(original_name__in=filenames).values_list('original_name', flat=True)) # Identify files that are not duplicated not_duplicated = [x not in db_filenames for x in filenames] files = [x for x, y in zip(files, not_duplicated) if y] existing_files += [{x: {\"message\": \"Already in database\", \"status\": 200}} for x, y in zip(filenames, not_duplicated) if not y] # If all files are duplicates, return early with a success status if len(files) == 0: if verbose: logger.info(\"All files are already in the database.\") return (uploaded_files, invalid_files, existing_files, status.HTTP_200_OK) # Filter recording datetime values based on non-duplicated files if recording_dt and len(recording_dt) > 1: recording_dt = [x for x, y in zip( recording_dt, not_duplicated) if y] # Filter extra data based on non-duplicated files if len(extra_data) > 1: extra_data = [x for x, y in zip( extra_data, not_duplicated) if y] # Filter data types based on non-duplicated files if data_types is not None: if len(data_types) > 1: data_types = [x for x, y in zip( data_types, not_duplicated) if y] # If no device_object is provided but a deployment_object exists, set the device_object from the deployment_object if device_object is None and deployment_object: if verbose: logger.info(\"Setting device_object from deployment_object...\") device_object = deployment_object.device # Initialize handler_tasks to None handler_tasks = None # If a device_object is available, process the files using the associated device model if device_object: if verbose: logger.info(\"Processing files with device_object...\") # Retrieve the device model object and data handlers from settings device_model_object = device_object.model data_handlers = settings.DATA_HANDLERS # Get the appropriate data handler for the device model type and name data_handler = data_handlers.get_handler( device_model_object.type.name, device_model_object.name) # If a data handler is found, validate and process the files if data_handler is not None: if verbose: logger.info( f\"Using data handler for {device_model_object.name}...\") # Validate the files using the data handler valid_files = data_handler.get_valid_files(files) # If no valid files are found, return an error response if len(valid_files) == 0: if verbose: logger.info(\"No valid files found for the device model.\") invalid_files += [{x.name: {\"message\": f\"Invalid file type for {device_model_object.name}\", \"status\": 400}} for x in files] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) else: # Identify invalid files and update the invalid_files list valid_files_bool = [x in valid_files for x in files] invalid_files += [{x.name: {\"message\": f\"Invalid file type for {device_model_object.name}\", \"status\": 400}} for x, y in zip(files, valid_files_bool) if not y] # Filter out invalid files from the files list files = valid_files # Update recording_dt, extra_data, and other metadata based on valid files if recording_dt is not None and len(recording_dt) > 1: recording_dt = [x for x, y in zip( recording_dt, valid_files_bool) if y] if extra_data and len(extra_data) > 1: extra_data = [x for x, y in zip( extra_data, valid_files_bool) if y] # Initialize list to store updated metadata for valid files handler_return_list = [] # Process each valid file using the data handler for i in range(len(files)): # Retrieve extra_data for the current file if len(extra_data) > 1: file_extra_data = extra_data[i] else: file_extra_data = extra_data[0] # Retrieve recording_dt for the current file if recording_dt is None: file_recording_dt = recording_dt elif len(recording_dt) > 1: file_recording_dt = recording_dt[i] else: file_recording_dt = recording_dt[0] file = files[i] if verbose: logger.info( f\"Handling file {file.name} with data handler...\") # Use the data handler to process the file and extract updated metadata new_file_recording_dt, new_file_extra_data, new_file_data_type, new_file_task = \\ data_handler.handle_file( file, file_recording_dt, file_extra_data, device_model_object.type.name ) # Append the updated metadata to the lists handler_return_list.append({\"file\": file, \"file_name\": file.name, \"recording_dt\": new_file_recording_dt, \"extra_data\": new_file_extra_data, \"data_type\": new_file_data_type, \"task\": new_file_task}) else: # If no device_object is linked to the files, return an error response if verbose: logger.info(\"No linked device found for the files.\") invalid_files += [{x.name: {\"message\": \"No linked device\", \"status\": 400}} for x in files] return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Check if all recording dates are None, indicating an inability to extract recording date times if all([x.get(\"recording_dt\") is None for x in handler_return_list]): # Add these files to the invalid_files list with an appropriate error message invalid_files += [{x.get(\"file_name\"): {\"message\": \"Unable to extract recording date time\", \"status\": 400}} for x in handler_return_list] # Return early with an HTTP 400 Bad Request status return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # If a deployment object is provided, validate permissions and recording dates if deployment_object: if verbose: logger.info(\"Checking permissions for deployment_object...\") if request_user: # Check if the user has permission to attach files to the deployment object if not request_user.has_perm('data_models.change_deployment', deployment_object): if verbose: logger.info( f\"User does not have permission to attach files to {deployment_object.deployment_device_ID}.\") # Add these files to the invalid_files list with a permission error message invalid_files += [ {x.get(\"file_name\"): { \"message\": f\"Not allowed to attach files to {deployment_object.deployment_device_ID}\", \"status\": 403}} for x in handler_return_list] # Return early with an HTTP 403 Forbidden status return (uploaded_files, invalid_files, existing_files, status.HTTP_403_FORBIDDEN) if verbose: logger.info(\"Validating recording dates for deployment_object...\") # Validate the recording dates against the deployment object valid_date_bool = deployment_object.check_dates( [x.get(\"recording_dt\") for x in handler_return_list]) # Set deployment_objects to a list containing the deployment_object deployment_objects = [deployment_object] # If no deployment object is provided, determine deployments from the device object and recording dates elif device_object: if verbose: logger.info( \"Determining deployments from device_object and recording dates...\") # Use the device object to find deployments based on recording dates deployment_objects = [device_object.deployment_from_date( x.get(\"recording_dt\")) for x in handler_return_list] # Check which deployments are valid (not None) valid_deployment_bool = [x is not None for x in deployment_objects] # Filter out None values from deployment_objects deployment_objects = [ x for x in deployment_objects if x is not None] if verbose: logger.info( \"Filtering invalid files based on deployment and recording dates...\") # Add invalid files to the invalid_files list with appropriate error messages if deployment_object: invalid_files += [{x.get(\"file_name\"): {\"message\": f\"Recording date time {z} does not exist in {deployment_object}\", \"status\": 400}} for x, y, z in zip(handler_return_list, valid_date_bool, recording_dt) if not y] # Filter out invalid files from the files list handler_return_list = [x for x, y in zip( handler_return_list, valid_date_bool) if y] else: invalid_files += [{x.get(\"file_name\"): {\"message\": f\"no suitable deployment of {device_object} found for recording date time {x.get('recording_dt')}\", \"status\": 400}} for x, y in zip(handler_return_list, valid_deployment_bool) if not y] # Filter out invalid files from the files list handler_return_list = [x for x, y in zip( handler_return_list, valid_deployment_bool) if y] # If no valid files remain after filtering, return an error response if len(handler_return_list) == 0: if verbose: logger.info(\"No valid files remain after filtering.\") return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) # Initialize lists to store project task primary keys, new DataFile objects, and handler tasks project_task_pks = [] all_new_objects = [] all_handler_tasks = [] # Process each valid file for i in range(len(handler_return_list)): handler_return = handler_return_list[i] file = handler_return.get(\"file\") filename = handler_return.get(\"file_name\") # Determine the deployment object for the current file if len(deployment_objects) > 1: file_deployment = deployment_objects[i] else: file_deployment = deployment_objects[0] if verbose: logger.info( f\"Processing file: {filename} for deployment: {file_deployment.deployment_device_ID}\") # Check if the user has permission to attach the file to the deployment if request_user: if not request_user.has_perm('data_models.change_deployment', file_deployment): if verbose: logger.info( f\"User does not have permission to attach file {filename} to {file_deployment.deployment_device_ID}.\") invalid_files.append( {filename: {\"message\": f\"Not allowed to attach files to {file_deployment.deployment_device_ID}\", \"status\": 403}}) continue # Determine the recording datetime for the current file file_recording_dt = handler_return.get(\"recording_dt\") # Retrieve the handler task for the current file, if available file_handler_task = handler_return.get(\"task\") if verbose: logger.info( f\"Localizing recording date time for file: {filename}...\") # Localize the recording datetime based on the deployment's timezone file_recording_dt = check_dt( file_recording_dt, file_deployment.time_zone) file_extra_data = handler_return.get(\"extra_data\") file_data_type_name = handler_return.get(\"data_type\") # Determine the data type for the current file file_data_type, created = DataType.objects.get_or_create( name=file_data_type_name) if verbose: logger.info(f\"Setting local path for file: {filename}...\") # Set the local path for the file based on the storage root file_local_path = os.path.join(settings.FILE_STORAGE_ROOT) # Check if the file is not part of a multipart upload or if it's a new multipart object if not multipart or (multipart and multipart_obj is None): # Log the process of setting the path for the file if verbose: logger.info(f\"Setting path for file: {filename}...\") # Construct the file path using the data type name, deployment device ID, and upload date file_path = os.path.join(file_data_type.name, file_deployment.deployment_device_ID, str(upload_dt.date())) # Extract the file extension from the original filename file_extension = os.path.splitext(filename)[1] # Generate a new unique name for the file based on deployment, recording datetime, and file count new_file_name = get_new_name(file_deployment, file_recording_dt, file_local_path, file_path ) # Get the size of the file file_size = file.size # Construct the full path where the file will be stored locally file_fullpath = os.path.join( file_local_path, file_path, f\"{new_file_name}{file_extension}\") # Log the creation of the database object for the file if verbose: logger.info(f\"Creating database object for: {filename}...\") # If the file is part of a multipart upload, mark it as incomplete in the extra data if multipart: file_extra_data[\"multipart_complete\"] = False # Create a new DataFile object with all the relevant metadata new_datafile_obj = DataFile( deployment=file_deployment, # Associated deployment file_type=file_data_type, # Type of the file file_name=new_file_name, # Generated unique name for the file original_name=filename, # Original name of the file file_format=file_extension, # File extension upload_dt=upload_dt, # Upload datetime recording_dt=file_recording_dt, # Recording datetime path=file_path, # Relative path for the file local_path=file_local_path, # Local storage path file_size=file_size, # Size of the file extra_data=file_extra_data # Additional metadata ) try: # Validate the new DataFile object to ensure all fields meet the model's constraints new_datafile_obj.full_clean() except ValidationError as e: # Handle validation errors specific to the DataFile model if verbose: logger.info( f\"Error creating database objects for: {filename}...\") # Add the file to the invalid_files list with a detailed error message invalid_files.append( {filename: {\"message\": f\"Error creating database records {repr(e)}\", \"status\": 400}}) # Skip further processing for this file continue except Exception as e: # Handle any other unexpected exceptions during validation invalid_files.append( {filename: {\"message\": repr(e), \"status\": 400}}) # Skip further processing for this file continue else: # Retrieve the full path for the multipart object file_fullpath = multipart_obj.full_path() try: if verbose: logger.info(f\"Saving file to path: {file_fullpath}...\") # Try to save the file handle_uploaded_file(file, file_fullpath, multipart, verbose) except Exception as e: if verbose: logger.info( f\"Error handling uploaded file for: {filename} - {repr(e)}\") invalid_files.append( {filename: {\"message\": repr(e), \"status\": 400}}) if multipart: # This is a complete failure when multipart return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) continue if not multipart or (multipart and multipart_obj is None): # Set the file URL when first registered in the database if verbose: logger.info(f\"Setting file URL for: {filename}...\") new_datafile_obj.set_file_url() all_new_objects.append(new_datafile_obj) # If a single file or a completing (checksum received) multipart if not multipart or (multipart and multipart_checksum is not None): # Flag to append tasks for processing append_tasks = True if multipart: # Perform MD5 checksum validation for multipart file uploads if verbose: logger.info( f\"Performing MD5 checksum validation for multipart file: {multipart_obj.original_name}...\") # Calculate the server-side checksum of the uploaded file server_checksum = get_md5(multipart_obj.full_path()) if verbose: logger.info( f\"Server checksum: {server_checksum}, Client checksum: {multipart_checksum}\") # Compare the server checksum with the client-provided checksum if not multipart_checksum == server_checksum: # If the checksums do not match, log the mismatch and add an error to invalid_files if verbose: logger.info( f\"Checksum mismatch for multipart file: {multipart_obj.original_name}\") invalid_files += [{multipart_obj.original_name: { \"message\": \"Multipart file upload checksum mismatch\", \"status\": 400}}] # Return early with an HTTP 400 Bad Request status return (uploaded_files, invalid_files, existing_files, status.HTTP_400_BAD_REQUEST) else: # If the checksums match, log the success and update the multipart file metadata if verbose: logger.info( f\"Checksum validation passed for multipart file: {multipart_obj.original_name}\") # Update the extra_data field with the validated checksum multipart_extra_data = multipart_obj.extra_data multipart_extra_data['md5_checksum'] = server_checksum # Remove the multipart_complete flag from the metadata multipart_extra_data.pop(\"multipart_complete\") # Save the updated metadata to the database multipart_obj.extra_data = multipart_extra_data multipart_obj.save() else: # Do not perform post upload tasks append_tasks = False # If post upload tasks are to be performed if append_tasks: # Fetch deployment tasks associated with the current file's deployment if verbose: logger.info( f\"Fetching deployment tasks for file: {filename}...\") file_deployment_tasks = list(file_deployment.project.all().values_list( 'automated_tasks__pk', flat=True)) # Retrieve primary keys of automated tasks linked to the deployment file_deployment_tasks = [ x for x in file_deployment_tasks if x is not None] # Filter out None values from the task list if verbose: logger.info( f\"Deployment tasks for file {filename}: {file_deployment_tasks}\") # Append the handler task for the current file to the list of all handler tasks all_handler_tasks.append( {\"original_name\": filename, \"task\": file_handler_task}) if verbose: logger.info( f\"Handler task for file {filename}: {file_handler_task}\") # Append the deployment tasks for the current file to the list of project task primary keys project_task_pks.append( {\"original_name\": filename, \"tasks\": file_deployment_tasks}) final_status = status.HTTP_200_OK if len(all_new_objects) > 0 or multipart: # If new objects are to be created if len(all_new_objects) > 0: if verbose: logger.info( f\"Bulk creating {len(all_new_objects)} new DataFile objects...\") uploaded_files = DataFile.objects.bulk_create(all_new_objects) uploaded_files_name_pks = [ {\"original_name\": x.original_name, \"pk\": x.pk} for x in uploaded_files] if verbose: logger.info( f\"Created DataFile objects with primary keys: {uploaded_files_name_pks}\") final_status = status.HTTP_201_CREATED # Otherwise if this part of a multipart upload elif multipart: uploaded_files = [multipart_obj] uploaded_files_name_pks = [ {\"original_name\": multipart_obj.original_name, \"pk\": multipart_obj.pk}] if verbose: logger.info( f\"Using existing multipart object with primary key: {multipart_obj.pk}\") # Is multipart completing if multipart_checksum is not None: # Multipart done final_status = status.HTTP_200_OK else: # Multipart continues final_status = status.HTTP_100_CONTINUE # Get all tasks all_tasks = [] # For unique data handler tasks, fire off jobs to perform them all_task_names = [x.get(\"task\") for x in all_handler_tasks] all_task_names_filtered = [x for x in all_task_names if x is not None] unique_tasks = list( set(all_task_names_filtered)) if verbose: logger.info( f\"Found unique tasks: {unique_tasks}\") if len(unique_tasks) > 0: for task_name in unique_tasks: # get original_name associated with this task task_original_names = [ x.get(\"original_name\") for x in all_handler_tasks if x.get(\"task\") == task_name] # get pks for this task task_file_pks = [x.get(\"pk\") for x in uploaded_files_name_pks if x.get( \"original_name\") in task_original_names] if verbose: logger.info( f\"Task {task_name}: {task_file_pks}\") if len(task_file_pks) > 0: new_task = app.signature( task_name, [task_file_pks], immutable=True) all_tasks.append(new_task) if verbose: logger.info( f\"All tasks:{all_tasks}\") # For unique project tasks, fire off jobs to perform them flat_project_task_pks = [ x for internal_list in project_task_pks for x in internal_list.get(\"tasks\")] unique_project_task_pks = list(set(flat_project_task_pks)) if len(unique_project_task_pks) > 0: for project_task_pk in unique_project_task_pks: project_task_original_names = [ x.get(\"original_name\") for x in project_task_pks if project_task_pk in x.get(\"task\")] project_task_file_pks = [x.get(\"pk\") for x in uploaded_files_name_pks if x.get( \"original_name\") in project_task_original_names] if len(task_file_pks) > 0: # get signature from the project job db object task_obj = ProjectJob.objects.get(pk=project_task_pk) new_task = task_obj.get_job_signature( project_task_file_pks) all_tasks.append(new_task) if len(all_tasks) > 0: task_chain = chain(all_tasks) task_chain.apply_async() else: if verbose: logger.info(\"Determining final status based on invalid files...\") final_status = status.HTTP_400_BAD_REQUEST if all([[y[x].get('status') == 403 for x in y.keys()][0] for y in invalid_files]): if verbose: logger.info( \"All invalid files have a status of 403. Setting final status to HTTP_403_FORBIDDEN.\") final_status = status.HTTP_403_FORBIDDEN return (uploaded_files, invalid_files, existing_files, final_status)","title":"Invalid files with errors"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.get_n_files","text":"Count the number of files in a directory that have an extension. Parameters: dir_path ( str ) \u2013 Directory path. Returns: int ( int ) \u2013 Number of files with an extension, or 0 if directory does not exist. Source code in data_models\\file_handling_functions.py def get_n_files(dir_path: str) -> int: \"\"\" Count the number of files in a directory that have an extension. Args: dir_path (str): Directory path. Returns: int: Number of files with an extension, or 0 if directory does not exist. \"\"\" if os.path.exists(dir_path): all_files = os.listdir(dir_path) # Filter files that have an extension all_files = [x for x in all_files if '.' in x] n_files = len(all_files) else: n_files = 0 return n_files","title":"get_n_files"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.get_new_name","text":"Generate a unique file name based on deployment, recording datetime, and file count. Parameters: deployment ( Deployment ) \u2013 Associated deployment object. recording_dt ( datetime ) \u2013 File recording datetime. file_local_path ( str ) \u2013 Root local path for storage. file_path ( str ) \u2013 Relative file path within storage root. file_n ( Optional [ int ] , default: None ) \u2013 File count for uniqueness. Defaults to None. Returns: str ( str ) \u2013 Unique file name in the format \"{deployment_device_ID} {YYYY-MM-DD_HH-MM-SS} ({file_n})\" Source code in data_models\\file_handling_functions.py def get_new_name( deployment: \"Deployment\", recording_dt: dt, file_local_path: str, file_path: str, file_n: Optional[int] = None ) -> str: \"\"\" Generate a unique file name based on deployment, recording datetime, and file count. Args: deployment (Deployment): Associated deployment object. recording_dt (datetime): File recording datetime. file_local_path (str): Root local path for storage. file_path (str): Relative file path within storage root. file_n (Optional[int], optional): File count for uniqueness. Defaults to None. Returns: str: Unique file name in the format \"{deployment_device_ID}_{YYYY-MM-DD_HH-MM-SS}_({file_n})\" \"\"\" if file_n is None: file_n = get_n_files(os.path.join(file_local_path, file_path)) + 1 newname = f\"{deployment.deployment_device_ID}_{dt.strftime(recording_dt, '%Y-%m-%d_%H-%M-%S')}_({file_n})\" return newname","title":"get_new_name"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.group_files_by_size","text":"Group files into batches by size, ensuring each batch does not exceed max_size (GB). Parameters: file_objs ( QuerySet ) \u2013 Django QuerySet with 'pk' and 'file_size' attributes. max_size ( float , default: MAX_ARCHIVE_SIZE_GB ) \u2013 Maximum group size in GB. Defaults to settings.MAX_ARCHIVE_SIZE_GB. Returns: list [ dict [ str , float | list [ int ]]] \u2013 list[dict[str, float | list[int]]]: List of groups, where each dict contains: - \"file_pks\": List[int] - Primary keys of files in the group. - \"total_size_gb\": float - Total size of the group in GB. Notes Groups files in order of their 'recording_dt' attribute. Source code in data_models\\file_handling_functions.py def group_files_by_size( file_objs: QuerySet, max_size: float = settings.MAX_ARCHIVE_SIZE_GB ) -> list[dict[str, float | list[int]]]: \"\"\" Group files into batches by size, ensuring each batch does not exceed max_size (GB). Args: file_objs (QuerySet): Django QuerySet with 'pk' and 'file_size' attributes. max_size (float, optional): Maximum group size in GB. Defaults to settings.MAX_ARCHIVE_SIZE_GB. Returns: list[dict[str, float | list[int]]]: List of groups, where each dict contains: - \"file_pks\": List[int] - Primary keys of files in the group. - \"total_size_gb\": float - Total size of the group in GB. Notes: - Groups files in order of their 'recording_dt' attribute. \"\"\" # Initialize variables to track the current group key, total size, and file information curr_key = 0 curr_total = 0 file_info = [] # Order the file objects by their recording datetime to ensure logical grouping file_objs = file_objs.order_by('recording_dt') # Extract primary key and file size values from the file objects file_values = file_objs.values('pk', 'file_size') for file_value in file_values: # Convert the file size to GB for comparison file_size = convert_unit(file_value['file_size'], \"GB\") # Check if adding the current file would exceed the maximum allowed size for a group if (curr_total + file_size) > max_size: # If the new file would push over the max size, start a new group curr_total = file_size # Reset the current total size to the new file's size curr_key += 1 # Increment the group key to start a new group else: # Otherwise, add the file's size to the current group's total size curr_total += file_size # Append the file's information along with its assigned group key file_info.append( {\"pk\": file_value['pk'], \"file_size\": file_size, \"key\": curr_key}) # Initialize an empty list to store the grouped file information groups = [] # Use itertools.groupby to group files by their assigned group key for k, g in itertools.groupby(file_info, lambda x: x.get(\"key\")): # Convert the group iterator into a list for processing files = list(g) # Calculate the total size of the files in the current group total_size_gb = sum([x.get('file_size') for x in files]) # Extract the primary keys of the files in the current group file_pks = [x.get('pk') for x in files] # Append the group information (file primary keys and total size) to the groups list groups.append({\"file_pks\": file_pks, \"total_size_gb\": total_size_gb}) return groups","title":"group_files_by_size"},{"location":"reference/data_models/file_handling_functions/#data_models.file_handling_functions.handle_uploaded_file","text":"Upload and save a file to the specified filepath. Parameters: file ( Union [ object , UploadedFile ] ) \u2013 The file object to save. Must provide a chunks() method. filepath ( str ) \u2013 The destination path. multipart ( bool , default: False ) \u2013 If True, append to an existing file. Defaults to False. verbose ( bool , default: False ) \u2013 If True, log debug info. Defaults to False. Raises: OSError \u2013 If creating directories or writing to the file fails. Example handle_uploaded_file( uploaded_file, '/path/to/save/file.txt', multipart=True, verbose=True) Source code in data_models\\file_handling_functions.py def handle_uploaded_file( file: Union[object, UploadedFile], filepath: str, multipart: bool = False, verbose: bool = False ) -> None: \"\"\" Upload and save a file to the specified filepath. Args: file (Union[object, UploadedFile]): The file object to save. Must provide a `chunks()` method. filepath (str): The destination path. multipart (bool, optional): If True, append to an existing file. Defaults to False. verbose (bool, optional): If True, log debug info. Defaults to False. Raises: OSError: If creating directories or writing to the file fails. Example: handle_uploaded_file( uploaded_file, '/path/to/save/file.txt', multipart=True, verbose=True) \"\"\" os.makedirs(os.path.split(filepath)[0], exist_ok=True) if multipart and os.path.exists(filepath): if verbose: logger.info(f\"Appending to {filepath}\") with open(filepath, 'ab+') as destination: for chunk in file.chunks(): destination.write(chunk) else: if verbose: logger.info(f\"Writing to {filepath}\") with open(filepath, 'wb+') as destination: for chunk in file.chunks(): destination.write(chunk)","title":"handle_uploaded_file"},{"location":"reference/data_models/filtersets/","text":"DataFileFilter Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for DataFile, supporting filters for favorite status, deployment activity, device type, observation presence and type, and observation uncertainty. Custom Methods filter_obs_type: Filter files by associated observation types. filter_uncertain: Filter files by uncertainty status in observations. field_help_dict provides help text for common fields. Source code in data_models\\filtersets.py class DataFileFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for DataFile, supporting filters for favorite status, deployment activity, device type, observation presence and type, and observation uncertainty. Custom Methods: filter_obs_type: Filter files by associated observation types. filter_uncertain: Filter files by uncertainty status in observations. field_help_dict provides help text for common fields. \"\"\" is_favourite = django_filters.BooleanFilter( field_name='favourite_of', exclude=True, lookup_expr='isnull', label='is favourite', help_text=\"Filter datafiles by whether a user has favourited them.\" ) is_active = django_filters.BooleanFilter( field_name=\"deployment__is_active\", help_text=\"Filter datafiles by whether their deployment is active\" ) device_type = django_filters.ModelChoiceFilter( field_name='deployment__device__type', queryset=DataType.objects.filter(devices__isnull=False).distinct(), label=\"device type\", help_text=\"Data type of device\" ) has_observations = django_filters.BooleanFilter( field_name=\"observations\", lookup_expr=\"isnull\", exclude=True, label=\"Has observations\", help_text=\"Filter datafiles by whether they have observations\" ) obs_type = django_filters.ChoiceFilter( choices=[ (\"no_obs\", \"No observations\"), (\"no_human_obs\", \"No human observations\"), (\"all_obs\", \"All observations\"), (\"has_human\", \"Human observations\"), (\"has_ai\", \"AI observations\"), (\"human_only\", \"Human observations only\"), (\"ai_only\", \"AI observations only\"), ], method=\"filter_obs_type\", label=\"Observation type\", help_text=\"Filter datafiles by type of observation present.\" ) def filter_obs_type(self, queryset, name, value): \"\"\" Filter DataFiles based on observation type. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Observation type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_obs\": return queryset.filter(observations__isnull=True) elif value == \"no_human_obs\": return queryset.exclude(observations__source=\"human\") elif value == \"all_obs\": return queryset.filter(observations__isnull=False) elif value == \"has_human\": return queryset.filter(observations__source=\"human\") elif value == \"has_ai\": return queryset.filter(observations__in=Observation.objects.all().exclude(source=\"human\")) elif value == \"ai_only\": return queryset.filter(observations__isnull=False).exclude(observations__source=\"human\") elif value == \"human_only\": return queryset.filter(observations__source=\"human\").exclude(observations__in=Observation.objects.all().exclude(source=\"human\")) uncertain = django_filters.ChoiceFilter( choices=[ (\"no_uncertain\", \"No uncertain observations\"), (\"uncertain\", \"Uncertain observations\"), (\"other_uncertain\", \"Other's uncertain observations\"), (\"my_uncertain\", \"My uncertain observations\"), ], method=\"filter_uncertain\", label=\"Uncertain observations\", help_text=\"Filter datafiles by whether they have uncertain observations.\" ) def filter_uncertain(self, queryset, name, value): \"\"\" Filter DataFiles based on uncertainty status of observations. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Uncertainty type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_uncertain\": return queryset.filter(Q(observations__isnull=True) | Q(observations__validation_requested=False)) elif value == \"uncertain\": return queryset.filter(observations__validation_requested=True) elif value == \"my_uncertain\": return queryset.filter(observations__validation_requested=True, observations__owner=self.request.user) elif value == \"other_uncertain\": return queryset.filter(observations__validation_requested=True).exclude(observations__owner=self.request.user) field_help_dict = { \"deployment__id\": \"Numeric database ID of deployment.\", \"id\": \"Numeric database ID of datafile.\", \"favourite_of__id\": \"Database ID of user that has favourited this file.\" } class Meta: model = DataFile fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'deployment__id': ['exact', 'in'], 'deployment__deployment_device_ID': ['exact', 'in', 'icontains'], 'deployment__device': ['exact', 'in'], 'file_type': ['exact', 'in'], 'file_type__name': ['exact', 'icontains', 'in'], 'file_name': ['exact', 'icontains', 'in'], 'file_size': ['lte', 'gte'], 'file_format': ['exact', 'icontains', 'in'], 'upload_dt': ['exact', 'gte', 'lte'], 'recording_dt': ['exact', 'date__exact', 'gte', 'lte', 'date__gte', 'date__lte', 'time__gte', 'time__lte'], 'local_storage': ['exact'], 'archived': ['exact'], 'original_name': ['exact', 'icontains', 'in'], 'favourite_of__id': ['exact', 'contains'], }) filter_obs_type(queryset, name, value) Filter DataFiles based on observation type. Parameters: queryset ( QuerySet ) \u2013 Initial queryset. name ( str ) \u2013 Filter name (unused). value ( str ) \u2013 Observation type to filter by. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def filter_obs_type(self, queryset, name, value): \"\"\" Filter DataFiles based on observation type. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Observation type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_obs\": return queryset.filter(observations__isnull=True) elif value == \"no_human_obs\": return queryset.exclude(observations__source=\"human\") elif value == \"all_obs\": return queryset.filter(observations__isnull=False) elif value == \"has_human\": return queryset.filter(observations__source=\"human\") elif value == \"has_ai\": return queryset.filter(observations__in=Observation.objects.all().exclude(source=\"human\")) elif value == \"ai_only\": return queryset.filter(observations__isnull=False).exclude(observations__source=\"human\") elif value == \"human_only\": return queryset.filter(observations__source=\"human\").exclude(observations__in=Observation.objects.all().exclude(source=\"human\")) filter_uncertain(queryset, name, value) Filter DataFiles based on uncertainty status of observations. Parameters: queryset ( QuerySet ) \u2013 Initial queryset. name ( str ) \u2013 Filter name (unused). value ( str ) \u2013 Uncertainty type to filter by. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def filter_uncertain(self, queryset, name, value): \"\"\" Filter DataFiles based on uncertainty status of observations. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Uncertainty type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_uncertain\": return queryset.filter(Q(observations__isnull=True) | Q(observations__validation_requested=False)) elif value == \"uncertain\": return queryset.filter(observations__validation_requested=True) elif value == \"my_uncertain\": return queryset.filter(observations__validation_requested=True, observations__owner=self.request.user) elif value == \"other_uncertain\": return queryset.filter(observations__validation_requested=True).exclude(observations__owner=self.request.user) DataTypeFilter Bases: GenericFilterMixIn FilterSet for DataType, providing filters to distinguish between file and device types. Filters file_type (BooleanFilter): Returns only data types attached to devices. device_type (BooleanFilter): Returns only data types attached to files. Source code in data_models\\filtersets.py class DataTypeFilter(GenericFilterMixIn): \"\"\" FilterSet for DataType, providing filters to distinguish between file and device types. Filters: file_type (BooleanFilter): Returns only data types attached to devices. device_type (BooleanFilter): Returns only data types attached to files. \"\"\" file_type = django_filters.BooleanFilter( method='is_file_type', label=\"file_type\", help_text=\"Return only data types attached to devices.\") device_type = django_filters.BooleanFilter( method='is_file_type', label=\"device_type\", help_text=\"Return only data types attached to files.\") def is_file_type(self, queryset, name, value): \"\"\" Filter queryset based on whether DataTypes are linked to devices or files. Args: queryset (QuerySet): Queryset to filter. name (str): 'file_type' or 'device_type'. value (bool): Whether to include only device/file types. Returns: QuerySet: Filtered queryset. \"\"\" if name == \"file_type\": return queryset return queryset.filter(device_models__isnull=not value) is_file_type(queryset, name, value) Filter queryset based on whether DataTypes are linked to devices or files. Parameters: queryset ( QuerySet ) \u2013 Queryset to filter. name ( str ) \u2013 'file_type' or 'device_type'. value ( bool ) \u2013 Whether to include only device/file types. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def is_file_type(self, queryset, name, value): \"\"\" Filter queryset based on whether DataTypes are linked to devices or files. Args: queryset (QuerySet): Queryset to filter. name (str): 'file_type' or 'device_type'. value (bool): Whether to include only device/file types. Returns: QuerySet: Filtered queryset. \"\"\" if name == \"file_type\": return queryset return queryset.filter(device_models__isnull=not value) DeploymentFilter Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for Deployment, allowing advanced filtering by deployment, device, site, and project fields, as well as activity status and time ranges. field_help_dict provides help text for common fields. Source code in data_models\\filtersets.py class DeploymentFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for Deployment, allowing advanced filtering by deployment, device, site, and project fields, as well as activity status and time ranges. field_help_dict provides help text for common fields. \"\"\" field_help_dict = { \"device__id\": \"Numeric database ID of a deployed device.\", \"owner__id\": \"Numeric database ID of user who created a deployment.\", \"id\": \"Numeric database ID of deployment.\", \"site__id\": \"Numeric database ID of site where the deployment is located.\", \"data_type__id\": \"Numeric database ID of primary data type of deployment.\" } class Meta: model = Deployment fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'deployment_device_ID': ['exact', 'icontains', 'in'], 'is_active': ['exact'], 'deployment_start': ['exact', 'lte', 'gte'], 'deployment_end': ['exact', 'lte', 'gte'], 'site': ['exact', 'in'], 'site__name': ['exact', 'icontains', 'in'], 'site__short_name': ['exact', 'icontains', 'in'], 'device__id': ['exact', 'in'], 'device__device_ID': ['exact', 'icontains', 'in'], 'device__name': ['exact', 'icontains', 'in'], 'project__id': ['exact'], 'project__project_ID': ['exact'], 'device_type': ['exact', 'in'], 'device_type__name': ['exact', 'icontains', 'in'], }) DeviceFilter Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for Device, enabling filtering by device type, deployment activity, and related fields. Filters is_active (BooleanFilter): Filters devices by the active status of their deployments. device_type (ModelChoiceFilter): Filters by device type. field_help_dict provides help text for key fields. Source code in data_models\\filtersets.py class DeviceFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for Device, enabling filtering by device type, deployment activity, and related fields. Filters: is_active (BooleanFilter): Filters devices by the active status of their deployments. device_type (ModelChoiceFilter): Filters by device type. field_help_dict provides help text for key fields. \"\"\" is_active = django_filters.BooleanFilter( field_name=\"deployments__is_active\", help_text=\"Filters devices by whether they have active deployments.\") device_type = django_filters.ModelChoiceFilter( field_name='type', queryset=DataType.objects.filter(devices__isnull=False).distinct(), label=\"device type\", help_text=\"Filters devices by their type. The queryset is restricted to DataType objects associated with devices.\" ) field_help_dict = { \"type\": \"Numeric database ID of device datatype.\", \"id\": \"Numeric database ID of device.\", } class Meta: model = Device fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'type': ['exact', 'in'], 'type__name': ['exact', 'icontains', 'in'], 'device_ID': ['exact', 'icontains', 'in'], 'model__name': ['exact', 'icontains', 'in'], }) DeviceModelFilter Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for DeviceModel, allowing queries on type and name fields. Meta model: DeviceModel fields: type, type__name, name (with various lookup options) Source code in data_models\\filtersets.py class DeviceModelFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for DeviceModel, allowing queries on type and name fields. Meta: model: DeviceModel fields: type, type__name, name (with various lookup options) \"\"\" class Meta: model = DeviceModel fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'type': ['exact', 'in'], 'type__name': ['exact', 'icontains', 'in'], 'name': ['exact', 'icontains', 'in'], }) ProjectFilter Bases: GenericFilterMixIn FilterSet for Project, allowing filtering by project details and deployment activity. Filters is_active (BooleanFilter): Filter projects by whether they have active deployments. Meta model: Project fields: project_ID, name, organisation (all support exact, icontains, in) Source code in data_models\\filtersets.py class ProjectFilter(GenericFilterMixIn): \"\"\" FilterSet for Project, allowing filtering by project details and deployment activity. Filters: is_active (BooleanFilter): Filter projects by whether they have active deployments. Meta: model: Project fields: project_ID, name, organisation (all support exact, icontains, in) \"\"\" is_active = django_filters.BooleanFilter( field_name=\"deployments__is_active\", help_text=\"Filters projects by whether they have active deployments.\") class Meta: model = Project fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'project_ID': ['exact', 'icontains', 'in'], 'name': ['exact', 'icontains', 'in'], 'organisation': ['exact', 'icontains', 'in'], })","title":"filtersets"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DataFileFilter","text":"Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for DataFile, supporting filters for favorite status, deployment activity, device type, observation presence and type, and observation uncertainty. Custom Methods filter_obs_type: Filter files by associated observation types. filter_uncertain: Filter files by uncertainty status in observations. field_help_dict provides help text for common fields. Source code in data_models\\filtersets.py class DataFileFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for DataFile, supporting filters for favorite status, deployment activity, device type, observation presence and type, and observation uncertainty. Custom Methods: filter_obs_type: Filter files by associated observation types. filter_uncertain: Filter files by uncertainty status in observations. field_help_dict provides help text for common fields. \"\"\" is_favourite = django_filters.BooleanFilter( field_name='favourite_of', exclude=True, lookup_expr='isnull', label='is favourite', help_text=\"Filter datafiles by whether a user has favourited them.\" ) is_active = django_filters.BooleanFilter( field_name=\"deployment__is_active\", help_text=\"Filter datafiles by whether their deployment is active\" ) device_type = django_filters.ModelChoiceFilter( field_name='deployment__device__type', queryset=DataType.objects.filter(devices__isnull=False).distinct(), label=\"device type\", help_text=\"Data type of device\" ) has_observations = django_filters.BooleanFilter( field_name=\"observations\", lookup_expr=\"isnull\", exclude=True, label=\"Has observations\", help_text=\"Filter datafiles by whether they have observations\" ) obs_type = django_filters.ChoiceFilter( choices=[ (\"no_obs\", \"No observations\"), (\"no_human_obs\", \"No human observations\"), (\"all_obs\", \"All observations\"), (\"has_human\", \"Human observations\"), (\"has_ai\", \"AI observations\"), (\"human_only\", \"Human observations only\"), (\"ai_only\", \"AI observations only\"), ], method=\"filter_obs_type\", label=\"Observation type\", help_text=\"Filter datafiles by type of observation present.\" ) def filter_obs_type(self, queryset, name, value): \"\"\" Filter DataFiles based on observation type. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Observation type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_obs\": return queryset.filter(observations__isnull=True) elif value == \"no_human_obs\": return queryset.exclude(observations__source=\"human\") elif value == \"all_obs\": return queryset.filter(observations__isnull=False) elif value == \"has_human\": return queryset.filter(observations__source=\"human\") elif value == \"has_ai\": return queryset.filter(observations__in=Observation.objects.all().exclude(source=\"human\")) elif value == \"ai_only\": return queryset.filter(observations__isnull=False).exclude(observations__source=\"human\") elif value == \"human_only\": return queryset.filter(observations__source=\"human\").exclude(observations__in=Observation.objects.all().exclude(source=\"human\")) uncertain = django_filters.ChoiceFilter( choices=[ (\"no_uncertain\", \"No uncertain observations\"), (\"uncertain\", \"Uncertain observations\"), (\"other_uncertain\", \"Other's uncertain observations\"), (\"my_uncertain\", \"My uncertain observations\"), ], method=\"filter_uncertain\", label=\"Uncertain observations\", help_text=\"Filter datafiles by whether they have uncertain observations.\" ) def filter_uncertain(self, queryset, name, value): \"\"\" Filter DataFiles based on uncertainty status of observations. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Uncertainty type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_uncertain\": return queryset.filter(Q(observations__isnull=True) | Q(observations__validation_requested=False)) elif value == \"uncertain\": return queryset.filter(observations__validation_requested=True) elif value == \"my_uncertain\": return queryset.filter(observations__validation_requested=True, observations__owner=self.request.user) elif value == \"other_uncertain\": return queryset.filter(observations__validation_requested=True).exclude(observations__owner=self.request.user) field_help_dict = { \"deployment__id\": \"Numeric database ID of deployment.\", \"id\": \"Numeric database ID of datafile.\", \"favourite_of__id\": \"Database ID of user that has favourited this file.\" } class Meta: model = DataFile fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'deployment__id': ['exact', 'in'], 'deployment__deployment_device_ID': ['exact', 'in', 'icontains'], 'deployment__device': ['exact', 'in'], 'file_type': ['exact', 'in'], 'file_type__name': ['exact', 'icontains', 'in'], 'file_name': ['exact', 'icontains', 'in'], 'file_size': ['lte', 'gte'], 'file_format': ['exact', 'icontains', 'in'], 'upload_dt': ['exact', 'gte', 'lte'], 'recording_dt': ['exact', 'date__exact', 'gte', 'lte', 'date__gte', 'date__lte', 'time__gte', 'time__lte'], 'local_storage': ['exact'], 'archived': ['exact'], 'original_name': ['exact', 'icontains', 'in'], 'favourite_of__id': ['exact', 'contains'], })","title":"DataFileFilter"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DataFileFilter.filter_obs_type","text":"Filter DataFiles based on observation type. Parameters: queryset ( QuerySet ) \u2013 Initial queryset. name ( str ) \u2013 Filter name (unused). value ( str ) \u2013 Observation type to filter by. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def filter_obs_type(self, queryset, name, value): \"\"\" Filter DataFiles based on observation type. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Observation type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_obs\": return queryset.filter(observations__isnull=True) elif value == \"no_human_obs\": return queryset.exclude(observations__source=\"human\") elif value == \"all_obs\": return queryset.filter(observations__isnull=False) elif value == \"has_human\": return queryset.filter(observations__source=\"human\") elif value == \"has_ai\": return queryset.filter(observations__in=Observation.objects.all().exclude(source=\"human\")) elif value == \"ai_only\": return queryset.filter(observations__isnull=False).exclude(observations__source=\"human\") elif value == \"human_only\": return queryset.filter(observations__source=\"human\").exclude(observations__in=Observation.objects.all().exclude(source=\"human\"))","title":"filter_obs_type"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DataFileFilter.filter_uncertain","text":"Filter DataFiles based on uncertainty status of observations. Parameters: queryset ( QuerySet ) \u2013 Initial queryset. name ( str ) \u2013 Filter name (unused). value ( str ) \u2013 Uncertainty type to filter by. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def filter_uncertain(self, queryset, name, value): \"\"\" Filter DataFiles based on uncertainty status of observations. Args: queryset (QuerySet): Initial queryset. name (str): Filter name (unused). value (str): Uncertainty type to filter by. Returns: QuerySet: Filtered queryset. \"\"\" if value == \"no_uncertain\": return queryset.filter(Q(observations__isnull=True) | Q(observations__validation_requested=False)) elif value == \"uncertain\": return queryset.filter(observations__validation_requested=True) elif value == \"my_uncertain\": return queryset.filter(observations__validation_requested=True, observations__owner=self.request.user) elif value == \"other_uncertain\": return queryset.filter(observations__validation_requested=True).exclude(observations__owner=self.request.user)","title":"filter_uncertain"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DataTypeFilter","text":"Bases: GenericFilterMixIn FilterSet for DataType, providing filters to distinguish between file and device types. Filters file_type (BooleanFilter): Returns only data types attached to devices. device_type (BooleanFilter): Returns only data types attached to files. Source code in data_models\\filtersets.py class DataTypeFilter(GenericFilterMixIn): \"\"\" FilterSet for DataType, providing filters to distinguish between file and device types. Filters: file_type (BooleanFilter): Returns only data types attached to devices. device_type (BooleanFilter): Returns only data types attached to files. \"\"\" file_type = django_filters.BooleanFilter( method='is_file_type', label=\"file_type\", help_text=\"Return only data types attached to devices.\") device_type = django_filters.BooleanFilter( method='is_file_type', label=\"device_type\", help_text=\"Return only data types attached to files.\") def is_file_type(self, queryset, name, value): \"\"\" Filter queryset based on whether DataTypes are linked to devices or files. Args: queryset (QuerySet): Queryset to filter. name (str): 'file_type' or 'device_type'. value (bool): Whether to include only device/file types. Returns: QuerySet: Filtered queryset. \"\"\" if name == \"file_type\": return queryset return queryset.filter(device_models__isnull=not value)","title":"DataTypeFilter"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DataTypeFilter.is_file_type","text":"Filter queryset based on whether DataTypes are linked to devices or files. Parameters: queryset ( QuerySet ) \u2013 Queryset to filter. name ( str ) \u2013 'file_type' or 'device_type'. value ( bool ) \u2013 Whether to include only device/file types. Returns: QuerySet \u2013 Filtered queryset. Source code in data_models\\filtersets.py def is_file_type(self, queryset, name, value): \"\"\" Filter queryset based on whether DataTypes are linked to devices or files. Args: queryset (QuerySet): Queryset to filter. name (str): 'file_type' or 'device_type'. value (bool): Whether to include only device/file types. Returns: QuerySet: Filtered queryset. \"\"\" if name == \"file_type\": return queryset return queryset.filter(device_models__isnull=not value)","title":"is_file_type"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DeploymentFilter","text":"Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for Deployment, allowing advanced filtering by deployment, device, site, and project fields, as well as activity status and time ranges. field_help_dict provides help text for common fields. Source code in data_models\\filtersets.py class DeploymentFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for Deployment, allowing advanced filtering by deployment, device, site, and project fields, as well as activity status and time ranges. field_help_dict provides help text for common fields. \"\"\" field_help_dict = { \"device__id\": \"Numeric database ID of a deployed device.\", \"owner__id\": \"Numeric database ID of user who created a deployment.\", \"id\": \"Numeric database ID of deployment.\", \"site__id\": \"Numeric database ID of site where the deployment is located.\", \"data_type__id\": \"Numeric database ID of primary data type of deployment.\" } class Meta: model = Deployment fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'deployment_device_ID': ['exact', 'icontains', 'in'], 'is_active': ['exact'], 'deployment_start': ['exact', 'lte', 'gte'], 'deployment_end': ['exact', 'lte', 'gte'], 'site': ['exact', 'in'], 'site__name': ['exact', 'icontains', 'in'], 'site__short_name': ['exact', 'icontains', 'in'], 'device__id': ['exact', 'in'], 'device__device_ID': ['exact', 'icontains', 'in'], 'device__name': ['exact', 'icontains', 'in'], 'project__id': ['exact'], 'project__project_ID': ['exact'], 'device_type': ['exact', 'in'], 'device_type__name': ['exact', 'icontains', 'in'], })","title":"DeploymentFilter"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DeviceFilter","text":"Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for Device, enabling filtering by device type, deployment activity, and related fields. Filters is_active (BooleanFilter): Filters devices by the active status of their deployments. device_type (ModelChoiceFilter): Filters by device type. field_help_dict provides help text for key fields. Source code in data_models\\filtersets.py class DeviceFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for Device, enabling filtering by device type, deployment activity, and related fields. Filters: is_active (BooleanFilter): Filters devices by the active status of their deployments. device_type (ModelChoiceFilter): Filters by device type. field_help_dict provides help text for key fields. \"\"\" is_active = django_filters.BooleanFilter( field_name=\"deployments__is_active\", help_text=\"Filters devices by whether they have active deployments.\") device_type = django_filters.ModelChoiceFilter( field_name='type', queryset=DataType.objects.filter(devices__isnull=False).distinct(), label=\"device type\", help_text=\"Filters devices by their type. The queryset is restricted to DataType objects associated with devices.\" ) field_help_dict = { \"type\": \"Numeric database ID of device datatype.\", \"id\": \"Numeric database ID of device.\", } class Meta: model = Device fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'type': ['exact', 'in'], 'type__name': ['exact', 'icontains', 'in'], 'device_ID': ['exact', 'icontains', 'in'], 'model__name': ['exact', 'icontains', 'in'], })","title":"DeviceFilter"},{"location":"reference/data_models/filtersets/#data_models.filtersets.DeviceModelFilter","text":"Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for DeviceModel, allowing queries on type and name fields. Meta model: DeviceModel fields: type, type__name, name (with various lookup options) Source code in data_models\\filtersets.py class DeviceModelFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for DeviceModel, allowing queries on type and name fields. Meta: model: DeviceModel fields: type, type__name, name (with various lookup options) \"\"\" class Meta: model = DeviceModel fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'type': ['exact', 'in'], 'type__name': ['exact', 'icontains', 'in'], 'name': ['exact', 'icontains', 'in'], })","title":"DeviceModelFilter"},{"location":"reference/data_models/filtersets/#data_models.filtersets.ProjectFilter","text":"Bases: GenericFilterMixIn FilterSet for Project, allowing filtering by project details and deployment activity. Filters is_active (BooleanFilter): Filter projects by whether they have active deployments. Meta model: Project fields: project_ID, name, organisation (all support exact, icontains, in) Source code in data_models\\filtersets.py class ProjectFilter(GenericFilterMixIn): \"\"\" FilterSet for Project, allowing filtering by project details and deployment activity. Filters: is_active (BooleanFilter): Filter projects by whether they have active deployments. Meta: model: Project fields: project_ID, name, organisation (all support exact, icontains, in) \"\"\" is_active = django_filters.BooleanFilter( field_name=\"deployments__is_active\", help_text=\"Filters projects by whether they have active deployments.\") class Meta: model = Project fields = GenericFilterMixIn.get_fields().copy() fields.update({ 'project_ID': ['exact', 'icontains', 'in'], 'name': ['exact', 'icontains', 'in'], 'organisation': ['exact', 'icontains', 'in'], })","title":"ProjectFilter"},{"location":"reference/data_models/general_functions/","text":"check_dt(dt, device_timezone=None, localise=True) Parse and localize a datetime object or string. Converts a datetime object or string to a timezone-aware datetime object, localized to the specified device_timezone. If no timezone is provided, the application's default timezone is used. Supports parsing ISO-format datetime strings and can localize naive datetime objects if requested. Parameters: dt ( datetime | str | None ) \u2013 The datetime object or ISO-format string to process. Returns None if input is None. device_timezone ( timezone , default: None ) \u2013 The timezone to use for localization. Defaults to settings.TIME_ZONE if not specified. localise ( bool , default: True ) \u2013 If True, localizes naive datetime objects to the specified timezone. Defaults to True. Returns: Optional [ datetime ] \u2013 datetime | None: A timezone-aware datetime object if parsing succeeds, or None Optional [ datetime ] \u2013 if input is None. Source code in data_models\\general_functions.py def check_dt( dt: Optional[Union[datetime, str]], device_timezone: Optional[pytz.timezone] = None, localise: bool = True ) -> Optional[datetime]: \"\"\" Parse and localize a datetime object or string. Converts a datetime object or string to a timezone-aware datetime object, localized to the specified device_timezone. If no timezone is provided, the application's default timezone is used. Supports parsing ISO-format datetime strings and can localize naive datetime objects if requested. Args: dt (datetime | str | None): The datetime object or ISO-format string to process. Returns None if input is None. device_timezone (pytz.timezone, optional): The timezone to use for localization. Defaults to settings.TIME_ZONE if not specified. localise (bool, optional): If True, localizes naive datetime objects to the specified timezone. Defaults to True. Returns: datetime | None: A timezone-aware datetime object if parsing succeeds, or None if input is None. \"\"\" # If the input datetime is None, return None immediately. if dt is None: return dt # If no device timezone is provided, use the default timezone from settings. if device_timezone is None: device_timezone = pytz.timezone(settings.TIME_ZONE) # If the input datetime is a string, parse it into a datetime object. # The parsing assumes dayfirst=False and yearfirst=True for the format. if isinstance(dt, str): dt = dateutil.parser.parse(dt, dayfirst=False, yearfirst=True) # If the datetime object is naive (lacks timezone info) and localization is enabled, # localize it to the specified device timezone. if dt.tzinfo is None and localise: mytz = device_timezone dt = mytz.localize(dt) return dt create_image(image_width=500, image_height=500, colors=[(255, 0, 0), (0, 0, 255), (255, 255, 0)]) Generate a PIL Image with random pixel colors from a list. Creates a new RGB image of the specified size, with each pixel randomly assigned a color from the provided list of RGB tuples. Parameters: image_width ( int , default: 500 ) \u2013 Width of the image in pixels. Defaults to 500. image_height ( int , default: 500 ) \u2013 Height of the image in pixels. Defaults to 500. colors ( list [ tuple [ int , int , int ]] , default: [(255, 0, 0), (0, 0, 255), (255, 255, 0)] ) \u2013 List of RGB tuples to use for pixel colors. Defaults to [(255, 0, 0), (0, 0, 255), (255, 255, 0)]. Returns: Image ( Image ) \u2013 A PIL Image object with randomly colored pixels. Source code in data_models\\general_functions.py def create_image( image_width: int = 500, image_height: int = 500, colors: list[tuple[int, int, int]] = [ (255, 0, 0), (0, 0, 255), (255, 255, 0)] ) -> Image: \"\"\" Generate a PIL Image with random pixel colors from a list. Creates a new RGB image of the specified size, with each pixel randomly assigned a color from the provided list of RGB tuples. Args: image_width (int, optional): Width of the image in pixels. Defaults to 500. image_height (int, optional): Height of the image in pixels. Defaults to 500. colors (list[tuple[int, int, int]], optional): List of RGB tuples to use for pixel colors. Defaults to [(255, 0, 0), (0, 0, 255), (255, 255, 0)]. Returns: Image: A PIL Image object with randomly colored pixels. \"\"\" image = Image.new('RGB', (image_width, image_height)) for x in range(image.width): for y in range(image.height): image.putpixel((x, y), random.choice(colors)) return image","title":"general_functions"},{"location":"reference/data_models/general_functions/#data_models.general_functions.check_dt","text":"Parse and localize a datetime object or string. Converts a datetime object or string to a timezone-aware datetime object, localized to the specified device_timezone. If no timezone is provided, the application's default timezone is used. Supports parsing ISO-format datetime strings and can localize naive datetime objects if requested. Parameters: dt ( datetime | str | None ) \u2013 The datetime object or ISO-format string to process. Returns None if input is None. device_timezone ( timezone , default: None ) \u2013 The timezone to use for localization. Defaults to settings.TIME_ZONE if not specified. localise ( bool , default: True ) \u2013 If True, localizes naive datetime objects to the specified timezone. Defaults to True. Returns: Optional [ datetime ] \u2013 datetime | None: A timezone-aware datetime object if parsing succeeds, or None Optional [ datetime ] \u2013 if input is None. Source code in data_models\\general_functions.py def check_dt( dt: Optional[Union[datetime, str]], device_timezone: Optional[pytz.timezone] = None, localise: bool = True ) -> Optional[datetime]: \"\"\" Parse and localize a datetime object or string. Converts a datetime object or string to a timezone-aware datetime object, localized to the specified device_timezone. If no timezone is provided, the application's default timezone is used. Supports parsing ISO-format datetime strings and can localize naive datetime objects if requested. Args: dt (datetime | str | None): The datetime object or ISO-format string to process. Returns None if input is None. device_timezone (pytz.timezone, optional): The timezone to use for localization. Defaults to settings.TIME_ZONE if not specified. localise (bool, optional): If True, localizes naive datetime objects to the specified timezone. Defaults to True. Returns: datetime | None: A timezone-aware datetime object if parsing succeeds, or None if input is None. \"\"\" # If the input datetime is None, return None immediately. if dt is None: return dt # If no device timezone is provided, use the default timezone from settings. if device_timezone is None: device_timezone = pytz.timezone(settings.TIME_ZONE) # If the input datetime is a string, parse it into a datetime object. # The parsing assumes dayfirst=False and yearfirst=True for the format. if isinstance(dt, str): dt = dateutil.parser.parse(dt, dayfirst=False, yearfirst=True) # If the datetime object is naive (lacks timezone info) and localization is enabled, # localize it to the specified device timezone. if dt.tzinfo is None and localise: mytz = device_timezone dt = mytz.localize(dt) return dt","title":"check_dt"},{"location":"reference/data_models/general_functions/#data_models.general_functions.create_image","text":"Generate a PIL Image with random pixel colors from a list. Creates a new RGB image of the specified size, with each pixel randomly assigned a color from the provided list of RGB tuples. Parameters: image_width ( int , default: 500 ) \u2013 Width of the image in pixels. Defaults to 500. image_height ( int , default: 500 ) \u2013 Height of the image in pixels. Defaults to 500. colors ( list [ tuple [ int , int , int ]] , default: [(255, 0, 0), (0, 0, 255), (255, 255, 0)] ) \u2013 List of RGB tuples to use for pixel colors. Defaults to [(255, 0, 0), (0, 0, 255), (255, 255, 0)]. Returns: Image ( Image ) \u2013 A PIL Image object with randomly colored pixels. Source code in data_models\\general_functions.py def create_image( image_width: int = 500, image_height: int = 500, colors: list[tuple[int, int, int]] = [ (255, 0, 0), (0, 0, 255), (255, 255, 0)] ) -> Image: \"\"\" Generate a PIL Image with random pixel colors from a list. Creates a new RGB image of the specified size, with each pixel randomly assigned a color from the provided list of RGB tuples. Args: image_width (int, optional): Width of the image in pixels. Defaults to 500. image_height (int, optional): Height of the image in pixels. Defaults to 500. colors (list[tuple[int, int, int]], optional): List of RGB tuples to use for pixel colors. Defaults to [(255, 0, 0), (0, 0, 255), (255, 255, 0)]. Returns: Image: A PIL Image object with randomly colored pixels. \"\"\" image = Image.new('RGB', (image_width, image_height)) for x in range(image.width): for y in range(image.height): image.putpixel((x, y), random.choice(colors)) return image","title":"create_image"},{"location":"reference/data_models/job_handling_functions/","text":"get_job_from_name(job_name, obj_type, obj_pks, job_args, user_pk=None) Creates a new task signature for a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: object: A Celery task signature object representing the job. Source code in data_models\\job_handling_functions.py def get_job_from_name( job_name: str, obj_type: str, obj_pks: list[int], job_args: dict[str, any], user_pk: int | None = None ) -> object: \"\"\" Creates a new task signature for a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: object: A Celery task signature object representing the job. \"\"\" all_args = {f\"{obj_type}_pks\": obj_pks, **job_args} if user_pk is not None: all_args[\"user_pk\"] = user_pk new_task = app.signature( job_name, kwargs=all_args, immutable=True) return new_task register_job(name, task_name, task_data_type, task_admin_only=False, max_items=500, default_args={}) Decorator to register a function as a plug-in for a generic job. Parameters: name ( str ) \u2013 The display name of the job. task_name ( str ) \u2013 The unique identifier for the task. task_data_type ( str ) \u2013 The type of data the task operates on. task_admin_only ( bool , default: False ) \u2013 Whether the task is restricted to admin users. Defaults to False. max_items ( int , default: 500 ) \u2013 The maximum number of items the task can process. Defaults to 500. default_args ( Dict [ str , Any ] , default: {} ) \u2013 Default arguments for the task. Defaults to an empty dictionary. Returns: Callable ( Callable ) \u2013 A decorator function that registers the task. Source code in data_models\\job_handling_functions.py def register_job( name: str, task_name: str, task_data_type: str, task_admin_only: bool = False, max_items: int = 500, default_args: Dict[str, Any] = {} ) -> Callable: \"\"\" Decorator to register a function as a plug-in for a generic job. Args: name (str): The display name of the job. task_name (str): The unique identifier for the task. task_data_type (str): The type of data the task operates on. task_admin_only (bool, optional): Whether the task is restricted to admin users. Defaults to False. max_items (int, optional): The maximum number of items the task can process. Defaults to 500. default_args (Dict[str, Any], optional): Default arguments for the task. Defaults to an empty dictionary. Returns: Callable: A decorator function that registers the task. \"\"\" def register_job_decorator(func: Callable) -> Callable: \"\"\"Register a function as a plug-in.\"\"\" settings.GENERIC_JOBS[task_name] = { \"id\": len(settings.GENERIC_JOBS.items()), \"name\": name, \"task_name\": task_name, \"task\": func, \"data_type\": task_data_type, \"admin_only\": task_admin_only, \"max_items\": max_items, \"default_args\": default_args, } logger.info(f\"Registered generic task {task_name}\") return func return register_job_decorator start_job_from_name(job_name, obj_type, obj_pks, job_args, user_pk=None) Starts a job based on the provided parameters. Parameters: job_name ( str ) \u2013 The name of the job to be executed. obj_type ( str ) \u2013 The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks ( list [ int ] ) \u2013 A list of primary keys for the objects related to the job. job_args ( dict [ str , Any ] ) \u2013 Additional arguments required for the job execution. user_pk ( int | None , default: None ) \u2013 The primary key of the user initiating the job. Defaults to None. Returns: tuple [ bool , str , int ] \u2013 tuple[bool, str, int]: A tuple containing: - A boolean indicating success or failure. - A message describing the result. - An HTTP status code. Source code in data_models\\job_handling_functions.py def start_job_from_name( job_name: str, obj_type: str, obj_pks: list[int], job_args: dict[str, Any], user_pk: int | None = None ) -> tuple[bool, str, int]: \"\"\" Starts a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, Any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: tuple[bool, str, int]: A tuple containing: - A boolean indicating success or failure. - A message describing the result. - An HTTP status code. \"\"\" from user_management.models import User job_dict = settings.GENERIC_JOBS.get(job_name) if job_dict is None: return False, \"Not a registered job\", status.HTTP_404_NOT_FOUND if user_pk is not None: user_obj = User.objects.get(pk=user_pk) if job_dict[\"admin_only\"] and not user_obj.is_staff: return False, \"You are not permitted to run this job\", status.HTTP_403_FORBIDDEN if len(obj_pks) > job_dict[\"max_items\"]: return False, \"Too many items for this job\", status.HTTP_400_BAD_REQUEST new_task = get_job_from_name( job_name, obj_type, obj_pks, job_args, user_pk ) new_task.apply_async() return True, f\"{job_name} started\", status.HTTP_200_OK","title":"job_handling_functions"},{"location":"reference/data_models/job_handling_functions/#data_models.job_handling_functions.get_job_from_name","text":"Creates a new task signature for a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: object: A Celery task signature object representing the job. Source code in data_models\\job_handling_functions.py def get_job_from_name( job_name: str, obj_type: str, obj_pks: list[int], job_args: dict[str, any], user_pk: int | None = None ) -> object: \"\"\" Creates a new task signature for a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: object: A Celery task signature object representing the job. \"\"\" all_args = {f\"{obj_type}_pks\": obj_pks, **job_args} if user_pk is not None: all_args[\"user_pk\"] = user_pk new_task = app.signature( job_name, kwargs=all_args, immutable=True) return new_task","title":"get_job_from_name"},{"location":"reference/data_models/job_handling_functions/#data_models.job_handling_functions.register_job","text":"Decorator to register a function as a plug-in for a generic job. Parameters: name ( str ) \u2013 The display name of the job. task_name ( str ) \u2013 The unique identifier for the task. task_data_type ( str ) \u2013 The type of data the task operates on. task_admin_only ( bool , default: False ) \u2013 Whether the task is restricted to admin users. Defaults to False. max_items ( int , default: 500 ) \u2013 The maximum number of items the task can process. Defaults to 500. default_args ( Dict [ str , Any ] , default: {} ) \u2013 Default arguments for the task. Defaults to an empty dictionary. Returns: Callable ( Callable ) \u2013 A decorator function that registers the task. Source code in data_models\\job_handling_functions.py def register_job( name: str, task_name: str, task_data_type: str, task_admin_only: bool = False, max_items: int = 500, default_args: Dict[str, Any] = {} ) -> Callable: \"\"\" Decorator to register a function as a plug-in for a generic job. Args: name (str): The display name of the job. task_name (str): The unique identifier for the task. task_data_type (str): The type of data the task operates on. task_admin_only (bool, optional): Whether the task is restricted to admin users. Defaults to False. max_items (int, optional): The maximum number of items the task can process. Defaults to 500. default_args (Dict[str, Any], optional): Default arguments for the task. Defaults to an empty dictionary. Returns: Callable: A decorator function that registers the task. \"\"\" def register_job_decorator(func: Callable) -> Callable: \"\"\"Register a function as a plug-in.\"\"\" settings.GENERIC_JOBS[task_name] = { \"id\": len(settings.GENERIC_JOBS.items()), \"name\": name, \"task_name\": task_name, \"task\": func, \"data_type\": task_data_type, \"admin_only\": task_admin_only, \"max_items\": max_items, \"default_args\": default_args, } logger.info(f\"Registered generic task {task_name}\") return func return register_job_decorator","title":"register_job"},{"location":"reference/data_models/job_handling_functions/#data_models.job_handling_functions.start_job_from_name","text":"Starts a job based on the provided parameters. Parameters: job_name ( str ) \u2013 The name of the job to be executed. obj_type ( str ) \u2013 The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks ( list [ int ] ) \u2013 A list of primary keys for the objects related to the job. job_args ( dict [ str , Any ] ) \u2013 Additional arguments required for the job execution. user_pk ( int | None , default: None ) \u2013 The primary key of the user initiating the job. Defaults to None. Returns: tuple [ bool , str , int ] \u2013 tuple[bool, str, int]: A tuple containing: - A boolean indicating success or failure. - A message describing the result. - An HTTP status code. Source code in data_models\\job_handling_functions.py def start_job_from_name( job_name: str, obj_type: str, obj_pks: list[int], job_args: dict[str, Any], user_pk: int | None = None ) -> tuple[bool, str, int]: \"\"\" Starts a job based on the provided parameters. Args: job_name (str): The name of the job to be executed. obj_type (str): The type of object associated with the job (e.g., \"sensor\", \"device\"). obj_pks (list[int]): A list of primary keys for the objects related to the job. job_args (dict[str, Any]): Additional arguments required for the job execution. user_pk (int | None, optional): The primary key of the user initiating the job. Defaults to None. Returns: tuple[bool, str, int]: A tuple containing: - A boolean indicating success or failure. - A message describing the result. - An HTTP status code. \"\"\" from user_management.models import User job_dict = settings.GENERIC_JOBS.get(job_name) if job_dict is None: return False, \"Not a registered job\", status.HTTP_404_NOT_FOUND if user_pk is not None: user_obj = User.objects.get(pk=user_pk) if job_dict[\"admin_only\"] and not user_obj.is_staff: return False, \"You are not permitted to run this job\", status.HTTP_403_FORBIDDEN if len(obj_pks) > job_dict[\"max_items\"]: return False, \"Too many items for this job\", status.HTTP_400_BAD_REQUEST new_task = get_job_from_name( job_name, obj_type, obj_pks, job_args, user_pk ) new_task.apply_async() return True, f\"{job_name} started\", status.HTTP_200_OK","title":"start_job_from_name"},{"location":"reference/data_models/metadata_functions/","text":"create_metadata_dict(file_objs) Construct a dictionary of serialized metadata for the given DataFile objects. This function collects all related deployments, projects, and devices associated with the provided DataFile queryset, and serializes each entity type using their respective serializers. The result is a dictionary suitable for JSON export. Parameters: file_objs ( QuerySet [ DataFile ] ) \u2013 Queryset of DataFile objects for which to generate metadata. Returns: dict ( dict ) \u2013 A dictionary with the following structure: { \"projects\": [ ], \"devices\": [ ], \"deployments\": [ ], \"data_files\": [ ] } Notes Only projects, devices, and deployments related to the input files are included. DataFile objects are serialized as provided in the input queryset. Source code in data_models\\metadata_functions.py def create_metadata_dict(file_objs: QuerySet[DataFile]) -> dict: \"\"\" Construct a dictionary of serialized metadata for the given DataFile objects. This function collects all related deployments, projects, and devices associated with the provided DataFile queryset, and serializes each entity type using their respective serializers. The result is a dictionary suitable for JSON export. Args: file_objs (QuerySet[DataFile]): Queryset of DataFile objects for which to generate metadata. Returns: dict: A dictionary with the following structure: { \"projects\": [<serialized Project objects>], \"devices\": [<serialized Device objects>], \"deployments\": [<serialized Deployment objects>], \"data_files\": [<serialized DataFile objects>] } Notes: - Only projects, devices, and deployments related to the input files are included. - DataFile objects are serialized as provided in the input queryset. \"\"\" deployment_objs = Deployment.objects.filter(files__in=file_objs).distinct() project_objs = Project.objects.filter( deployments__in=deployment_objs).distinct() device_objs = Device.objects.filter( deployments__in=deployment_objs).distinct() file_dict = DataFileSerializer(file_objs, many=True).data deployment_dict = DeploymentSerializer(deployment_objs, many=True).data project_dict = ProjectSerializer(project_objs, many=True).data device_dict = DeviceSerializer(device_objs, many=True).data all_dict = {\"projects\": project_dict, \"devices\": device_dict, \"deployments\": deployment_dict, \"data_files\": file_dict} return all_dict metadata_json_from_files(file_objs, output_path) Create a JSON file containing metadata for a collection of DataFile objects. This function gathers metadata from the provided DataFile queryset, serializes it (including related projects, devices, and deployments), and writes the result to a \"metadata.json\" file in the specified output directory. Parameters: file_objs ( QuerySet [ DataFile ] ) \u2013 Queryset of DataFile objects whose metadata will be included. output_path ( str ) \u2013 Directory path where the resulting JSON file should be saved. Returns: str \u2013 Full file path to the generated \"metadata.json\". Raises: OSError \u2013 If the output directory cannot be created or the file cannot be written. Notes The output directory will be created if it does not already exist. The resulting JSON file is formatted with an indentation of 2 spaces. Related metadata for projects, devices, and deployments is automatically included. Source code in data_models\\metadata_functions.py def metadata_json_from_files(file_objs: QuerySet[DataFile], output_path: str): \"\"\" Create a JSON file containing metadata for a collection of DataFile objects. This function gathers metadata from the provided DataFile queryset, serializes it (including related projects, devices, and deployments), and writes the result to a \"metadata.json\" file in the specified output directory. Args: file_objs (QuerySet[DataFile]): Queryset of DataFile objects whose metadata will be included. output_path (str): Directory path where the resulting JSON file should be saved. Returns: str: Full file path to the generated \"metadata.json\". Raises: OSError: If the output directory cannot be created or the file cannot be written. Notes: - The output directory will be created if it does not already exist. - The resulting JSON file is formatted with an indentation of 2 spaces. - Related metadata for projects, devices, and deployments is automatically included. \"\"\" metadata_dict = create_metadata_dict(file_objs) os.makedirs(output_path, exist_ok=True) metadata_json_path = os.path.join(output_path, \"metadata.json\") # json dump file with open(metadata_json_path, \"w\") as f: f.write(json.dumps(metadata_dict, indent=2)) return metadata_json_path","title":"metadata_functions"},{"location":"reference/data_models/metadata_functions/#data_models.metadata_functions.create_metadata_dict","text":"Construct a dictionary of serialized metadata for the given DataFile objects. This function collects all related deployments, projects, and devices associated with the provided DataFile queryset, and serializes each entity type using their respective serializers. The result is a dictionary suitable for JSON export. Parameters: file_objs ( QuerySet [ DataFile ] ) \u2013 Queryset of DataFile objects for which to generate metadata. Returns: dict ( dict ) \u2013 A dictionary with the following structure: { \"projects\": [ ], \"devices\": [ ], \"deployments\": [ ], \"data_files\": [ ] } Notes Only projects, devices, and deployments related to the input files are included. DataFile objects are serialized as provided in the input queryset. Source code in data_models\\metadata_functions.py def create_metadata_dict(file_objs: QuerySet[DataFile]) -> dict: \"\"\" Construct a dictionary of serialized metadata for the given DataFile objects. This function collects all related deployments, projects, and devices associated with the provided DataFile queryset, and serializes each entity type using their respective serializers. The result is a dictionary suitable for JSON export. Args: file_objs (QuerySet[DataFile]): Queryset of DataFile objects for which to generate metadata. Returns: dict: A dictionary with the following structure: { \"projects\": [<serialized Project objects>], \"devices\": [<serialized Device objects>], \"deployments\": [<serialized Deployment objects>], \"data_files\": [<serialized DataFile objects>] } Notes: - Only projects, devices, and deployments related to the input files are included. - DataFile objects are serialized as provided in the input queryset. \"\"\" deployment_objs = Deployment.objects.filter(files__in=file_objs).distinct() project_objs = Project.objects.filter( deployments__in=deployment_objs).distinct() device_objs = Device.objects.filter( deployments__in=deployment_objs).distinct() file_dict = DataFileSerializer(file_objs, many=True).data deployment_dict = DeploymentSerializer(deployment_objs, many=True).data project_dict = ProjectSerializer(project_objs, many=True).data device_dict = DeviceSerializer(device_objs, many=True).data all_dict = {\"projects\": project_dict, \"devices\": device_dict, \"deployments\": deployment_dict, \"data_files\": file_dict} return all_dict","title":"create_metadata_dict"},{"location":"reference/data_models/metadata_functions/#data_models.metadata_functions.metadata_json_from_files","text":"Create a JSON file containing metadata for a collection of DataFile objects. This function gathers metadata from the provided DataFile queryset, serializes it (including related projects, devices, and deployments), and writes the result to a \"metadata.json\" file in the specified output directory. Parameters: file_objs ( QuerySet [ DataFile ] ) \u2013 Queryset of DataFile objects whose metadata will be included. output_path ( str ) \u2013 Directory path where the resulting JSON file should be saved. Returns: str \u2013 Full file path to the generated \"metadata.json\". Raises: OSError \u2013 If the output directory cannot be created or the file cannot be written. Notes The output directory will be created if it does not already exist. The resulting JSON file is formatted with an indentation of 2 spaces. Related metadata for projects, devices, and deployments is automatically included. Source code in data_models\\metadata_functions.py def metadata_json_from_files(file_objs: QuerySet[DataFile], output_path: str): \"\"\" Create a JSON file containing metadata for a collection of DataFile objects. This function gathers metadata from the provided DataFile queryset, serializes it (including related projects, devices, and deployments), and writes the result to a \"metadata.json\" file in the specified output directory. Args: file_objs (QuerySet[DataFile]): Queryset of DataFile objects whose metadata will be included. output_path (str): Directory path where the resulting JSON file should be saved. Returns: str: Full file path to the generated \"metadata.json\". Raises: OSError: If the output directory cannot be created or the file cannot be written. Notes: - The output directory will be created if it does not already exist. - The resulting JSON file is formatted with an indentation of 2 spaces. - Related metadata for projects, devices, and deployments is automatically included. \"\"\" metadata_dict = create_metadata_dict(file_objs) os.makedirs(output_path, exist_ok=True) metadata_json_path = os.path.join(output_path, \"metadata.json\") # json dump file with open(metadata_json_path, \"w\") as f: f.write(json.dumps(metadata_dict, indent=2)) return metadata_json_path","title":"metadata_json_from_files"},{"location":"reference/data_models/models/","text":"DataFile Bases: BaseModel Represents a data file associated with a deployment. Source code in data_models\\models.py class DataFile(BaseModel): \"\"\" Represents a data file associated with a deployment. \"\"\" deployment = models.ForeignKey( Deployment, on_delete=models.CASCADE, related_name=\"files\", db_index=True, help_text=\"Deployment to which this datafile is linked.\") file_type = models.ForeignKey( DataType, models.PROTECT, related_name=\"files\", null=True, default=None, db_index=True, help_text=\"Data type of file.\") file_name = models.CharField( max_length=150, unique=True, db_index=True, help_text=\"File name.\") file_size = FileSizeField(help_text=\"Size of file in bytes.\") file_format = models.CharField(max_length=10, help_text=\"File extension.\") upload_dt = models.DateTimeField( default=djtimezone.now, help_text=\"Datetime at which the file was uploaded.\") recording_dt = models.DateTimeField( null=True, db_index=True, help_text=\"Datetime at which the file was recorded.\") path = models.CharField(max_length=500, help_text=\"Relative path.\") local_path = models.CharField(max_length=500, blank=True, help_text=\"Absolute file location on local storage, from which path is relative.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that does not fit in existing columns.\") linked_files = models.JSONField( default=dict, blank=True, help_text=\"Linked files, such as alternative representations of this file.\") thumb_url = models.CharField( max_length=500, null=True, blank=True, help_text=\"Thumbnail URL.\") local_storage = models.BooleanField( default=True, db_index=True, help_text=\"Is the file available on local storage?\") archived = models.BooleanField( default=False, help_text=\"Has the file been archived?\") tar_file = models.ForeignKey( TarFile, on_delete=models.SET_NULL, blank=True, null=True, related_name=\"files\", help_text=\"TAR file containing this file.\") favourite_of = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"favourites\", help_text=\"Users who have favourited this file.\") do_not_remove = models.BooleanField( default=False, help_text=\"If True, this file will not be removed during cleaning.\") original_name = models.CharField( max_length=100, blank=True, null=True, help_text=\"Original name of this file.\") file_url = models.CharField( max_length=500, null=True, blank=True, help_text=\"URL of this file\") tag = models.CharField(max_length=250, null=True, blank=True, db_index=True, help_text=\"Additional identifying tag of this file.\") has_human = models.BooleanField( default=False, db_index=True, help_text=\"True if this image has been annotated with a human.\") objects = DataFileQuerySet.as_manager() class Meta: indexes = [ GinIndex( OpClass(Upper('tag'), name='gin_trgm_ops'), name='upper_tag_gin_idx', ), GinIndex( OpClass(Upper('file_name'), name='gin_trgm_ops'), name='upper_file_name_gin_idx', ) ] def __str__(self): return f\"{self.file_name}{self.file_format}\" def get_absolute_url(self): return f\"/datafiles/{self.pk}\" def add_favourite(self, user: \"User\") -> None: \"\"\" Add a user to this file's favourites. \"\"\" self.favourite_of.add(user) self.save() def remove_favourite(self, user: \"User\") -> None: \"\"\" Remove a user from this file's favourites. \"\"\" self.favourite_of.remove(user) self.save() def full_path(self): \"\"\" Returns the full path to this file on local storage. \"\"\" return os.path.join(self.local_path, self.path, f\"{self.file_name}{self.file_format}\") def thumb_path(self): \"\"\" Returns the path to the thumbnail version of this file. \"\"\" return os.path.join(self.local_path, self.path, self.file_name+\"_THUMB.jpg\") def set_file_url(self): \"\"\" Sets the file_url for this file using storage settings. \"\"\" if self.local_storage: self.file_url = os.path.normpath( os.path.join(settings.FILE_STORAGE_URL, self.path, (self.file_name + self.file_format)) ).replace(\"\\\\\", \"/\") else: self.file_url = None def set_linked_files_urls(self): \"\"\" Set file URLs for each linked file. \"\"\" for key in self.linked_files.keys(): file_path = self.linked_files[key][\"path\"] rel_file_path = os.path.relpath( file_path, settings.FILE_STORAGE_ROOT) self.linked_files[key][\"url\"] = os.path.join( settings.FILE_STORAGE_URL, rel_file_path) def set_thumb_url(self, has_thumb: bool = True) -> None: \"\"\" Set the thumbnail URL for this file. \"\"\" if has_thumb: self.thumb_url = os.path.normpath(os.path.join( settings.FILE_STORAGE_URL, self.path, self.file_name+\"_THUMB.jpg\")) else: self.thumb_url = None def check_human(self): \"\"\" Checks for human observations and updates has_human accordingly. \"\"\" old_has_human = self.has_human new_has_human = self.observations.filter( taxon__taxon_code=settings.HUMAN_TAXON_CODE).exists() if old_has_human != new_has_human: self.has_human = new_has_human self.save() self.deployment.set_thumb_url() self.deployment.save() def clean_file(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Clean up this file and its associated resources. Args: delete_obj (bool): True if the object will be deleted from the database after cleaning. force_delete (bool): If True, force deletion/alteration even if file can't be deleted. Returns: bool: True if the file was successfully cleaned, False otherwise. \"\"\" logger.info( f\"Clean {self.file_name} - Delete object: {delete_obj} - Force delete:{force_delete}\") if (delete_obj and self.archived) and not force_delete: logger.info( f\"Clean {self.file_name} - Full delete failed - Archived file\") return False if (self.do_not_remove or self.deployment_last_image.exists() or self.favourite_of.exists()) and (not delete_obj or force_delete): logger.info(f\"Clean {self.file_name} - Failed - Protected file\") return False if self.local_storage: logger.info(f\"Clean {self.file_name} - Try to remove file\") success = try_remove_file_clean_dirs(self.full_path()) if not success and not force_delete: logger.error( f\"Clean {self.file_name} - Failed - Cannot delete local file\") return False elif success: logger.info( f\"Clean {self.file_name} - Try to remove file - success\") logger.info(f\"Clean {self.file_name} - Try to remove thumbnail\") if self.thumb_url is not None and self.thumb_url != \"\": thumb_path = self.thumb_path() success = try_remove_file_clean_dirs(thumb_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - failed\") for key, value in self.linked_files.items(): extra_version_path = value[\"path\"] logger.info(f\"Clean {self.file_name} - Try to remove {key}\") success = try_remove_file_clean_dirs(extra_version_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove {key} - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove {key} - failed\") if not delete_obj: logger.info(f\"Clean {self.file_name} - Altering database\") self.local_storage = False self.local_path = \"\" self.linked_files = {} self.set_thumb_url(False) self.save() return True def save(self, *args, **kwargs): \"\"\" Assigns file_type from deployment if not set, sets file_url, then saves. \"\"\" if self.file_type is None: self.file_type = self.deployment.device.type self.set_file_url() super().save(*args, **kwargs) def clean(self): \"\"\" Validates that this file is within its deployment's date range. Raises ValidationError if invalid. \"\"\" result, message = validators.data_file_in_deployment( self.recording_dt, self.deployment) if not result: raise ValidationError(message) super(DataFile, self).clean() add_favourite(user) Add a user to this file's favourites. Source code in data_models\\models.py def add_favourite(self, user: \"User\") -> None: \"\"\" Add a user to this file's favourites. \"\"\" self.favourite_of.add(user) self.save() check_human() Checks for human observations and updates has_human accordingly. Source code in data_models\\models.py def check_human(self): \"\"\" Checks for human observations and updates has_human accordingly. \"\"\" old_has_human = self.has_human new_has_human = self.observations.filter( taxon__taxon_code=settings.HUMAN_TAXON_CODE).exists() if old_has_human != new_has_human: self.has_human = new_has_human self.save() self.deployment.set_thumb_url() self.deployment.save() clean() Validates that this file is within its deployment's date range. Raises ValidationError if invalid. Source code in data_models\\models.py def clean(self): \"\"\" Validates that this file is within its deployment's date range. Raises ValidationError if invalid. \"\"\" result, message = validators.data_file_in_deployment( self.recording_dt, self.deployment) if not result: raise ValidationError(message) super(DataFile, self).clean() clean_file(delete_obj=False, force_delete=False) Clean up this file and its associated resources. Parameters: delete_obj ( bool , default: False ) \u2013 True if the object will be deleted from the database after cleaning. force_delete ( bool , default: False ) \u2013 If True, force deletion/alteration even if file can't be deleted. Returns: bool ( bool ) \u2013 True if the file was successfully cleaned, False otherwise. Source code in data_models\\models.py def clean_file(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Clean up this file and its associated resources. Args: delete_obj (bool): True if the object will be deleted from the database after cleaning. force_delete (bool): If True, force deletion/alteration even if file can't be deleted. Returns: bool: True if the file was successfully cleaned, False otherwise. \"\"\" logger.info( f\"Clean {self.file_name} - Delete object: {delete_obj} - Force delete:{force_delete}\") if (delete_obj and self.archived) and not force_delete: logger.info( f\"Clean {self.file_name} - Full delete failed - Archived file\") return False if (self.do_not_remove or self.deployment_last_image.exists() or self.favourite_of.exists()) and (not delete_obj or force_delete): logger.info(f\"Clean {self.file_name} - Failed - Protected file\") return False if self.local_storage: logger.info(f\"Clean {self.file_name} - Try to remove file\") success = try_remove_file_clean_dirs(self.full_path()) if not success and not force_delete: logger.error( f\"Clean {self.file_name} - Failed - Cannot delete local file\") return False elif success: logger.info( f\"Clean {self.file_name} - Try to remove file - success\") logger.info(f\"Clean {self.file_name} - Try to remove thumbnail\") if self.thumb_url is not None and self.thumb_url != \"\": thumb_path = self.thumb_path() success = try_remove_file_clean_dirs(thumb_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - failed\") for key, value in self.linked_files.items(): extra_version_path = value[\"path\"] logger.info(f\"Clean {self.file_name} - Try to remove {key}\") success = try_remove_file_clean_dirs(extra_version_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove {key} - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove {key} - failed\") if not delete_obj: logger.info(f\"Clean {self.file_name} - Altering database\") self.local_storage = False self.local_path = \"\" self.linked_files = {} self.set_thumb_url(False) self.save() return True full_path() Returns the full path to this file on local storage. Source code in data_models\\models.py def full_path(self): \"\"\" Returns the full path to this file on local storage. \"\"\" return os.path.join(self.local_path, self.path, f\"{self.file_name}{self.file_format}\") remove_favourite(user) Remove a user from this file's favourites. Source code in data_models\\models.py def remove_favourite(self, user: \"User\") -> None: \"\"\" Remove a user from this file's favourites. \"\"\" self.favourite_of.remove(user) self.save() save(*args, **kwargs) Assigns file_type from deployment if not set, sets file_url, then saves. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Assigns file_type from deployment if not set, sets file_url, then saves. \"\"\" if self.file_type is None: self.file_type = self.deployment.device.type self.set_file_url() super().save(*args, **kwargs) set_file_url() Sets the file_url for this file using storage settings. Source code in data_models\\models.py def set_file_url(self): \"\"\" Sets the file_url for this file using storage settings. \"\"\" if self.local_storage: self.file_url = os.path.normpath( os.path.join(settings.FILE_STORAGE_URL, self.path, (self.file_name + self.file_format)) ).replace(\"\\\\\", \"/\") else: self.file_url = None set_linked_files_urls() Set file URLs for each linked file. Source code in data_models\\models.py def set_linked_files_urls(self): \"\"\" Set file URLs for each linked file. \"\"\" for key in self.linked_files.keys(): file_path = self.linked_files[key][\"path\"] rel_file_path = os.path.relpath( file_path, settings.FILE_STORAGE_ROOT) self.linked_files[key][\"url\"] = os.path.join( settings.FILE_STORAGE_URL, rel_file_path) set_thumb_url(has_thumb=True) Set the thumbnail URL for this file. Source code in data_models\\models.py def set_thumb_url(self, has_thumb: bool = True) -> None: \"\"\" Set the thumbnail URL for this file. \"\"\" if has_thumb: self.thumb_url = os.path.normpath(os.path.join( settings.FILE_STORAGE_URL, self.path, self.file_name+\"_THUMB.jpg\")) else: self.thumb_url = None thumb_path() Returns the path to the thumbnail version of this file. Source code in data_models\\models.py def thumb_path(self): \"\"\" Returns the path to the thumbnail version of this file. \"\"\" return os.path.join(self.local_path, self.path, self.file_name+\"_THUMB.jpg\") DataFileQuerySet Bases: ApproximateCountQuerySet Custom QuerySet for DataFile, providing path annotations and statistics. Source code in data_models\\models.py class DataFileQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for DataFile, providing path annotations and statistics. \"\"\" def full_paths(self): self = self.relative_paths() return self.annotate(full_path=Concat(F('local_path'), Value(os.sep), F('relative_path'))) def relative_paths(self): self = self.full_names() return self.annotate(relative_path=Concat(F('path'), Value(os.sep), F('full_name'))) def full_names(self): return self.annotate(full_name=Concat(F('file_name'), F('file_format'))) def file_size_unit(self, unit=\"\"): total_file_size = self.aggregate(total_file_size=Sum(\"file_size\"))[ \"total_file_size\"] converted_file_size = convert_unit(total_file_size, unit) return converted_file_size def file_count(self): return self.aggregate(total_file_size=Cast(Sum(\"file_size\"), models.FloatField())/Cast(Value(1024*1024*1024), models.FloatField()), object_n=Count(\"pk\"), archived_file_n=Sum(Case(When(local_storage=False, archived=True, then=Value(1)), default=Value(0)))) def min_date(self): return self.aggregate(min_date=Min(\"recording_dt\"))[\"min_date\"] def max_date(self): return self.aggregate(max_date=Max(\"recording_dt\"))[\"max_date\"] def device_type(self): return self.annotate(device_type=F('deployment__device__type__name')) DataType Bases: BaseModel Describes a type of data, including its name, color, and optional icon. Source code in data_models\\models.py class DataType(BaseModel): \"\"\" Describes a type of data, including its name, color, and optional icon. \"\"\" name = models.CharField(max_length=20, help_text=\"Name of data type.\") colour = ColorField(default=\"#FFFFFF\", help_text=\"Colour to use for this data type.\") symbol = IconField( blank=True, help_text=\"Symbol to use for this data type.\") def __str__(self): return self.name Deployment Bases: BaseModel Records a deployment of a device to a site, within a project. Source code in data_models\\models.py class Deployment(BaseModel): \"\"\" Records a deployment of a device to a site, within a project. \"\"\" deployment_device_ID = models.CharField( max_length=110, blank=True, editable=False, unique=True, help_text=\"Unique identifier combining 'deployment_ID', 'device_type' and 'device_n'.\") deployment_ID = models.CharField( max_length=80, help_text=\"An identifier for a deployment.\") device_type = models.ForeignKey( DataType, models.PROTECT, related_name=\"deployments\", null=True, db_index=True, help_text=\"Primary data type of deployment.\") device_n = models.IntegerField(default=1, help_text=\"Numeric suffix of deployment, allowing for multiple deployments to share the same 'deployment_ID' and 'device_type'.\") deployment_start = models.DateTimeField( default=djtimezone.now, help_text=\"Start datetime of deployment.\") deployment_end = models.DateTimeField( blank=True, null=True, help_text=\"End time of deployment. Can be NULL if deployment is ongoing.\") device = models.ForeignKey( Device, on_delete=models.PROTECT, related_name=\"deployments\", db_index=True, help_text=\"Device of which this is a deployment.\") site = models.ForeignKey(Site, models.PROTECT, related_name=\"deployments\", help_text=\"Site at which this deployment is placed.\") project = models.ManyToManyField( Project, related_name=\"deployments\", blank=True, db_index=True, help_text=\"Projects to which this deployment is attached.\") latitude = models.DecimalField( max_digits=8, decimal_places=6, blank=True, null=True, help_text=\"Latitude at which this deployment is placed.\") longitude = models.DecimalField( max_digits=8, decimal_places=6, blank=True, null=True, help_text=\"Longitude at which this deployment is placed.\") point = gis_models.PointField( blank=True, null=True, spatial_index=True, help_text=\"Spatial point representing this deployment.\" ) extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that does not fit in other fields.\") is_active = models.BooleanField( default=True, help_text=\"Is the deployment currently active? Checked every hour.\") time_zone = TimeZoneField( use_pytz=True, default=settings.TIME_ZONE, help_text=\"Time zone for this deployment.\") owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_deployments\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Owner of deployment.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_deployments\", db_index=True, help_text=\"Managers of deployment.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_deployments\", db_index=True, help_text=\"Annotators of deployment.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_deployments\", db_index=True, help_text=\"Viewers of deployment.\") combo_project = models.CharField( max_length=100, blank=True, null=True, editable=False, help_text=\"String combining all projects.\") last_image = models.ForeignKey(\"DataFile\", blank=True, on_delete=models.SET_NULL, null=True, editable=False, related_name=\"deployment_last_image\", help_text=\"Last image (if any) linked to this deployment.\") thumb_url = models.CharField( max_length=500, null=True, blank=True, editable=False, help_text=\"Deployment thumbnail URL.\") def get_absolute_url(self): \"\"\" Returns the URL to this deployment detail. \"\"\" return f\"/deployments/{self.pk}\" def __str__(self): return self.deployment_device_ID def clean(self): \"\"\" Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. \"\"\" result, message = validators.deployment_start_time_after_end_time( self.deployment_start, self.deployment_end) if not result: raise ValidationError(message) result, message = validators.deployment_check_overlap( self.deployment_start, self.deployment_end, self.device, self.pk) if not result: raise ValidationError(message) super(Deployment, self).clean() def save(self, *args, **kwargs): \"\"\" Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. \"\"\" self.deployment_device_ID = f\"{self.deployment_ID}_{self.device.type.name}_{self.device_n}\" self.is_active = self.check_active() if self.device_type is None: self.device_type = self.device.type if self.longitude and self.latitude: self.point = Point(float(self.longitude), float(self.latitude), srid=4326) elif (self.longitude is None and self.latitude is None) and self.point is not None: self.longitude, self.latitude = self.point.coords else: self.point = None if self.id: self.combo_project = self.get_combo_project() super().save(*args, **kwargs) self.get_permissions() def get_permissions(self): \"\"\" Propagates permissions from device and project to this deployment. \"\"\" logger.info( f\"Set deployment {self} permissions from device and projects\") all_managers = self.device.managers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('managers__pk', flat=True))) self.managers.set(all_managers) all_annotators = self.device.annotators.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('annotators__pk', flat=True))) self.annotators.set(all_annotators) all_viewers = self.device.viewers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('viewers__pk', flat=True))) self.viewers.set(all_viewers) if self.owner: self.managers.add(self.owner) self.annotators.add(self.owner) self.viewers.add(self.owner) def get_combo_project(self): \"\"\" Returns a single space-separated string of sorted project IDs for this deployment. \"\"\" if self.project.all().exists(): all_proj_id = list( self.project.all().values_list(\"project_ID\", flat=True)) all_proj_id.sort() return \" \".join(all_proj_id) else: return \"\" def check_active(self): \"\"\" Returns True if the deployment is currently active, otherwise False. \"\"\" self.deployment_start = check_dt(self.deployment_start) if self.deployment_end: self.deployment_end = check_dt(self.deployment_end) if self.deployment_start <= djtimezone.now(): if self.deployment_end is None or self.deployment_end >= djtimezone.now(): return True return False def check_dates(self, dt_list: List[datetime]) -> List[bool]: \"\"\" Checks if datetimes in dt_list fall within the deployment's period. Args: dt_list (List[datetime]): List of datetimes to check. Returns: List[bool]: True for each value in dt_list that is within the deployment, else False. \"\"\" result_list = [] for dt in dt_list: dt = check_dt(dt, self.time_zone) result_list.append((dt >= self.deployment_start) and ((self.deployment_end is None) or (dt <= self.deployment_end))) return result_list def set_thumb_url(self): \"\"\" Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. \"\"\" last_file = self.files.filter( thumb_url__isnull=False, has_human=False).order_by('recording_dt').last() if last_file is not None: self.last_image = last_file self.thumb_url = last_file.thumb_url else: self.last_image = None self.thumb_url = None check_active() Returns True if the deployment is currently active, otherwise False. Source code in data_models\\models.py def check_active(self): \"\"\" Returns True if the deployment is currently active, otherwise False. \"\"\" self.deployment_start = check_dt(self.deployment_start) if self.deployment_end: self.deployment_end = check_dt(self.deployment_end) if self.deployment_start <= djtimezone.now(): if self.deployment_end is None or self.deployment_end >= djtimezone.now(): return True return False check_dates(dt_list) Checks if datetimes in dt_list fall within the deployment's period. Parameters: dt_list ( List [ datetime ] ) \u2013 List of datetimes to check. Returns: List [ bool ] \u2013 List[bool]: True for each value in dt_list that is within the deployment, else False. Source code in data_models\\models.py def check_dates(self, dt_list: List[datetime]) -> List[bool]: \"\"\" Checks if datetimes in dt_list fall within the deployment's period. Args: dt_list (List[datetime]): List of datetimes to check. Returns: List[bool]: True for each value in dt_list that is within the deployment, else False. \"\"\" result_list = [] for dt in dt_list: dt = check_dt(dt, self.time_zone) result_list.append((dt >= self.deployment_start) and ((self.deployment_end is None) or (dt <= self.deployment_end))) return result_list clean() Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. Source code in data_models\\models.py def clean(self): \"\"\" Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. \"\"\" result, message = validators.deployment_start_time_after_end_time( self.deployment_start, self.deployment_end) if not result: raise ValidationError(message) result, message = validators.deployment_check_overlap( self.deployment_start, self.deployment_end, self.device, self.pk) if not result: raise ValidationError(message) super(Deployment, self).clean() get_absolute_url() Returns the URL to this deployment detail. Source code in data_models\\models.py def get_absolute_url(self): \"\"\" Returns the URL to this deployment detail. \"\"\" return f\"/deployments/{self.pk}\" get_combo_project() Returns a single space-separated string of sorted project IDs for this deployment. Source code in data_models\\models.py def get_combo_project(self): \"\"\" Returns a single space-separated string of sorted project IDs for this deployment. \"\"\" if self.project.all().exists(): all_proj_id = list( self.project.all().values_list(\"project_ID\", flat=True)) all_proj_id.sort() return \" \".join(all_proj_id) else: return \"\" get_permissions() Propagates permissions from device and project to this deployment. Source code in data_models\\models.py def get_permissions(self): \"\"\" Propagates permissions from device and project to this deployment. \"\"\" logger.info( f\"Set deployment {self} permissions from device and projects\") all_managers = self.device.managers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('managers__pk', flat=True))) self.managers.set(all_managers) all_annotators = self.device.annotators.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('annotators__pk', flat=True))) self.annotators.set(all_annotators) all_viewers = self.device.viewers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('viewers__pk', flat=True))) self.viewers.set(all_viewers) if self.owner: self.managers.add(self.owner) self.annotators.add(self.owner) self.viewers.add(self.owner) save(*args, **kwargs) Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. \"\"\" self.deployment_device_ID = f\"{self.deployment_ID}_{self.device.type.name}_{self.device_n}\" self.is_active = self.check_active() if self.device_type is None: self.device_type = self.device.type if self.longitude and self.latitude: self.point = Point(float(self.longitude), float(self.latitude), srid=4326) elif (self.longitude is None and self.latitude is None) and self.point is not None: self.longitude, self.latitude = self.point.coords else: self.point = None if self.id: self.combo_project = self.get_combo_project() super().save(*args, **kwargs) self.get_permissions() set_thumb_url() Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. Source code in data_models\\models.py def set_thumb_url(self): \"\"\" Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. \"\"\" last_file = self.files.filter( thumb_url__isnull=False, has_human=False).order_by('recording_dt').last() if last_file is not None: self.last_image = last_file self.thumb_url = last_file.thumb_url else: self.last_image = None self.thumb_url = None Device Bases: BaseModel Represents a physical device or sensor. Source code in data_models\\models.py class Device(BaseModel): \"\"\" Represents a physical device or sensor. \"\"\" device_ID = models.CharField( max_length=20, unique=True, help_text=\"Unique identifier for device, such as a serial number.\") name = models.CharField(max_length=50, blank=True, help_text=\"Optional alternative name for device.\") model = models.ForeignKey( DeviceModel, models.PROTECT, related_name=\"registered_devices\", help_text=\"Device model.\") type = models.ForeignKey(DataType, models.PROTECT, related_name=\"devices\", null=True, db_index=True) owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_devices\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Device owner.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_devices\", db_index=True, help_text=\"Device managers.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_devices\", db_index=True, help_text=\"Device viewers.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_devices\", db_index=True, help_text=\"Device annotators.\") autoupdate = models.BooleanField( default=False, help_text=\"Is the device expected to autoupdate?\") update_time = models.IntegerField( default=48, help_text=\"Hours between expected updates. Notify users after this time.\") username = models.CharField( max_length=100, unique=True, null=True, blank=True, default=None, help_text=\"Device username for use with external storage.\") password = EncryptedCharField(max_length=100, blank=True, null=True, help_text=\"Device password for use with external storage.\") input_storage = models.ForeignKey( DataStorageInput, null=True, blank=True, related_name=\"linked_devices\", on_delete=models.SET_NULL, help_text=\"External storage for device.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that doesn't fit in existing fields.\") def is_active(self): \"\"\" Returns True if the device has at least one active deployment. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False def __str__(self): return self.device_ID def get_absolute_url(self): return f\"/devices/{self.pk}\" def save(self, *args, **kwargs): \"\"\" Assigns the device type from the model if not explicitly set. \"\"\" if not self.type: self.type = self.model.type super().save(*args, **kwargs) def clean(self): \"\"\" Validates device type matches model type. \"\"\" result, message = validators.device_check_type(self.type, self.model) if not result: raise ValidationError(message) super(Device, self).clean() def deployment_from_date(self, dt: datetime) -> \"Deployment\": \"\"\" Return the deployment for this device active at the given datetime. Args: dt (datetime): Datetime to check. If None, returns None. Returns: Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist: If no matching deployment found. MultipleObjectsReturned: If more than one deployment matches. \"\"\" logger.info( f\"Attempt to find deployment for device {self.device_ID} for {dt}\") if dt is None: return None all_deploys = self.deployments.all() all_tz = all_deploys.values('time_zone', 'pk') all_tz = [{'time_zone': x.get( 'time_zone', settings.TIME_ZONE), 'pk': x['pk']} for x in all_tz] all_dt = {x['pk']: check_dt(dt, x['time_zone']) for x in all_tz} whens = [When(pk=k, then=Value(v)) for k, v in all_dt.items()] all_deploys = all_deploys.annotate( dt=Case(*whens, output_field=DateTimeField(), default=Value(None))) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_start__lte=F('dt')) & Q( deployment_end_indefinite__gte=F('dt'))), output_field=BooleanField())) try: correct_deployment = all_deploys.get(in_deployment=True) return correct_deployment except (ObjectDoesNotExist, MultipleObjectsReturned): all_true_deployments = all_deploys.filter(in_deployment=True) logger.info( f\"Error: found {all_true_deployments.count()} deployments\") return None def check_overlap(self, new_start: datetime, new_end: Optional[datetime], deployment_pk: Optional[int]) -> List[str]: \"\"\" Check for overlapping deployments in the given date range, excluding a given deployment. Args: new_start (datetime): Start of new deployment. new_end (Optional[datetime]): End of new deployment (None = indefinite). deployment_pk (Optional[int]): Deployment to exclude. Returns: List[str]: Device IDs of overlapping deployments. \"\"\" new_start = check_dt(new_start) if new_end is None: new_end = new_start + timedelta(days=365 * 100) else: new_end = check_dt(new_end) all_deploys = self.deployments.all().exclude(pk=deployment_pk) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_end_indefinite__gte=new_start) & Q(deployment_start__lte=new_end)), output_field=BooleanField())) overlapping_deploys = all_deploys.filter(in_deployment=True) return list(overlapping_deploys.values_list('deployment_device_ID', flat=True)) check_overlap(new_start, new_end, deployment_pk) Check for overlapping deployments in the given date range, excluding a given deployment. Parameters: new_start ( datetime ) \u2013 Start of new deployment. new_end ( Optional [ datetime ] ) \u2013 End of new deployment (None = indefinite). deployment_pk ( Optional [ int ] ) \u2013 Deployment to exclude. Returns: List [ str ] \u2013 List[str]: Device IDs of overlapping deployments. Source code in data_models\\models.py def check_overlap(self, new_start: datetime, new_end: Optional[datetime], deployment_pk: Optional[int]) -> List[str]: \"\"\" Check for overlapping deployments in the given date range, excluding a given deployment. Args: new_start (datetime): Start of new deployment. new_end (Optional[datetime]): End of new deployment (None = indefinite). deployment_pk (Optional[int]): Deployment to exclude. Returns: List[str]: Device IDs of overlapping deployments. \"\"\" new_start = check_dt(new_start) if new_end is None: new_end = new_start + timedelta(days=365 * 100) else: new_end = check_dt(new_end) all_deploys = self.deployments.all().exclude(pk=deployment_pk) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_end_indefinite__gte=new_start) & Q(deployment_start__lte=new_end)), output_field=BooleanField())) overlapping_deploys = all_deploys.filter(in_deployment=True) return list(overlapping_deploys.values_list('deployment_device_ID', flat=True)) clean() Validates device type matches model type. Source code in data_models\\models.py def clean(self): \"\"\" Validates device type matches model type. \"\"\" result, message = validators.device_check_type(self.type, self.model) if not result: raise ValidationError(message) super(Device, self).clean() deployment_from_date(dt) Return the deployment for this device active at the given datetime. Parameters: dt ( datetime ) \u2013 Datetime to check. If None, returns None. Returns: Deployment \u2013 Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist \u2013 If no matching deployment found. MultipleObjectsReturned \u2013 If more than one deployment matches. Source code in data_models\\models.py def deployment_from_date(self, dt: datetime) -> \"Deployment\": \"\"\" Return the deployment for this device active at the given datetime. Args: dt (datetime): Datetime to check. If None, returns None. Returns: Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist: If no matching deployment found. MultipleObjectsReturned: If more than one deployment matches. \"\"\" logger.info( f\"Attempt to find deployment for device {self.device_ID} for {dt}\") if dt is None: return None all_deploys = self.deployments.all() all_tz = all_deploys.values('time_zone', 'pk') all_tz = [{'time_zone': x.get( 'time_zone', settings.TIME_ZONE), 'pk': x['pk']} for x in all_tz] all_dt = {x['pk']: check_dt(dt, x['time_zone']) for x in all_tz} whens = [When(pk=k, then=Value(v)) for k, v in all_dt.items()] all_deploys = all_deploys.annotate( dt=Case(*whens, output_field=DateTimeField(), default=Value(None))) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_start__lte=F('dt')) & Q( deployment_end_indefinite__gte=F('dt'))), output_field=BooleanField())) try: correct_deployment = all_deploys.get(in_deployment=True) return correct_deployment except (ObjectDoesNotExist, MultipleObjectsReturned): all_true_deployments = all_deploys.filter(in_deployment=True) logger.info( f\"Error: found {all_true_deployments.count()} deployments\") return None is_active() Returns True if the device has at least one active deployment. Source code in data_models\\models.py def is_active(self): \"\"\" Returns True if the device has at least one active deployment. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False save(*args, **kwargs) Assigns the device type from the model if not explicitly set. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Assigns the device type from the model if not explicitly set. \"\"\" if not self.type: self.type = self.model.type super().save(*args, **kwargs) DeviceModel Bases: BaseModel Represents a type of device, its manufacturer, and data type. Source code in data_models\\models.py class DeviceModel(BaseModel): \"\"\" Represents a type of device, its manufacturer, and data type. \"\"\" name = models.CharField(max_length=50, blank=True, unique=True, help_text=\"Name of device model. Used to find a data handler if available.\") manufacturer = models.CharField(max_length=50, blank=True, help_text=\"Device model manufacturer.\") type = models.ForeignKey(DataType, models.PROTECT, related_name=\"device_models\", help_text=\"Primary data type of device.\") owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_device_models\", on_delete=models.SET_NULL, null=True, help_text=\"User who registered this device model.\") colour = ColorField( blank=True, help_text=\"Override data type colour. Leave blank to use the default from this data type.\") symbol = IconField( blank=True, help_text=\"Override data type symbol. Leave blank to use the default from this data type.\") def __str__(self): return self.name Project Bases: BaseModel Represents a project and its metadata, including ownership and relations. Source code in data_models\\models.py class Project(BaseModel): \"\"\" Represents a project and its metadata, including ownership and relations. \"\"\" project_ID = models.CharField( max_length=20, unique=True, blank=True, help_text=\"Unique project identifier.\") name = models.CharField(max_length=50, help_text=\"Full project name.\") objectives = models.CharField( max_length=500, blank=True, help_text=\"Project objectives description.\") principal_investigator = models.CharField( max_length=50, blank=True, help_text=\"Full name of principal investigator.\") principal_investigator_email = models.CharField( max_length=100, blank=True, help_text=\"Principal investigator email.\") contact = models.CharField( max_length=50, blank=True, help_text=\"Name of primary contact.\") contact_email = models.CharField( max_length=100, blank=True, help_text=\"Contact email.\") organisation = models.CharField( max_length=100, blank=True, help_text=\"Organisation with which this project is associated.\") data_storages = models.ManyToManyField( DataStorageInput, related_name=\"linked_projects\", blank=True, help_text=\"External data storages available to this project.\" ) archive = models.ForeignKey( Archive, related_name=\"linked_projects\", null=True, blank=True, on_delete=models.SET_NULL, help_text=\"Data archive for project data.\" ) automated_tasks = models.ManyToManyField( \"ProjectJob\", related_name=\"linked_projects\", blank=True) def is_active(self): \"\"\" Returns True if this project has at least one active deployment, else False. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_projects\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Project owner.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_projects\", db_index=True, help_text=\"Project managers.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_projects\", db_index=True, help_text=\"Project viewers.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_projects\", db_index=True, help_text=\"Project annotators.\") clean_time = models.IntegerField( default=90, help_text=\"Days after last modification before archived file is removed from storage.\") def __str__(self): return self.project_ID def get_absolute_url(self): \"\"\" Returns the absolute URL for the project detail view. \"\"\" return reverse('project-detail', kwargs={'pk': self.pk}) def save(self, *args, **kwargs): \"\"\" Auto-generate project_ID if not provided. \"\"\" if self.project_ID == \"\" or self.project_ID is None: self.project_ID = self.name[0:10] return super().save(*args, **kwargs) get_absolute_url() Returns the absolute URL for the project detail view. Source code in data_models\\models.py def get_absolute_url(self): \"\"\" Returns the absolute URL for the project detail view. \"\"\" return reverse('project-detail', kwargs={'pk': self.pk}) is_active() Returns True if this project has at least one active deployment, else False. Source code in data_models\\models.py def is_active(self): \"\"\" Returns True if this project has at least one active deployment, else False. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False save(*args, **kwargs) Auto-generate project_ID if not provided. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Auto-generate project_ID if not provided. \"\"\" if self.project_ID == \"\" or self.project_ID is None: self.project_ID = self.name[0:10] return super().save(*args, **kwargs) ProjectJob Bases: BaseModel Represents a project-level job configuration. Source code in data_models\\models.py class ProjectJob(BaseModel): \"\"\" Represents a project-level job configuration. \"\"\" job_name = models.CharField(max_length=50, help_text=\"Name of job\") celery_job_name = models.CharField( max_length=50, help_text=\"Name of registered celery task.\") job_args = models.JSONField( default=dict, help_text=\"Additional arguments.\") def __str__(self): return self.job_name def get_job_signature(self, file_pks: list) -> dict: \"\"\" Generate a job signature for a Celery task. Args: file_pks (list): List of file primary keys to process. Returns: dict: Dictionary representing the job signature. \"\"\" return get_job_from_name(self.celery_job_name, \"datafile\", file_pks, self.job_args) get_job_signature(file_pks) Generate a job signature for a Celery task. Parameters: file_pks ( list ) \u2013 List of file primary keys to process. Returns: dict ( dict ) \u2013 Dictionary representing the job signature. Source code in data_models\\models.py def get_job_signature(self, file_pks: list) -> dict: \"\"\" Generate a job signature for a Celery task. Args: file_pks (list): List of file primary keys to process. Returns: dict: Dictionary representing the job signature. \"\"\" return get_job_from_name(self.celery_job_name, \"datafile\", file_pks, self.job_args) Site Bases: BaseModel Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name. Source code in data_models\\models.py class Site(BaseModel): \"\"\" Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name. \"\"\" name = models.CharField(max_length=50, help_text=\"Site name.\") short_name = models.CharField( max_length=10, blank=True, help_text=\"Site short name.\") def __str__(self): return self.name def save(self, *args, **kwargs): if self.short_name == \"\": self.short_name = self.name[0:10] return super().save(*args, **kwargs)","title":"models"},{"location":"reference/data_models/models/#data_models.models.DataFile","text":"Bases: BaseModel Represents a data file associated with a deployment. Source code in data_models\\models.py class DataFile(BaseModel): \"\"\" Represents a data file associated with a deployment. \"\"\" deployment = models.ForeignKey( Deployment, on_delete=models.CASCADE, related_name=\"files\", db_index=True, help_text=\"Deployment to which this datafile is linked.\") file_type = models.ForeignKey( DataType, models.PROTECT, related_name=\"files\", null=True, default=None, db_index=True, help_text=\"Data type of file.\") file_name = models.CharField( max_length=150, unique=True, db_index=True, help_text=\"File name.\") file_size = FileSizeField(help_text=\"Size of file in bytes.\") file_format = models.CharField(max_length=10, help_text=\"File extension.\") upload_dt = models.DateTimeField( default=djtimezone.now, help_text=\"Datetime at which the file was uploaded.\") recording_dt = models.DateTimeField( null=True, db_index=True, help_text=\"Datetime at which the file was recorded.\") path = models.CharField(max_length=500, help_text=\"Relative path.\") local_path = models.CharField(max_length=500, blank=True, help_text=\"Absolute file location on local storage, from which path is relative.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that does not fit in existing columns.\") linked_files = models.JSONField( default=dict, blank=True, help_text=\"Linked files, such as alternative representations of this file.\") thumb_url = models.CharField( max_length=500, null=True, blank=True, help_text=\"Thumbnail URL.\") local_storage = models.BooleanField( default=True, db_index=True, help_text=\"Is the file available on local storage?\") archived = models.BooleanField( default=False, help_text=\"Has the file been archived?\") tar_file = models.ForeignKey( TarFile, on_delete=models.SET_NULL, blank=True, null=True, related_name=\"files\", help_text=\"TAR file containing this file.\") favourite_of = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"favourites\", help_text=\"Users who have favourited this file.\") do_not_remove = models.BooleanField( default=False, help_text=\"If True, this file will not be removed during cleaning.\") original_name = models.CharField( max_length=100, blank=True, null=True, help_text=\"Original name of this file.\") file_url = models.CharField( max_length=500, null=True, blank=True, help_text=\"URL of this file\") tag = models.CharField(max_length=250, null=True, blank=True, db_index=True, help_text=\"Additional identifying tag of this file.\") has_human = models.BooleanField( default=False, db_index=True, help_text=\"True if this image has been annotated with a human.\") objects = DataFileQuerySet.as_manager() class Meta: indexes = [ GinIndex( OpClass(Upper('tag'), name='gin_trgm_ops'), name='upper_tag_gin_idx', ), GinIndex( OpClass(Upper('file_name'), name='gin_trgm_ops'), name='upper_file_name_gin_idx', ) ] def __str__(self): return f\"{self.file_name}{self.file_format}\" def get_absolute_url(self): return f\"/datafiles/{self.pk}\" def add_favourite(self, user: \"User\") -> None: \"\"\" Add a user to this file's favourites. \"\"\" self.favourite_of.add(user) self.save() def remove_favourite(self, user: \"User\") -> None: \"\"\" Remove a user from this file's favourites. \"\"\" self.favourite_of.remove(user) self.save() def full_path(self): \"\"\" Returns the full path to this file on local storage. \"\"\" return os.path.join(self.local_path, self.path, f\"{self.file_name}{self.file_format}\") def thumb_path(self): \"\"\" Returns the path to the thumbnail version of this file. \"\"\" return os.path.join(self.local_path, self.path, self.file_name+\"_THUMB.jpg\") def set_file_url(self): \"\"\" Sets the file_url for this file using storage settings. \"\"\" if self.local_storage: self.file_url = os.path.normpath( os.path.join(settings.FILE_STORAGE_URL, self.path, (self.file_name + self.file_format)) ).replace(\"\\\\\", \"/\") else: self.file_url = None def set_linked_files_urls(self): \"\"\" Set file URLs for each linked file. \"\"\" for key in self.linked_files.keys(): file_path = self.linked_files[key][\"path\"] rel_file_path = os.path.relpath( file_path, settings.FILE_STORAGE_ROOT) self.linked_files[key][\"url\"] = os.path.join( settings.FILE_STORAGE_URL, rel_file_path) def set_thumb_url(self, has_thumb: bool = True) -> None: \"\"\" Set the thumbnail URL for this file. \"\"\" if has_thumb: self.thumb_url = os.path.normpath(os.path.join( settings.FILE_STORAGE_URL, self.path, self.file_name+\"_THUMB.jpg\")) else: self.thumb_url = None def check_human(self): \"\"\" Checks for human observations and updates has_human accordingly. \"\"\" old_has_human = self.has_human new_has_human = self.observations.filter( taxon__taxon_code=settings.HUMAN_TAXON_CODE).exists() if old_has_human != new_has_human: self.has_human = new_has_human self.save() self.deployment.set_thumb_url() self.deployment.save() def clean_file(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Clean up this file and its associated resources. Args: delete_obj (bool): True if the object will be deleted from the database after cleaning. force_delete (bool): If True, force deletion/alteration even if file can't be deleted. Returns: bool: True if the file was successfully cleaned, False otherwise. \"\"\" logger.info( f\"Clean {self.file_name} - Delete object: {delete_obj} - Force delete:{force_delete}\") if (delete_obj and self.archived) and not force_delete: logger.info( f\"Clean {self.file_name} - Full delete failed - Archived file\") return False if (self.do_not_remove or self.deployment_last_image.exists() or self.favourite_of.exists()) and (not delete_obj or force_delete): logger.info(f\"Clean {self.file_name} - Failed - Protected file\") return False if self.local_storage: logger.info(f\"Clean {self.file_name} - Try to remove file\") success = try_remove_file_clean_dirs(self.full_path()) if not success and not force_delete: logger.error( f\"Clean {self.file_name} - Failed - Cannot delete local file\") return False elif success: logger.info( f\"Clean {self.file_name} - Try to remove file - success\") logger.info(f\"Clean {self.file_name} - Try to remove thumbnail\") if self.thumb_url is not None and self.thumb_url != \"\": thumb_path = self.thumb_path() success = try_remove_file_clean_dirs(thumb_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - failed\") for key, value in self.linked_files.items(): extra_version_path = value[\"path\"] logger.info(f\"Clean {self.file_name} - Try to remove {key}\") success = try_remove_file_clean_dirs(extra_version_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove {key} - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove {key} - failed\") if not delete_obj: logger.info(f\"Clean {self.file_name} - Altering database\") self.local_storage = False self.local_path = \"\" self.linked_files = {} self.set_thumb_url(False) self.save() return True def save(self, *args, **kwargs): \"\"\" Assigns file_type from deployment if not set, sets file_url, then saves. \"\"\" if self.file_type is None: self.file_type = self.deployment.device.type self.set_file_url() super().save(*args, **kwargs) def clean(self): \"\"\" Validates that this file is within its deployment's date range. Raises ValidationError if invalid. \"\"\" result, message = validators.data_file_in_deployment( self.recording_dt, self.deployment) if not result: raise ValidationError(message) super(DataFile, self).clean()","title":"DataFile"},{"location":"reference/data_models/models/#data_models.models.DataFile.add_favourite","text":"Add a user to this file's favourites. Source code in data_models\\models.py def add_favourite(self, user: \"User\") -> None: \"\"\" Add a user to this file's favourites. \"\"\" self.favourite_of.add(user) self.save()","title":"add_favourite"},{"location":"reference/data_models/models/#data_models.models.DataFile.check_human","text":"Checks for human observations and updates has_human accordingly. Source code in data_models\\models.py def check_human(self): \"\"\" Checks for human observations and updates has_human accordingly. \"\"\" old_has_human = self.has_human new_has_human = self.observations.filter( taxon__taxon_code=settings.HUMAN_TAXON_CODE).exists() if old_has_human != new_has_human: self.has_human = new_has_human self.save() self.deployment.set_thumb_url() self.deployment.save()","title":"check_human"},{"location":"reference/data_models/models/#data_models.models.DataFile.clean","text":"Validates that this file is within its deployment's date range. Raises ValidationError if invalid. Source code in data_models\\models.py def clean(self): \"\"\" Validates that this file is within its deployment's date range. Raises ValidationError if invalid. \"\"\" result, message = validators.data_file_in_deployment( self.recording_dt, self.deployment) if not result: raise ValidationError(message) super(DataFile, self).clean()","title":"clean"},{"location":"reference/data_models/models/#data_models.models.DataFile.clean_file","text":"Clean up this file and its associated resources. Parameters: delete_obj ( bool , default: False ) \u2013 True if the object will be deleted from the database after cleaning. force_delete ( bool , default: False ) \u2013 If True, force deletion/alteration even if file can't be deleted. Returns: bool ( bool ) \u2013 True if the file was successfully cleaned, False otherwise. Source code in data_models\\models.py def clean_file(self, delete_obj: bool = False, force_delete: bool = False) -> bool: \"\"\" Clean up this file and its associated resources. Args: delete_obj (bool): True if the object will be deleted from the database after cleaning. force_delete (bool): If True, force deletion/alteration even if file can't be deleted. Returns: bool: True if the file was successfully cleaned, False otherwise. \"\"\" logger.info( f\"Clean {self.file_name} - Delete object: {delete_obj} - Force delete:{force_delete}\") if (delete_obj and self.archived) and not force_delete: logger.info( f\"Clean {self.file_name} - Full delete failed - Archived file\") return False if (self.do_not_remove or self.deployment_last_image.exists() or self.favourite_of.exists()) and (not delete_obj or force_delete): logger.info(f\"Clean {self.file_name} - Failed - Protected file\") return False if self.local_storage: logger.info(f\"Clean {self.file_name} - Try to remove file\") success = try_remove_file_clean_dirs(self.full_path()) if not success and not force_delete: logger.error( f\"Clean {self.file_name} - Failed - Cannot delete local file\") return False elif success: logger.info( f\"Clean {self.file_name} - Try to remove file - success\") logger.info(f\"Clean {self.file_name} - Try to remove thumbnail\") if self.thumb_url is not None and self.thumb_url != \"\": thumb_path = self.thumb_path() success = try_remove_file_clean_dirs(thumb_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove thumbnail - failed\") for key, value in self.linked_files.items(): extra_version_path = value[\"path\"] logger.info(f\"Clean {self.file_name} - Try to remove {key}\") success = try_remove_file_clean_dirs(extra_version_path) if success: logger.info( f\"Clean {self.file_name} - Try to remove {key} - success\") else: logger.info( f\"Clean {self.file_name} - Try to remove {key} - failed\") if not delete_obj: logger.info(f\"Clean {self.file_name} - Altering database\") self.local_storage = False self.local_path = \"\" self.linked_files = {} self.set_thumb_url(False) self.save() return True","title":"clean_file"},{"location":"reference/data_models/models/#data_models.models.DataFile.full_path","text":"Returns the full path to this file on local storage. Source code in data_models\\models.py def full_path(self): \"\"\" Returns the full path to this file on local storage. \"\"\" return os.path.join(self.local_path, self.path, f\"{self.file_name}{self.file_format}\")","title":"full_path"},{"location":"reference/data_models/models/#data_models.models.DataFile.remove_favourite","text":"Remove a user from this file's favourites. Source code in data_models\\models.py def remove_favourite(self, user: \"User\") -> None: \"\"\" Remove a user from this file's favourites. \"\"\" self.favourite_of.remove(user) self.save()","title":"remove_favourite"},{"location":"reference/data_models/models/#data_models.models.DataFile.save","text":"Assigns file_type from deployment if not set, sets file_url, then saves. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Assigns file_type from deployment if not set, sets file_url, then saves. \"\"\" if self.file_type is None: self.file_type = self.deployment.device.type self.set_file_url() super().save(*args, **kwargs)","title":"save"},{"location":"reference/data_models/models/#data_models.models.DataFile.set_file_url","text":"Sets the file_url for this file using storage settings. Source code in data_models\\models.py def set_file_url(self): \"\"\" Sets the file_url for this file using storage settings. \"\"\" if self.local_storage: self.file_url = os.path.normpath( os.path.join(settings.FILE_STORAGE_URL, self.path, (self.file_name + self.file_format)) ).replace(\"\\\\\", \"/\") else: self.file_url = None","title":"set_file_url"},{"location":"reference/data_models/models/#data_models.models.DataFile.set_linked_files_urls","text":"Set file URLs for each linked file. Source code in data_models\\models.py def set_linked_files_urls(self): \"\"\" Set file URLs for each linked file. \"\"\" for key in self.linked_files.keys(): file_path = self.linked_files[key][\"path\"] rel_file_path = os.path.relpath( file_path, settings.FILE_STORAGE_ROOT) self.linked_files[key][\"url\"] = os.path.join( settings.FILE_STORAGE_URL, rel_file_path)","title":"set_linked_files_urls"},{"location":"reference/data_models/models/#data_models.models.DataFile.set_thumb_url","text":"Set the thumbnail URL for this file. Source code in data_models\\models.py def set_thumb_url(self, has_thumb: bool = True) -> None: \"\"\" Set the thumbnail URL for this file. \"\"\" if has_thumb: self.thumb_url = os.path.normpath(os.path.join( settings.FILE_STORAGE_URL, self.path, self.file_name+\"_THUMB.jpg\")) else: self.thumb_url = None","title":"set_thumb_url"},{"location":"reference/data_models/models/#data_models.models.DataFile.thumb_path","text":"Returns the path to the thumbnail version of this file. Source code in data_models\\models.py def thumb_path(self): \"\"\" Returns the path to the thumbnail version of this file. \"\"\" return os.path.join(self.local_path, self.path, self.file_name+\"_THUMB.jpg\")","title":"thumb_path"},{"location":"reference/data_models/models/#data_models.models.DataFileQuerySet","text":"Bases: ApproximateCountQuerySet Custom QuerySet for DataFile, providing path annotations and statistics. Source code in data_models\\models.py class DataFileQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for DataFile, providing path annotations and statistics. \"\"\" def full_paths(self): self = self.relative_paths() return self.annotate(full_path=Concat(F('local_path'), Value(os.sep), F('relative_path'))) def relative_paths(self): self = self.full_names() return self.annotate(relative_path=Concat(F('path'), Value(os.sep), F('full_name'))) def full_names(self): return self.annotate(full_name=Concat(F('file_name'), F('file_format'))) def file_size_unit(self, unit=\"\"): total_file_size = self.aggregate(total_file_size=Sum(\"file_size\"))[ \"total_file_size\"] converted_file_size = convert_unit(total_file_size, unit) return converted_file_size def file_count(self): return self.aggregate(total_file_size=Cast(Sum(\"file_size\"), models.FloatField())/Cast(Value(1024*1024*1024), models.FloatField()), object_n=Count(\"pk\"), archived_file_n=Sum(Case(When(local_storage=False, archived=True, then=Value(1)), default=Value(0)))) def min_date(self): return self.aggregate(min_date=Min(\"recording_dt\"))[\"min_date\"] def max_date(self): return self.aggregate(max_date=Max(\"recording_dt\"))[\"max_date\"] def device_type(self): return self.annotate(device_type=F('deployment__device__type__name'))","title":"DataFileQuerySet"},{"location":"reference/data_models/models/#data_models.models.DataType","text":"Bases: BaseModel Describes a type of data, including its name, color, and optional icon. Source code in data_models\\models.py class DataType(BaseModel): \"\"\" Describes a type of data, including its name, color, and optional icon. \"\"\" name = models.CharField(max_length=20, help_text=\"Name of data type.\") colour = ColorField(default=\"#FFFFFF\", help_text=\"Colour to use for this data type.\") symbol = IconField( blank=True, help_text=\"Symbol to use for this data type.\") def __str__(self): return self.name","title":"DataType"},{"location":"reference/data_models/models/#data_models.models.Deployment","text":"Bases: BaseModel Records a deployment of a device to a site, within a project. Source code in data_models\\models.py class Deployment(BaseModel): \"\"\" Records a deployment of a device to a site, within a project. \"\"\" deployment_device_ID = models.CharField( max_length=110, blank=True, editable=False, unique=True, help_text=\"Unique identifier combining 'deployment_ID', 'device_type' and 'device_n'.\") deployment_ID = models.CharField( max_length=80, help_text=\"An identifier for a deployment.\") device_type = models.ForeignKey( DataType, models.PROTECT, related_name=\"deployments\", null=True, db_index=True, help_text=\"Primary data type of deployment.\") device_n = models.IntegerField(default=1, help_text=\"Numeric suffix of deployment, allowing for multiple deployments to share the same 'deployment_ID' and 'device_type'.\") deployment_start = models.DateTimeField( default=djtimezone.now, help_text=\"Start datetime of deployment.\") deployment_end = models.DateTimeField( blank=True, null=True, help_text=\"End time of deployment. Can be NULL if deployment is ongoing.\") device = models.ForeignKey( Device, on_delete=models.PROTECT, related_name=\"deployments\", db_index=True, help_text=\"Device of which this is a deployment.\") site = models.ForeignKey(Site, models.PROTECT, related_name=\"deployments\", help_text=\"Site at which this deployment is placed.\") project = models.ManyToManyField( Project, related_name=\"deployments\", blank=True, db_index=True, help_text=\"Projects to which this deployment is attached.\") latitude = models.DecimalField( max_digits=8, decimal_places=6, blank=True, null=True, help_text=\"Latitude at which this deployment is placed.\") longitude = models.DecimalField( max_digits=8, decimal_places=6, blank=True, null=True, help_text=\"Longitude at which this deployment is placed.\") point = gis_models.PointField( blank=True, null=True, spatial_index=True, help_text=\"Spatial point representing this deployment.\" ) extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that does not fit in other fields.\") is_active = models.BooleanField( default=True, help_text=\"Is the deployment currently active? Checked every hour.\") time_zone = TimeZoneField( use_pytz=True, default=settings.TIME_ZONE, help_text=\"Time zone for this deployment.\") owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_deployments\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Owner of deployment.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_deployments\", db_index=True, help_text=\"Managers of deployment.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_deployments\", db_index=True, help_text=\"Annotators of deployment.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_deployments\", db_index=True, help_text=\"Viewers of deployment.\") combo_project = models.CharField( max_length=100, blank=True, null=True, editable=False, help_text=\"String combining all projects.\") last_image = models.ForeignKey(\"DataFile\", blank=True, on_delete=models.SET_NULL, null=True, editable=False, related_name=\"deployment_last_image\", help_text=\"Last image (if any) linked to this deployment.\") thumb_url = models.CharField( max_length=500, null=True, blank=True, editable=False, help_text=\"Deployment thumbnail URL.\") def get_absolute_url(self): \"\"\" Returns the URL to this deployment detail. \"\"\" return f\"/deployments/{self.pk}\" def __str__(self): return self.deployment_device_ID def clean(self): \"\"\" Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. \"\"\" result, message = validators.deployment_start_time_after_end_time( self.deployment_start, self.deployment_end) if not result: raise ValidationError(message) result, message = validators.deployment_check_overlap( self.deployment_start, self.deployment_end, self.device, self.pk) if not result: raise ValidationError(message) super(Deployment, self).clean() def save(self, *args, **kwargs): \"\"\" Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. \"\"\" self.deployment_device_ID = f\"{self.deployment_ID}_{self.device.type.name}_{self.device_n}\" self.is_active = self.check_active() if self.device_type is None: self.device_type = self.device.type if self.longitude and self.latitude: self.point = Point(float(self.longitude), float(self.latitude), srid=4326) elif (self.longitude is None and self.latitude is None) and self.point is not None: self.longitude, self.latitude = self.point.coords else: self.point = None if self.id: self.combo_project = self.get_combo_project() super().save(*args, **kwargs) self.get_permissions() def get_permissions(self): \"\"\" Propagates permissions from device and project to this deployment. \"\"\" logger.info( f\"Set deployment {self} permissions from device and projects\") all_managers = self.device.managers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('managers__pk', flat=True))) self.managers.set(all_managers) all_annotators = self.device.annotators.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('annotators__pk', flat=True))) self.annotators.set(all_annotators) all_viewers = self.device.viewers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('viewers__pk', flat=True))) self.viewers.set(all_viewers) if self.owner: self.managers.add(self.owner) self.annotators.add(self.owner) self.viewers.add(self.owner) def get_combo_project(self): \"\"\" Returns a single space-separated string of sorted project IDs for this deployment. \"\"\" if self.project.all().exists(): all_proj_id = list( self.project.all().values_list(\"project_ID\", flat=True)) all_proj_id.sort() return \" \".join(all_proj_id) else: return \"\" def check_active(self): \"\"\" Returns True if the deployment is currently active, otherwise False. \"\"\" self.deployment_start = check_dt(self.deployment_start) if self.deployment_end: self.deployment_end = check_dt(self.deployment_end) if self.deployment_start <= djtimezone.now(): if self.deployment_end is None or self.deployment_end >= djtimezone.now(): return True return False def check_dates(self, dt_list: List[datetime]) -> List[bool]: \"\"\" Checks if datetimes in dt_list fall within the deployment's period. Args: dt_list (List[datetime]): List of datetimes to check. Returns: List[bool]: True for each value in dt_list that is within the deployment, else False. \"\"\" result_list = [] for dt in dt_list: dt = check_dt(dt, self.time_zone) result_list.append((dt >= self.deployment_start) and ((self.deployment_end is None) or (dt <= self.deployment_end))) return result_list def set_thumb_url(self): \"\"\" Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. \"\"\" last_file = self.files.filter( thumb_url__isnull=False, has_human=False).order_by('recording_dt').last() if last_file is not None: self.last_image = last_file self.thumb_url = last_file.thumb_url else: self.last_image = None self.thumb_url = None","title":"Deployment"},{"location":"reference/data_models/models/#data_models.models.Deployment.check_active","text":"Returns True if the deployment is currently active, otherwise False. Source code in data_models\\models.py def check_active(self): \"\"\" Returns True if the deployment is currently active, otherwise False. \"\"\" self.deployment_start = check_dt(self.deployment_start) if self.deployment_end: self.deployment_end = check_dt(self.deployment_end) if self.deployment_start <= djtimezone.now(): if self.deployment_end is None or self.deployment_end >= djtimezone.now(): return True return False","title":"check_active"},{"location":"reference/data_models/models/#data_models.models.Deployment.check_dates","text":"Checks if datetimes in dt_list fall within the deployment's period. Parameters: dt_list ( List [ datetime ] ) \u2013 List of datetimes to check. Returns: List [ bool ] \u2013 List[bool]: True for each value in dt_list that is within the deployment, else False. Source code in data_models\\models.py def check_dates(self, dt_list: List[datetime]) -> List[bool]: \"\"\" Checks if datetimes in dt_list fall within the deployment's period. Args: dt_list (List[datetime]): List of datetimes to check. Returns: List[bool]: True for each value in dt_list that is within the deployment, else False. \"\"\" result_list = [] for dt in dt_list: dt = check_dt(dt, self.time_zone) result_list.append((dt >= self.deployment_start) and ((self.deployment_end is None) or (dt <= self.deployment_end))) return result_list","title":"check_dates"},{"location":"reference/data_models/models/#data_models.models.Deployment.clean","text":"Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. Source code in data_models\\models.py def clean(self): \"\"\" Validates deployment start/end time and checks for overlapping deployments. Raises ValidationError on failure. \"\"\" result, message = validators.deployment_start_time_after_end_time( self.deployment_start, self.deployment_end) if not result: raise ValidationError(message) result, message = validators.deployment_check_overlap( self.deployment_start, self.deployment_end, self.device, self.pk) if not result: raise ValidationError(message) super(Deployment, self).clean()","title":"clean"},{"location":"reference/data_models/models/#data_models.models.Deployment.get_absolute_url","text":"Returns the URL to this deployment detail. Source code in data_models\\models.py def get_absolute_url(self): \"\"\" Returns the URL to this deployment detail. \"\"\" return f\"/deployments/{self.pk}\"","title":"get_absolute_url"},{"location":"reference/data_models/models/#data_models.models.Deployment.get_combo_project","text":"Returns a single space-separated string of sorted project IDs for this deployment. Source code in data_models\\models.py def get_combo_project(self): \"\"\" Returns a single space-separated string of sorted project IDs for this deployment. \"\"\" if self.project.all().exists(): all_proj_id = list( self.project.all().values_list(\"project_ID\", flat=True)) all_proj_id.sort() return \" \".join(all_proj_id) else: return \"\"","title":"get_combo_project"},{"location":"reference/data_models/models/#data_models.models.Deployment.get_permissions","text":"Propagates permissions from device and project to this deployment. Source code in data_models\\models.py def get_permissions(self): \"\"\" Propagates permissions from device and project to this deployment. \"\"\" logger.info( f\"Set deployment {self} permissions from device and projects\") all_managers = self.device.managers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('managers__pk', flat=True))) self.managers.set(all_managers) all_annotators = self.device.annotators.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('annotators__pk', flat=True))) self.annotators.set(all_annotators) all_viewers = self.device.viewers.all().union( get_user_model().objects.filter(pk__in=self.project.all().values_list('viewers__pk', flat=True))) self.viewers.set(all_viewers) if self.owner: self.managers.add(self.owner) self.annotators.add(self.owner) self.viewers.add(self.owner)","title":"get_permissions"},{"location":"reference/data_models/models/#data_models.models.Deployment.save","text":"Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Sets deployment_device_ID, is_active, device_type, point, and combo_project before saving. \"\"\" self.deployment_device_ID = f\"{self.deployment_ID}_{self.device.type.name}_{self.device_n}\" self.is_active = self.check_active() if self.device_type is None: self.device_type = self.device.type if self.longitude and self.latitude: self.point = Point(float(self.longitude), float(self.latitude), srid=4326) elif (self.longitude is None and self.latitude is None) and self.point is not None: self.longitude, self.latitude = self.point.coords else: self.point = None if self.id: self.combo_project = self.get_combo_project() super().save(*args, **kwargs) self.get_permissions()","title":"save"},{"location":"reference/data_models/models/#data_models.models.Deployment.set_thumb_url","text":"Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. Source code in data_models\\models.py def set_thumb_url(self): \"\"\" Sets thumb_url and last_image based on latest file that has a thumbnail and no human involvement. \"\"\" last_file = self.files.filter( thumb_url__isnull=False, has_human=False).order_by('recording_dt').last() if last_file is not None: self.last_image = last_file self.thumb_url = last_file.thumb_url else: self.last_image = None self.thumb_url = None","title":"set_thumb_url"},{"location":"reference/data_models/models/#data_models.models.Device","text":"Bases: BaseModel Represents a physical device or sensor. Source code in data_models\\models.py class Device(BaseModel): \"\"\" Represents a physical device or sensor. \"\"\" device_ID = models.CharField( max_length=20, unique=True, help_text=\"Unique identifier for device, such as a serial number.\") name = models.CharField(max_length=50, blank=True, help_text=\"Optional alternative name for device.\") model = models.ForeignKey( DeviceModel, models.PROTECT, related_name=\"registered_devices\", help_text=\"Device model.\") type = models.ForeignKey(DataType, models.PROTECT, related_name=\"devices\", null=True, db_index=True) owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_devices\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Device owner.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_devices\", db_index=True, help_text=\"Device managers.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_devices\", db_index=True, help_text=\"Device viewers.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_devices\", db_index=True, help_text=\"Device annotators.\") autoupdate = models.BooleanField( default=False, help_text=\"Is the device expected to autoupdate?\") update_time = models.IntegerField( default=48, help_text=\"Hours between expected updates. Notify users after this time.\") username = models.CharField( max_length=100, unique=True, null=True, blank=True, default=None, help_text=\"Device username for use with external storage.\") password = EncryptedCharField(max_length=100, blank=True, null=True, help_text=\"Device password for use with external storage.\") input_storage = models.ForeignKey( DataStorageInput, null=True, blank=True, related_name=\"linked_devices\", on_delete=models.SET_NULL, help_text=\"External storage for device.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that doesn't fit in existing fields.\") def is_active(self): \"\"\" Returns True if the device has at least one active deployment. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False def __str__(self): return self.device_ID def get_absolute_url(self): return f\"/devices/{self.pk}\" def save(self, *args, **kwargs): \"\"\" Assigns the device type from the model if not explicitly set. \"\"\" if not self.type: self.type = self.model.type super().save(*args, **kwargs) def clean(self): \"\"\" Validates device type matches model type. \"\"\" result, message = validators.device_check_type(self.type, self.model) if not result: raise ValidationError(message) super(Device, self).clean() def deployment_from_date(self, dt: datetime) -> \"Deployment\": \"\"\" Return the deployment for this device active at the given datetime. Args: dt (datetime): Datetime to check. If None, returns None. Returns: Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist: If no matching deployment found. MultipleObjectsReturned: If more than one deployment matches. \"\"\" logger.info( f\"Attempt to find deployment for device {self.device_ID} for {dt}\") if dt is None: return None all_deploys = self.deployments.all() all_tz = all_deploys.values('time_zone', 'pk') all_tz = [{'time_zone': x.get( 'time_zone', settings.TIME_ZONE), 'pk': x['pk']} for x in all_tz] all_dt = {x['pk']: check_dt(dt, x['time_zone']) for x in all_tz} whens = [When(pk=k, then=Value(v)) for k, v in all_dt.items()] all_deploys = all_deploys.annotate( dt=Case(*whens, output_field=DateTimeField(), default=Value(None))) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_start__lte=F('dt')) & Q( deployment_end_indefinite__gte=F('dt'))), output_field=BooleanField())) try: correct_deployment = all_deploys.get(in_deployment=True) return correct_deployment except (ObjectDoesNotExist, MultipleObjectsReturned): all_true_deployments = all_deploys.filter(in_deployment=True) logger.info( f\"Error: found {all_true_deployments.count()} deployments\") return None def check_overlap(self, new_start: datetime, new_end: Optional[datetime], deployment_pk: Optional[int]) -> List[str]: \"\"\" Check for overlapping deployments in the given date range, excluding a given deployment. Args: new_start (datetime): Start of new deployment. new_end (Optional[datetime]): End of new deployment (None = indefinite). deployment_pk (Optional[int]): Deployment to exclude. Returns: List[str]: Device IDs of overlapping deployments. \"\"\" new_start = check_dt(new_start) if new_end is None: new_end = new_start + timedelta(days=365 * 100) else: new_end = check_dt(new_end) all_deploys = self.deployments.all().exclude(pk=deployment_pk) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_end_indefinite__gte=new_start) & Q(deployment_start__lte=new_end)), output_field=BooleanField())) overlapping_deploys = all_deploys.filter(in_deployment=True) return list(overlapping_deploys.values_list('deployment_device_ID', flat=True))","title":"Device"},{"location":"reference/data_models/models/#data_models.models.Device.check_overlap","text":"Check for overlapping deployments in the given date range, excluding a given deployment. Parameters: new_start ( datetime ) \u2013 Start of new deployment. new_end ( Optional [ datetime ] ) \u2013 End of new deployment (None = indefinite). deployment_pk ( Optional [ int ] ) \u2013 Deployment to exclude. Returns: List [ str ] \u2013 List[str]: Device IDs of overlapping deployments. Source code in data_models\\models.py def check_overlap(self, new_start: datetime, new_end: Optional[datetime], deployment_pk: Optional[int]) -> List[str]: \"\"\" Check for overlapping deployments in the given date range, excluding a given deployment. Args: new_start (datetime): Start of new deployment. new_end (Optional[datetime]): End of new deployment (None = indefinite). deployment_pk (Optional[int]): Deployment to exclude. Returns: List[str]: Device IDs of overlapping deployments. \"\"\" new_start = check_dt(new_start) if new_end is None: new_end = new_start + timedelta(days=365 * 100) else: new_end = check_dt(new_end) all_deploys = self.deployments.all().exclude(pk=deployment_pk) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_end_indefinite__gte=new_start) & Q(deployment_start__lte=new_end)), output_field=BooleanField())) overlapping_deploys = all_deploys.filter(in_deployment=True) return list(overlapping_deploys.values_list('deployment_device_ID', flat=True))","title":"check_overlap"},{"location":"reference/data_models/models/#data_models.models.Device.clean","text":"Validates device type matches model type. Source code in data_models\\models.py def clean(self): \"\"\" Validates device type matches model type. \"\"\" result, message = validators.device_check_type(self.type, self.model) if not result: raise ValidationError(message) super(Device, self).clean()","title":"clean"},{"location":"reference/data_models/models/#data_models.models.Device.deployment_from_date","text":"Return the deployment for this device active at the given datetime. Parameters: dt ( datetime ) \u2013 Datetime to check. If None, returns None. Returns: Deployment \u2013 Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist \u2013 If no matching deployment found. MultipleObjectsReturned \u2013 If more than one deployment matches. Source code in data_models\\models.py def deployment_from_date(self, dt: datetime) -> \"Deployment\": \"\"\" Return the deployment for this device active at the given datetime. Args: dt (datetime): Datetime to check. If None, returns None. Returns: Deployment or None: The matching deployment, or None if ambiguous or not found. Raises: ObjectDoesNotExist: If no matching deployment found. MultipleObjectsReturned: If more than one deployment matches. \"\"\" logger.info( f\"Attempt to find deployment for device {self.device_ID} for {dt}\") if dt is None: return None all_deploys = self.deployments.all() all_tz = all_deploys.values('time_zone', 'pk') all_tz = [{'time_zone': x.get( 'time_zone', settings.TIME_ZONE), 'pk': x['pk']} for x in all_tz] all_dt = {x['pk']: check_dt(dt, x['time_zone']) for x in all_tz} whens = [When(pk=k, then=Value(v)) for k, v in all_dt.items()] all_deploys = all_deploys.annotate( dt=Case(*whens, output_field=DateTimeField(), default=Value(None))) all_deploys = all_deploys.annotate(deployment_end_indefinite=Case( When(deployment_end__isnull=True, then=ExpressionWrapper(F('deployment_start') + timedelta(days=365 * 100), output_field=DateTimeField())), default=F('deployment_end') )) all_deploys = all_deploys.annotate(in_deployment=ExpressionWrapper( Q(Q(deployment_start__lte=F('dt')) & Q( deployment_end_indefinite__gte=F('dt'))), output_field=BooleanField())) try: correct_deployment = all_deploys.get(in_deployment=True) return correct_deployment except (ObjectDoesNotExist, MultipleObjectsReturned): all_true_deployments = all_deploys.filter(in_deployment=True) logger.info( f\"Error: found {all_true_deployments.count()} deployments\") return None","title":"deployment_from_date"},{"location":"reference/data_models/models/#data_models.models.Device.is_active","text":"Returns True if the device has at least one active deployment. Source code in data_models\\models.py def is_active(self): \"\"\" Returns True if the device has at least one active deployment. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False","title":"is_active"},{"location":"reference/data_models/models/#data_models.models.Device.save","text":"Assigns the device type from the model if not explicitly set. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Assigns the device type from the model if not explicitly set. \"\"\" if not self.type: self.type = self.model.type super().save(*args, **kwargs)","title":"save"},{"location":"reference/data_models/models/#data_models.models.DeviceModel","text":"Bases: BaseModel Represents a type of device, its manufacturer, and data type. Source code in data_models\\models.py class DeviceModel(BaseModel): \"\"\" Represents a type of device, its manufacturer, and data type. \"\"\" name = models.CharField(max_length=50, blank=True, unique=True, help_text=\"Name of device model. Used to find a data handler if available.\") manufacturer = models.CharField(max_length=50, blank=True, help_text=\"Device model manufacturer.\") type = models.ForeignKey(DataType, models.PROTECT, related_name=\"device_models\", help_text=\"Primary data type of device.\") owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_device_models\", on_delete=models.SET_NULL, null=True, help_text=\"User who registered this device model.\") colour = ColorField( blank=True, help_text=\"Override data type colour. Leave blank to use the default from this data type.\") symbol = IconField( blank=True, help_text=\"Override data type symbol. Leave blank to use the default from this data type.\") def __str__(self): return self.name","title":"DeviceModel"},{"location":"reference/data_models/models/#data_models.models.Project","text":"Bases: BaseModel Represents a project and its metadata, including ownership and relations. Source code in data_models\\models.py class Project(BaseModel): \"\"\" Represents a project and its metadata, including ownership and relations. \"\"\" project_ID = models.CharField( max_length=20, unique=True, blank=True, help_text=\"Unique project identifier.\") name = models.CharField(max_length=50, help_text=\"Full project name.\") objectives = models.CharField( max_length=500, blank=True, help_text=\"Project objectives description.\") principal_investigator = models.CharField( max_length=50, blank=True, help_text=\"Full name of principal investigator.\") principal_investigator_email = models.CharField( max_length=100, blank=True, help_text=\"Principal investigator email.\") contact = models.CharField( max_length=50, blank=True, help_text=\"Name of primary contact.\") contact_email = models.CharField( max_length=100, blank=True, help_text=\"Contact email.\") organisation = models.CharField( max_length=100, blank=True, help_text=\"Organisation with which this project is associated.\") data_storages = models.ManyToManyField( DataStorageInput, related_name=\"linked_projects\", blank=True, help_text=\"External data storages available to this project.\" ) archive = models.ForeignKey( Archive, related_name=\"linked_projects\", null=True, blank=True, on_delete=models.SET_NULL, help_text=\"Data archive for project data.\" ) automated_tasks = models.ManyToManyField( \"ProjectJob\", related_name=\"linked_projects\", blank=True) def is_active(self): \"\"\" Returns True if this project has at least one active deployment, else False. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_projects\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"Project owner.\") managers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"managed_projects\", db_index=True, help_text=\"Project managers.\") viewers = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"viewable_projects\", db_index=True, help_text=\"Project viewers.\") annotators = models.ManyToManyField( settings.AUTH_USER_MODEL, blank=True, related_name=\"annotatable_projects\", db_index=True, help_text=\"Project annotators.\") clean_time = models.IntegerField( default=90, help_text=\"Days after last modification before archived file is removed from storage.\") def __str__(self): return self.project_ID def get_absolute_url(self): \"\"\" Returns the absolute URL for the project detail view. \"\"\" return reverse('project-detail', kwargs={'pk': self.pk}) def save(self, *args, **kwargs): \"\"\" Auto-generate project_ID if not provided. \"\"\" if self.project_ID == \"\" or self.project_ID is None: self.project_ID = self.name[0:10] return super().save(*args, **kwargs)","title":"Project"},{"location":"reference/data_models/models/#data_models.models.Project.get_absolute_url","text":"Returns the absolute URL for the project detail view. Source code in data_models\\models.py def get_absolute_url(self): \"\"\" Returns the absolute URL for the project detail view. \"\"\" return reverse('project-detail', kwargs={'pk': self.pk})","title":"get_absolute_url"},{"location":"reference/data_models/models/#data_models.models.Project.is_active","text":"Returns True if this project has at least one active deployment, else False. Source code in data_models\\models.py def is_active(self): \"\"\" Returns True if this project has at least one active deployment, else False. \"\"\" if self.id: return self.deployments.filter(is_active=True).exists() return False","title":"is_active"},{"location":"reference/data_models/models/#data_models.models.Project.save","text":"Auto-generate project_ID if not provided. Source code in data_models\\models.py def save(self, *args, **kwargs): \"\"\" Auto-generate project_ID if not provided. \"\"\" if self.project_ID == \"\" or self.project_ID is None: self.project_ID = self.name[0:10] return super().save(*args, **kwargs)","title":"save"},{"location":"reference/data_models/models/#data_models.models.ProjectJob","text":"Bases: BaseModel Represents a project-level job configuration. Source code in data_models\\models.py class ProjectJob(BaseModel): \"\"\" Represents a project-level job configuration. \"\"\" job_name = models.CharField(max_length=50, help_text=\"Name of job\") celery_job_name = models.CharField( max_length=50, help_text=\"Name of registered celery task.\") job_args = models.JSONField( default=dict, help_text=\"Additional arguments.\") def __str__(self): return self.job_name def get_job_signature(self, file_pks: list) -> dict: \"\"\" Generate a job signature for a Celery task. Args: file_pks (list): List of file primary keys to process. Returns: dict: Dictionary representing the job signature. \"\"\" return get_job_from_name(self.celery_job_name, \"datafile\", file_pks, self.job_args)","title":"ProjectJob"},{"location":"reference/data_models/models/#data_models.models.ProjectJob.get_job_signature","text":"Generate a job signature for a Celery task. Parameters: file_pks ( list ) \u2013 List of file primary keys to process. Returns: dict ( dict ) \u2013 Dictionary representing the job signature. Source code in data_models\\models.py def get_job_signature(self, file_pks: list) -> dict: \"\"\" Generate a job signature for a Celery task. Args: file_pks (list): List of file primary keys to process. Returns: dict: Dictionary representing the job signature. \"\"\" return get_job_from_name(self.celery_job_name, \"datafile\", file_pks, self.job_args)","title":"get_job_signature"},{"location":"reference/data_models/models/#data_models.models.Site","text":"Bases: BaseModel Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name. Source code in data_models\\models.py class Site(BaseModel): \"\"\" Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name. \"\"\" name = models.CharField(max_length=50, help_text=\"Site name.\") short_name = models.CharField( max_length=10, blank=True, help_text=\"Site short name.\") def __str__(self): return self.name def save(self, *args, **kwargs): if self.short_name == \"\": self.short_name = self.name[0:10] return super().save(*args, **kwargs)","title":"Site"},{"location":"reference/data_models/plotting_functions/","text":"create_metric_dicts(file_dict, x_key, x_label, plot_type) Create a dictionary of metrics for plotting. This function processes a dictionary of file metrics and generates a structured dictionary for plotting purposes, including x-values, y-values, labels, and plot types. Parameters: file_dict ( Dict [ str , List [ Any ]] ) \u2013 A dictionary containing file metrics. Keys represent metric names, and values are lists of metric data. x_key ( str ) \u2013 The key in file_dict representing the x-axis values. x_label ( str ) \u2013 The label for the x-axis. plot_type ( List [ str ] ) \u2013 A list of plot types (e.g., \"bar\", \"scatter\") to be associated with the metrics. Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary Dict [ str , Dict [ str , Any ]] \u2013 containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Source code in data_models\\plotting_functions.py def create_metric_dicts(file_dict: Dict[str, List[Any]], x_key: str, x_label: str, plot_type: List[str]) -> Dict[str, Dict[str, Any]]: \"\"\" Create a dictionary of metrics for plotting. This function processes a dictionary of file metrics and generates a structured dictionary for plotting purposes, including x-values, y-values, labels, and plot types. Args: file_dict (Dict[str, List[Any]]): A dictionary containing file metrics. Keys represent metric names, and values are lists of metric data. x_key (str): The key in `file_dict` representing the x-axis values. x_label (str): The label for the x-axis. plot_type (List[str]): A list of plot types (e.g., \"bar\", \"scatter\") to be associated with the metrics. Returns: Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). \"\"\" x_values = [str(x) if isinstance(x, (datetime, pd.Timestamp)) else x for x in file_dict[x_key]] metrics = [x for x in file_dict.keys() if x != x_key] all_metrics_dict: Dict[str, Dict[str, Any]] = {} for metric in metrics: split_metric = metric.split('__') if len(split_metric) > 1: metric_name, y_label = split_metric if y_label == \"\": continue else: metric_name = metric y_label = metric y_values = file_dict[metric] metric_dict = { \"name\": metric_name, \"x_label\": x_label, \"y_label\": y_label, \"x_values\": x_values, \"y_values\": y_values, \"plot_type\": plot_type, } all_metrics_dict[metric_name] = metric_dict return all_metrics_dict get_all_file_metric_dicts(data_files, get_report_metrics=True) Generate a dictionary containing metrics for all files. This function aggregates metrics from the database and optionally from report files. Parameters: data_files ( QuerySet ) \u2013 A Django QuerySet containing file data. get_report_metrics ( bool , default: True ) \u2013 Whether to include metrics from report files. Defaults to True. Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary containing metrics for all files. Each key represents a metric name, Dict [ str , Dict [ str , Any ]] \u2013 and the value is a dictionary with details about the metric (e.g., x_values, y_values, labels, plot types). Source code in data_models\\plotting_functions.py def get_all_file_metric_dicts(data_files: QuerySet, get_report_metrics: bool = True) -> Dict[str, Dict[str, Any]]: \"\"\" Generate a dictionary containing metrics for all files. This function aggregates metrics from the database and optionally from report files. Args: data_files (QuerySet): A Django QuerySet containing file data. get_report_metrics (bool): Whether to include metrics from report files. Defaults to True. Returns: Dict[str, Dict[str, Any]]: A dictionary containing metrics for all files. Each key represents a metric name, and the value is a dictionary with details about the metric (e.g., x_values, y_values, labels, plot types). \"\"\" all_file_metric_dict = {} # Database metrics db_file_dict = get_database_file_metrics(data_files) if db_file_dict is not None: db_file_metric_dict = create_metric_dicts( db_file_dict, 'recording_dt__date', 'Date', [\"bar\", \"scatter\"] ) all_file_metric_dict.update(db_file_metric_dict) if get_report_metrics: # Report file metrics report_file_metric_dict = report_file_metrics(data_files) all_file_metric_dict.update(report_file_metric_dict) return all_file_metric_dict get_database_file_metrics(data_files) Aggregates metrics from a database query of data files, grouping by recording date. Args: data_files (QuerySet): A Django QuerySet containing data file records. Each record is expected to have fields recording_dt__date , id , and file_size . Returns: Optional[Dict[str, List[Union[date, int]]]]: A dictionary containing aggregated metrics if data exists, or None if no data is available. The dictionary keys are: - 'recording_dt__date': List of unique recording dates. - 'files_per_day__number_of_files': List of the number of files recorded per day. - 'file_volume_per_day__bytes': List of the total file size (in bytes) recorded per day. Source code in data_models\\plotting_functions.py def get_database_file_metrics(data_files: QuerySet) -> Optional[Dict[str, List[Union[date, int]]]]: \"\"\" Aggregates metrics from a database query of data files, grouping by recording date. Args: data_files (QuerySet): A Django QuerySet containing data file records. Each record is expected to have fields `recording_dt__date`, `id`, and `file_size`. Returns: Optional[Dict[str, List[Union[date, int]]]]: A dictionary containing aggregated metrics if data exists, or `None` if no data is available. The dictionary keys are: - 'recording_dt__date': List of unique recording dates. - 'files_per_day__number_of_files': List of the number of files recorded per day. - 'file_volume_per_day__bytes': List of the total file size (in bytes) recorded per day. \"\"\" # do aggregation of files per day file_dict = data_files.values('recording_dt__date').order_by('recording_dt__date').annotate( files_per_day__number_of_files=Count('id'), file_volume_per_day__bytes=Sum('file_size')).values( 'recording_dt__date', 'files_per_day__number_of_files', 'file_volume_per_day__bytes') file_dict = list(file_dict) if len(file_dict) == 0: return None file_dict = {k: [current_dict[k] for current_dict in file_dict] for k in file_dict[0]} return file_dict report_file_metrics(data_files) Extract metrics from report files and prepare them for plotting. This function filters report files from the provided QuerySet, reads their contents, processes the data, and generates a dictionary of metrics suitable for plotting. Parameters: data_files ( QuerySet ) \u2013 A Django QuerySet containing file data. Each record is expected to have fields file_type__name , local_storage , file_format , local_path , path , file_name , and file_format . Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary Dict [ str , Dict [ str , Any ]] \u2013 containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Dict [ str , Dict [ str , Any ]] \u2013 Returns an empty dictionary if no valid report files are found or if no datetime columns exist in the data. Source code in data_models\\plotting_functions.py def report_file_metrics(data_files: QuerySet) -> Dict[str, Dict[str, Any]]: \"\"\" Extract metrics from report files and prepare them for plotting. This function filters report files from the provided QuerySet, reads their contents, processes the data, and generates a dictionary of metrics suitable for plotting. Args: data_files (QuerySet): A Django QuerySet containing file data. Each record is expected to have fields `file_type__name`, `local_storage`, `file_format`, `local_path`, `path`, `file_name`, and `file_format`. Returns: Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Returns an empty dictionary if no valid report files are found or if no datetime columns exist in the data. \"\"\" data_files = data_files.filter( file_type__name=\"report\", local_storage=True, file_format=\".csv\" ) if not data_files.exists(): return {} file_path_dicts = list(data_files.values( \"local_path\", \"path\", \"file_name\", \"file_format\" )) # Read the CSV files into DataFrames all_df_list: List[pd.DataFrame] = [] for file_path_dict in file_path_dicts: file_path = os.path.join( file_path_dict[\"local_path\"], file_path_dict[\"path\"], file_path_dict[\"file_name\"] + file_path_dict[\"file_format\"] ) df = pd.read_csv(file_path) df.columns = df.columns.str.lower() all_df_list.append(df) full_df = pd.concat(all_df_list) # Attempt to convert columns to datetime if applicable full_df = full_df.apply( lambda col: pd.to_datetime(col, errors='ignore') if col.dtypes == object and 'date' in col.name else col, axis=0 ) full_df = full_df.convert_dtypes() date_time_cols = full_df.select_dtypes(include=[np.datetime64]) # Assume the first datetime column is the correct one date_time_keys = list(date_time_cols.columns.values) if len(date_time_keys) == 0: return {} date_time_key = date_time_keys[0] if len(date_time_keys) > 1: full_df.drop(columns=date_time_keys[1:], inplace=True) full_df = full_df.sort_values(by=date_time_key) full_df_numeric = full_df.select_dtypes(include=[np.datetime64, np.number]) file_dict = full_df_numeric.to_dict(orient=\"list\") metric_dict = create_metric_dicts( file_dict, date_time_key, \"Date\", [\"scatter\"] ) return metric_dict","title":"plotting_functions"},{"location":"reference/data_models/plotting_functions/#data_models.plotting_functions.create_metric_dicts","text":"Create a dictionary of metrics for plotting. This function processes a dictionary of file metrics and generates a structured dictionary for plotting purposes, including x-values, y-values, labels, and plot types. Parameters: file_dict ( Dict [ str , List [ Any ]] ) \u2013 A dictionary containing file metrics. Keys represent metric names, and values are lists of metric data. x_key ( str ) \u2013 The key in file_dict representing the x-axis values. x_label ( str ) \u2013 The label for the x-axis. plot_type ( List [ str ] ) \u2013 A list of plot types (e.g., \"bar\", \"scatter\") to be associated with the metrics. Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary Dict [ str , Dict [ str , Any ]] \u2013 containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Source code in data_models\\plotting_functions.py def create_metric_dicts(file_dict: Dict[str, List[Any]], x_key: str, x_label: str, plot_type: List[str]) -> Dict[str, Dict[str, Any]]: \"\"\" Create a dictionary of metrics for plotting. This function processes a dictionary of file metrics and generates a structured dictionary for plotting purposes, including x-values, y-values, labels, and plot types. Args: file_dict (Dict[str, List[Any]]): A dictionary containing file metrics. Keys represent metric names, and values are lists of metric data. x_key (str): The key in `file_dict` representing the x-axis values. x_label (str): The label for the x-axis. plot_type (List[str]): A list of plot types (e.g., \"bar\", \"scatter\") to be associated with the metrics. Returns: Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). \"\"\" x_values = [str(x) if isinstance(x, (datetime, pd.Timestamp)) else x for x in file_dict[x_key]] metrics = [x for x in file_dict.keys() if x != x_key] all_metrics_dict: Dict[str, Dict[str, Any]] = {} for metric in metrics: split_metric = metric.split('__') if len(split_metric) > 1: metric_name, y_label = split_metric if y_label == \"\": continue else: metric_name = metric y_label = metric y_values = file_dict[metric] metric_dict = { \"name\": metric_name, \"x_label\": x_label, \"y_label\": y_label, \"x_values\": x_values, \"y_values\": y_values, \"plot_type\": plot_type, } all_metrics_dict[metric_name] = metric_dict return all_metrics_dict","title":"create_metric_dicts"},{"location":"reference/data_models/plotting_functions/#data_models.plotting_functions.get_all_file_metric_dicts","text":"Generate a dictionary containing metrics for all files. This function aggregates metrics from the database and optionally from report files. Parameters: data_files ( QuerySet ) \u2013 A Django QuerySet containing file data. get_report_metrics ( bool , default: True ) \u2013 Whether to include metrics from report files. Defaults to True. Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary containing metrics for all files. Each key represents a metric name, Dict [ str , Dict [ str , Any ]] \u2013 and the value is a dictionary with details about the metric (e.g., x_values, y_values, labels, plot types). Source code in data_models\\plotting_functions.py def get_all_file_metric_dicts(data_files: QuerySet, get_report_metrics: bool = True) -> Dict[str, Dict[str, Any]]: \"\"\" Generate a dictionary containing metrics for all files. This function aggregates metrics from the database and optionally from report files. Args: data_files (QuerySet): A Django QuerySet containing file data. get_report_metrics (bool): Whether to include metrics from report files. Defaults to True. Returns: Dict[str, Dict[str, Any]]: A dictionary containing metrics for all files. Each key represents a metric name, and the value is a dictionary with details about the metric (e.g., x_values, y_values, labels, plot types). \"\"\" all_file_metric_dict = {} # Database metrics db_file_dict = get_database_file_metrics(data_files) if db_file_dict is not None: db_file_metric_dict = create_metric_dicts( db_file_dict, 'recording_dt__date', 'Date', [\"bar\", \"scatter\"] ) all_file_metric_dict.update(db_file_metric_dict) if get_report_metrics: # Report file metrics report_file_metric_dict = report_file_metrics(data_files) all_file_metric_dict.update(report_file_metric_dict) return all_file_metric_dict","title":"get_all_file_metric_dicts"},{"location":"reference/data_models/plotting_functions/#data_models.plotting_functions.get_database_file_metrics","text":"Aggregates metrics from a database query of data files, grouping by recording date. Args: data_files (QuerySet): A Django QuerySet containing data file records. Each record is expected to have fields recording_dt__date , id , and file_size . Returns: Optional[Dict[str, List[Union[date, int]]]]: A dictionary containing aggregated metrics if data exists, or None if no data is available. The dictionary keys are: - 'recording_dt__date': List of unique recording dates. - 'files_per_day__number_of_files': List of the number of files recorded per day. - 'file_volume_per_day__bytes': List of the total file size (in bytes) recorded per day. Source code in data_models\\plotting_functions.py def get_database_file_metrics(data_files: QuerySet) -> Optional[Dict[str, List[Union[date, int]]]]: \"\"\" Aggregates metrics from a database query of data files, grouping by recording date. Args: data_files (QuerySet): A Django QuerySet containing data file records. Each record is expected to have fields `recording_dt__date`, `id`, and `file_size`. Returns: Optional[Dict[str, List[Union[date, int]]]]: A dictionary containing aggregated metrics if data exists, or `None` if no data is available. The dictionary keys are: - 'recording_dt__date': List of unique recording dates. - 'files_per_day__number_of_files': List of the number of files recorded per day. - 'file_volume_per_day__bytes': List of the total file size (in bytes) recorded per day. \"\"\" # do aggregation of files per day file_dict = data_files.values('recording_dt__date').order_by('recording_dt__date').annotate( files_per_day__number_of_files=Count('id'), file_volume_per_day__bytes=Sum('file_size')).values( 'recording_dt__date', 'files_per_day__number_of_files', 'file_volume_per_day__bytes') file_dict = list(file_dict) if len(file_dict) == 0: return None file_dict = {k: [current_dict[k] for current_dict in file_dict] for k in file_dict[0]} return file_dict","title":"get_database_file_metrics"},{"location":"reference/data_models/plotting_functions/#data_models.plotting_functions.report_file_metrics","text":"Extract metrics from report files and prepare them for plotting. This function filters report files from the provided QuerySet, reads their contents, processes the data, and generates a dictionary of metrics suitable for plotting. Parameters: data_files ( QuerySet ) \u2013 A Django QuerySet containing file data. Each record is expected to have fields file_type__name , local_storage , file_format , local_path , path , file_name , and file_format . Returns: Dict [ str , Dict [ str , Any ]] \u2013 Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary Dict [ str , Dict [ str , Any ]] \u2013 containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Dict [ str , Dict [ str , Any ]] \u2013 Returns an empty dictionary if no valid report files are found or if no datetime columns exist in the data. Source code in data_models\\plotting_functions.py def report_file_metrics(data_files: QuerySet) -> Dict[str, Dict[str, Any]]: \"\"\" Extract metrics from report files and prepare them for plotting. This function filters report files from the provided QuerySet, reads their contents, processes the data, and generates a dictionary of metrics suitable for plotting. Args: data_files (QuerySet): A Django QuerySet containing file data. Each record is expected to have fields `file_type__name`, `local_storage`, `file_format`, `local_path`, `path`, `file_name`, and `file_format`. Returns: Dict[str, Dict[str, Any]]: A dictionary where each key is a metric name, and the value is another dictionary containing details about the metric (e.g., x_label, y_label, x_values, y_values, plot_type). Returns an empty dictionary if no valid report files are found or if no datetime columns exist in the data. \"\"\" data_files = data_files.filter( file_type__name=\"report\", local_storage=True, file_format=\".csv\" ) if not data_files.exists(): return {} file_path_dicts = list(data_files.values( \"local_path\", \"path\", \"file_name\", \"file_format\" )) # Read the CSV files into DataFrames all_df_list: List[pd.DataFrame] = [] for file_path_dict in file_path_dicts: file_path = os.path.join( file_path_dict[\"local_path\"], file_path_dict[\"path\"], file_path_dict[\"file_name\"] + file_path_dict[\"file_format\"] ) df = pd.read_csv(file_path) df.columns = df.columns.str.lower() all_df_list.append(df) full_df = pd.concat(all_df_list) # Attempt to convert columns to datetime if applicable full_df = full_df.apply( lambda col: pd.to_datetime(col, errors='ignore') if col.dtypes == object and 'date' in col.name else col, axis=0 ) full_df = full_df.convert_dtypes() date_time_cols = full_df.select_dtypes(include=[np.datetime64]) # Assume the first datetime column is the correct one date_time_keys = list(date_time_cols.columns.values) if len(date_time_keys) == 0: return {} date_time_key = date_time_keys[0] if len(date_time_keys) > 1: full_df.drop(columns=date_time_keys[1:], inplace=True) full_df = full_df.sort_values(by=date_time_key) full_df_numeric = full_df.select_dtypes(include=[np.datetime64, np.number]) file_dict = full_df_numeric.to_dict(orient=\"list\") metric_dict = create_metric_dicts( file_dict, date_time_key, \"Date\", [\"scatter\"] ) return metric_dict","title":"report_file_metrics"},{"location":"reference/data_models/rules/","text":"CanAnnotateDataFileDeployment Bases: R Rule for determining if a user can annotate a data file deployment. Source code in data_models\\rules.py class CanAnnotateDataFileDeployment(R): \"\"\" Rule for determining if a user can annotate a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.annotators.all() def query(self, user): \"\"\" Construct a query for data file deployments user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is an annotator of the data file deployment. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.annotators.all() query(user) Construct a query for data file deployments user can annotate. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__annotators=user) return final_query(accumulated_q) CanAnnotateDeviceContainingDataFile Bases: R Rule for determining if a user can annotate a device containing a data file. Source code in data_models\\rules.py class CanAnnotateDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can annotate a device containing a data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.annotators.all().values_list(\"pk\", flat=True) def query(self, user): \"\"\" Construct a query for devices containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is an annotator of the device containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.annotators.all().values_list(\"pk\", flat=True) query(user) Construct a query for devices containing data files user can annotate. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__annotators=user) return final_query(accumulated_q) CanAnnotateProjectContainingDataFile Bases: R Rule for determining if a user can annotate a project containing a specific data file. Source code in data_models\\rules.py class CanAnnotateProjectContainingDataFile(R): \"\"\" Rule for determining if a user can annotate a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"annotators__pk\", flat=True) def query(self, user): \"\"\" Construct a query for projects containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is an annotator for the project containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"annotators__pk\", flat=True) query(user) Construct a query for projects containing data files user can annotate. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__annotators=user) return final_query(accumulated_q) CanManageDataFileDeployment Bases: R Rule for determining if a user can manage a data file deployment. Source code in data_models\\rules.py class CanManageDataFileDeployment(R): \"\"\" Rule for determining if a user can manage a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.managers.all() def query(self, user): \"\"\" Construct a query for data file deployments user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__managers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is a manager of the data file deployment. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.managers.all() query(user) Construct a query for data file deployments user can manage. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__managers=user) return final_query(accumulated_q) CanManageDeployedDevice Bases: R Rule for determining if a user can manage a deployed device. Source code in data_models\\rules.py class CanManageDeployedDevice(R): \"\"\" Rule for determining if a user can manage a deployed device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can manage the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.managers.all() or user.pk == instance.device.owner def query(self, user): \"\"\" Construct a query for deployed devices user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__managers=user) | Q(device__owner=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can manage the deployed device. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can manage the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.managers.all() or user.pk == instance.device.owner query(user) Construct a query for deployed devices user can manage. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployed devices user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__managers=user) | Q(device__owner=user) return final_query(accumulated_q) CanManageDeviceContainingDataFile Bases: R Rule for determining if a user can manage a device containing a data file. Source code in data_models\\rules.py class CanManageDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can manage a device containing a data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is the owner or a manager of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user == instance.deployment.device.owner or user in instance.deployment.device.managers.all().values_list(\"pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for devices containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__managers=user) | Q( deployment__device__owner=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is the owner or a manager of the device containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is the owner or a manager of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user == instance.deployment.device.owner or user in instance.deployment.device.managers.all().values_list(\"pk\", flat=True) ) query(user) Construct a query for devices containing data files user can manage. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__managers=user) | Q( deployment__device__owner=user) return final_query(accumulated_q) CanManageProjectContainingDataFile Bases: R Rule for determining if a user can manage a project containing a specific data file. Source code in data_models\\rules.py class CanManageProjectContainingDataFile(R): \"\"\" Rule for determining if a user can manage a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.deployment.project.all().values_list(\"managers__pk\", flat=True) or user.pk in instance.deployment.project.all().values_list(\"owner__pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for projects containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__managers=user) | Q( deployment__project__owner=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is a manager or owner of the project containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.deployment.project.all().values_list(\"managers__pk\", flat=True) or user.pk in instance.deployment.project.all().values_list(\"owner__pk\", flat=True) ) query(user) Construct a query for projects containing data files user can manage. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__managers=user) | Q( deployment__project__owner=user) return final_query(accumulated_q) CanManageProjectContainingDeployment Bases: R Rule for determining if a user can manage or own a project containing a deployment. Source code in data_models\\rules.py class CanManageProjectContainingDeployment(R): \"\"\" Rule for determining if a user can manage or own a project containing a deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.all().values_list('managers__pk', flat=True) or user.pk in instance.project.all().values_list('owner__pk', flat=True) ) def query(self, user): \"\"\" Construct a query for deployments user can manage or own. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__managers=user) | Q(project__owner=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is a manager or owner of the deployment's project. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.all().values_list('managers__pk', flat=True) or user.pk in instance.project.all().values_list('owner__pk', flat=True) ) query(user) Construct a query for deployments user can manage or own. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments user can manage or own. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__managers=user) | Q(project__owner=user) return final_query(accumulated_q) CanViewDataFileDeployment Bases: R Rule for determining if a user can view a data file deployment. Source code in data_models\\rules.py class CanViewDataFileDeployment(R): \"\"\" Rule for determining if a user can view a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.viewers.all() def query(self, user): \"\"\" Construct a query for data file deployments user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__viewers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the data file deployment. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.viewers.all() query(user) Construct a query for data file deployments user can view. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__viewers=user) return final_query(accumulated_q) CanViewDeployedDevice Bases: R Rule for determining if a user can view a deployed device. Source code in data_models\\rules.py class CanViewDeployedDevice(R): \"\"\" Rule for determining if a user can view a deployed device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.viewers.all() or user in instance.device.annotators.all() def query(self, user): \"\"\" Construct a query for deployed devices user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__viewers=user) | Q(device__annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the deployed device. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.viewers.all() or user in instance.device.annotators.all() query(user) Construct a query for deployed devices user can view. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployed devices user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__viewers=user) | Q(device__annotators=user) return final_query(accumulated_q) CanViewDeploymentInProject Bases: R Rule for determining if a user can view a deployment within a project. Source code in data_models\\rules.py class CanViewDeploymentInProject(R): \"\"\" Rule for determining if a user can view a deployment within a project. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployment via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.viewers.values_list('pk', flat=True) or user.pk in instance.project.annotators.values_list('pk', flat=True) or user.pk in instance.project.managers.values_list('pk', flat=True) or user.pk == instance.project.owner.pk ) def query(self, user): \"\"\" Construct a query for deployments the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(project__viewers=user) | Q(project__annotators=user) | Q(project__managers=user) | Q(project__owner=user) ) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the deployment via project roles. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployment via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.viewers.values_list('pk', flat=True) or user.pk in instance.project.annotators.values_list('pk', flat=True) or user.pk in instance.project.managers.values_list('pk', flat=True) or user.pk == instance.project.owner.pk ) query(user) Construct a query for deployments the user can view via project roles. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(project__viewers=user) | Q(project__annotators=user) | Q(project__managers=user) | Q(project__owner=user) ) return final_query(accumulated_q) CanViewDeviceContainingDataFile Bases: R Rule for determining if a user can view a device containing a specific data file. Source code in data_models\\rules.py class CanViewDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can view a device containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.viewers.all().values_list(\"pk\", flat=True) def query(self, user): \"\"\" Construct a query for devices containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__viewers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the device containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.viewers.all().values_list(\"pk\", flat=True) query(user) Construct a query for devices containing data files user can view. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__viewers=user) return final_query(accumulated_q) CanViewDeviceInProject Bases: R Rule for determining if a user can view a device within a project. Source code in data_models\\rules.py class CanViewDeviceInProject(R): \"\"\" Rule for determining if a user can view a device within a project. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the device via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( 'deployments__project__viewers__pk', flat=True) or user.pk in instance.values_list('deployments__project__annotators__pk', flat=True) or user.pk in instance.values_list('deployments__project__managers__pk', flat=True) or user.pk in instance.values_list('deployments__project__owner__pk', flat=True) ) def query(self, user): \"\"\" Construct a query for devices the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the device via project roles. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the device via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( 'deployments__project__viewers__pk', flat=True) or user.pk in instance.values_list('deployments__project__annotators__pk', flat=True) or user.pk in instance.values_list('deployments__project__managers__pk', flat=True) or user.pk in instance.values_list('deployments__project__owner__pk', flat=True) ) query(user) Construct a query for devices the user can view via project roles. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q) CanViewProjectContainingDataFile Bases: R Rule for determining if a user can view a project containing a specific data file. Source code in data_models\\rules.py class CanViewProjectContainingDataFile(R): \"\"\" Rule for determining if a user can view a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"viewers__pk\", flat=True) def query(self, user): \"\"\" Construct a query for projects containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__viewers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the project containing the data file. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"viewers__pk\", flat=True) query(user) Construct a query for projects containing data files user can view. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__viewers=user) return final_query(accumulated_q) CanViewProjectContainingDeployment Bases: R Rule for determining if a user can view a project containing a deployment. Source code in data_models\\rules.py class CanViewProjectContainingDeployment(R): \"\"\" Rule for determining if a user can view a project containing a deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.project.all().values_list(\"viewers__pk\", flat=True) def query(self, user): \"\"\" Construct a query for deployments in projects the user can view or annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__viewers=user) | Q(project__annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the deployment's project. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.project.all().values_list(\"viewers__pk\", flat=True) query(user) Construct a query for deployments in projects the user can view or annotate. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments in projects the user can view or annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__viewers=user) | Q(project__annotators=user) return final_query(accumulated_q) CanViewProjectContainingDevice Bases: R Rule for determining if a user can view a project containing a device. Source code in data_models\\rules.py class CanViewProjectContainingDevice(R): \"\"\" Rule for determining if a user can view a project containing a device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the project via device deployments. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( \"deployments__project__viewers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__annotator__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__managers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__owner__pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for devices in projects the user can access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q) check(user, instance=None) Check if the user can view the project via device deployments. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the project via device deployments. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( \"deployments__project__viewers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__annotator__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__managers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__owner__pk\", flat=True) ) query(user) Construct a query for devices in projects the user can access. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices in projects the user can access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q) DataFileHasNoHuman Bases: R Rule for determining if a data file does not contain human-related data. Source code in data_models\\rules.py class DataFileHasNoHuman(R): \"\"\" Rule for determining if a data file does not contain human-related data. \"\"\" def check(self, user, instance=None): \"\"\" Check if the data file does not contain human data. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return not instance.has_human def query(self, user): \"\"\" Construct a query for data files without human data. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(has_human=False) return final_query(accumulated_q) check(user, instance=None) Check if the data file does not contain human data. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the data file does not contain human data. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return not instance.has_human query(user) Construct a query for data files without human data. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data files without human data. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(has_human=False) return final_query(accumulated_q) IsAnnotator Bases: R Rule for checking if a user is an annotator for a given instance. Source code in data_models\\rules.py class IsAnnotator(R): \"\"\" Rule for checking if a user is an annotator for a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is an annotator, False otherwise. \"\"\" return user in instance.annotators.all() def query(self, user): \"\"\" Construct a query to filter instances where user is an annotator. Parameters ---------- user : User Returns ------- Q Query for filtering instances. \"\"\" accumulated_q = Q(annotators=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is an annotator for the instance. Parameters user : User The user to check. instance : Model, optional The instance to check against. Returns bool True if user is an annotator, False otherwise. Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is an annotator, False otherwise. \"\"\" return user in instance.annotators.all() query(user) Construct a query to filter instances where user is an annotator. Parameters user : User Returns Q Query for filtering instances. Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query to filter instances where user is an annotator. Parameters ---------- user : User Returns ------- Q Query for filtering instances. \"\"\" accumulated_q = Q(annotators=user) return final_query(accumulated_q) IsManager Bases: R Rule for checking if a user is a manager of a given instance. Source code in data_models\\rules.py class IsManager(R): \"\"\" Rule for checking if a user is a manager of a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager of the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is a manager, False otherwise. \"\"\" return user in instance.managers.all() def query(self, user): \"\"\" Construct a query to filter instances managed by the user. Parameters ---------- user : User The user to filter by. Returns ------- Q Query for filtering managed instances. \"\"\" accumulated_q = Q(managers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is a manager of the instance. Parameters user : User The user to check. instance : Model, optional The instance to check against. Returns bool True if user is a manager, False otherwise. Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager of the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is a manager, False otherwise. \"\"\" return user in instance.managers.all() query(user) Construct a query to filter instances managed by the user. Parameters user : User The user to filter by. Returns Q Query for filtering managed instances. Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query to filter instances managed by the user. Parameters ---------- user : User The user to filter by. Returns ------- Q Query for filtering managed instances. \"\"\" accumulated_q = Q(managers=user) return final_query(accumulated_q) IsViewer Bases: R Rule for checking if a user is a viewer of a given instance. Source code in data_models\\rules.py class IsViewer(R): \"\"\" Rule for checking if a user is a viewer of a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a viewer for the instance. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.viewers.all() def query(self, user): \"\"\" Construct a query for viewer access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(viewers=user) return final_query(accumulated_q) check(user, instance=None) Check if the user is a viewer for the instance. Parameters user : User instance : Model, optional Returns bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a viewer for the instance. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.viewers.all() query(user) Construct a query for viewer access. Parameters user : User Returns Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for viewer access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(viewers=user) return final_query(accumulated_q)","title":"rules"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment","text":"Bases: R Rule for determining if a user can annotate a data file deployment. Source code in data_models\\rules.py class CanAnnotateDataFileDeployment(R): \"\"\" Rule for determining if a user can annotate a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.annotators.all() def query(self, user): \"\"\" Construct a query for data file deployments user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__annotators=user) return final_query(accumulated_q)","title":"CanAnnotateDataFileDeployment"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.check","text":"Check if the user is an annotator of the data file deployment.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.annotators.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.query","text":"Construct a query for data file deployments user can annotate.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDataFileDeployment.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile","text":"Bases: R Rule for determining if a user can annotate a device containing a data file. Source code in data_models\\rules.py class CanAnnotateDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can annotate a device containing a data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.annotators.all().values_list(\"pk\", flat=True) def query(self, user): \"\"\" Construct a query for devices containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__annotators=user) return final_query(accumulated_q)","title":"CanAnnotateDeviceContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.check","text":"Check if the user is an annotator of the device containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.annotators.all().values_list(\"pk\", flat=True)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.query","text":"Construct a query for devices containing data files user can annotate.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateDeviceContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile","text":"Bases: R Rule for determining if a user can annotate a project containing a specific data file. Source code in data_models\\rules.py class CanAnnotateProjectContainingDataFile(R): \"\"\" Rule for determining if a user can annotate a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"annotators__pk\", flat=True) def query(self, user): \"\"\" Construct a query for projects containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__annotators=user) return final_query(accumulated_q)","title":"CanAnnotateProjectContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.check","text":"Check if the user is an annotator for the project containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"annotators__pk\", flat=True)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.query","text":"Construct a query for projects containing data files user can annotate.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanAnnotateProjectContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment","text":"Bases: R Rule for determining if a user can manage a data file deployment. Source code in data_models\\rules.py class CanManageDataFileDeployment(R): \"\"\" Rule for determining if a user can manage a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.managers.all() def query(self, user): \"\"\" Construct a query for data file deployments user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__managers=user) return final_query(accumulated_q)","title":"CanManageDataFileDeployment"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.check","text":"Check if the user is a manager of the data file deployment.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager of the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.managers.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.query","text":"Construct a query for data file deployments user can manage.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDataFileDeployment.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__managers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice","text":"Bases: R Rule for determining if a user can manage a deployed device. Source code in data_models\\rules.py class CanManageDeployedDevice(R): \"\"\" Rule for determining if a user can manage a deployed device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can manage the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.managers.all() or user.pk == instance.device.owner def query(self, user): \"\"\" Construct a query for deployed devices user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__managers=user) | Q(device__owner=user) return final_query(accumulated_q)","title":"CanManageDeployedDevice"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.check","text":"Check if the user can manage the deployed device.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can manage the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.managers.all() or user.pk == instance.device.owner","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.query","text":"Construct a query for deployed devices user can manage.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeployedDevice.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployed devices user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__managers=user) | Q(device__owner=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile","text":"Bases: R Rule for determining if a user can manage a device containing a data file. Source code in data_models\\rules.py class CanManageDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can manage a device containing a data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is the owner or a manager of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user == instance.deployment.device.owner or user in instance.deployment.device.managers.all().values_list(\"pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for devices containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__managers=user) | Q( deployment__device__owner=user) return final_query(accumulated_q)","title":"CanManageDeviceContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.check","text":"Check if the user is the owner or a manager of the device containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is the owner or a manager of the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user == instance.deployment.device.owner or user in instance.deployment.device.managers.all().values_list(\"pk\", flat=True) )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.query","text":"Construct a query for devices containing data files user can manage.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageDeviceContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__managers=user) | Q( deployment__device__owner=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile","text":"Bases: R Rule for determining if a user can manage a project containing a specific data file. Source code in data_models\\rules.py class CanManageProjectContainingDataFile(R): \"\"\" Rule for determining if a user can manage a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.deployment.project.all().values_list(\"managers__pk\", flat=True) or user.pk in instance.deployment.project.all().values_list(\"owner__pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for projects containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__managers=user) | Q( deployment__project__owner=user) return final_query(accumulated_q)","title":"CanManageProjectContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.check","text":"Check if the user is a manager or owner of the project containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.deployment.project.all().values_list(\"managers__pk\", flat=True) or user.pk in instance.deployment.project.all().values_list(\"owner__pk\", flat=True) )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.query","text":"Construct a query for projects containing data files user can manage.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can manage. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__managers=user) | Q( deployment__project__owner=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment","text":"Bases: R Rule for determining if a user can manage or own a project containing a deployment. Source code in data_models\\rules.py class CanManageProjectContainingDeployment(R): \"\"\" Rule for determining if a user can manage or own a project containing a deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.all().values_list('managers__pk', flat=True) or user.pk in instance.project.all().values_list('owner__pk', flat=True) ) def query(self, user): \"\"\" Construct a query for deployments user can manage or own. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__managers=user) | Q(project__owner=user) return final_query(accumulated_q)","title":"CanManageProjectContainingDeployment"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.check","text":"Check if the user is a manager or owner of the deployment's project.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager or owner of the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.all().values_list('managers__pk', flat=True) or user.pk in instance.project.all().values_list('owner__pk', flat=True) )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.query","text":"Construct a query for deployments user can manage or own.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanManageProjectContainingDeployment.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments user can manage or own. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__managers=user) | Q(project__owner=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment","text":"Bases: R Rule for determining if a user can view a data file deployment. Source code in data_models\\rules.py class CanViewDataFileDeployment(R): \"\"\" Rule for determining if a user can view a data file deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.viewers.all() def query(self, user): \"\"\" Construct a query for data file deployments user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__viewers=user) return final_query(accumulated_q)","title":"CanViewDataFileDeployment"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.check","text":"Check if the user can view the data file deployment.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the data file deployment. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.deployment.viewers.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.query","text":"Construct a query for data file deployments user can view.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDataFileDeployment.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data file deployments user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__viewers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice","text":"Bases: R Rule for determining if a user can view a deployed device. Source code in data_models\\rules.py class CanViewDeployedDevice(R): \"\"\" Rule for determining if a user can view a deployed device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.viewers.all() or user in instance.device.annotators.all() def query(self, user): \"\"\" Construct a query for deployed devices user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__viewers=user) | Q(device__annotators=user) return final_query(accumulated_q)","title":"CanViewDeployedDevice"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.check","text":"Check if the user can view the deployed device.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployed device. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.device.viewers.all() or user in instance.device.annotators.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.query","text":"Construct a query for deployed devices user can view.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeployedDevice.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployed devices user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(device__viewers=user) | Q(device__annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject","text":"Bases: R Rule for determining if a user can view a deployment within a project. Source code in data_models\\rules.py class CanViewDeploymentInProject(R): \"\"\" Rule for determining if a user can view a deployment within a project. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployment via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.viewers.values_list('pk', flat=True) or user.pk in instance.project.annotators.values_list('pk', flat=True) or user.pk in instance.project.managers.values_list('pk', flat=True) or user.pk == instance.project.owner.pk ) def query(self, user): \"\"\" Construct a query for deployments the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(project__viewers=user) | Q(project__annotators=user) | Q(project__managers=user) | Q(project__owner=user) ) return final_query(accumulated_q)","title":"CanViewDeploymentInProject"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.check","text":"Check if the user can view the deployment via project roles.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployment via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.project.viewers.values_list('pk', flat=True) or user.pk in instance.project.annotators.values_list('pk', flat=True) or user.pk in instance.project.managers.values_list('pk', flat=True) or user.pk == instance.project.owner.pk )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.query","text":"Construct a query for deployments the user can view via project roles.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeploymentInProject.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(project__viewers=user) | Q(project__annotators=user) | Q(project__managers=user) | Q(project__owner=user) ) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile","text":"Bases: R Rule for determining if a user can view a device containing a specific data file. Source code in data_models\\rules.py class CanViewDeviceContainingDataFile(R): \"\"\" Rule for determining if a user can view a device containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.viewers.all().values_list(\"pk\", flat=True) def query(self, user): \"\"\" Construct a query for devices containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__viewers=user) return final_query(accumulated_q)","title":"CanViewDeviceContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.check","text":"Check if the user can view the device containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the device containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.device.viewers.all().values_list(\"pk\", flat=True)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.query","text":"Construct a query for devices containing data files user can view.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__device__viewers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject","text":"Bases: R Rule for determining if a user can view a device within a project. Source code in data_models\\rules.py class CanViewDeviceInProject(R): \"\"\" Rule for determining if a user can view a device within a project. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the device via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( 'deployments__project__viewers__pk', flat=True) or user.pk in instance.values_list('deployments__project__annotators__pk', flat=True) or user.pk in instance.values_list('deployments__project__managers__pk', flat=True) or user.pk in instance.values_list('deployments__project__owner__pk', flat=True) ) def query(self, user): \"\"\" Construct a query for devices the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q)","title":"CanViewDeviceInProject"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.check","text":"Check if the user can view the device via project roles.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the device via project roles. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( 'deployments__project__viewers__pk', flat=True) or user.pk in instance.values_list('deployments__project__annotators__pk', flat=True) or user.pk in instance.values_list('deployments__project__managers__pk', flat=True) or user.pk in instance.values_list('deployments__project__owner__pk', flat=True) )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.query","text":"Construct a query for devices the user can view via project roles.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewDeviceInProject.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices the user can view via project roles. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile","text":"Bases: R Rule for determining if a user can view a project containing a specific data file. Source code in data_models\\rules.py class CanViewProjectContainingDataFile(R): \"\"\" Rule for determining if a user can view a project containing a specific data file. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"viewers__pk\", flat=True) def query(self, user): \"\"\" Construct a query for projects containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__viewers=user) return final_query(accumulated_q)","title":"CanViewProjectContainingDataFile"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.check","text":"Check if the user can view the project containing the data file.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the project containing the data file. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.deployment.project.all().values_list(\"viewers__pk\", flat=True)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.query","text":"Construct a query for projects containing data files user can view.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDataFile.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for projects containing data files user can view. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(deployment__project__viewers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment","text":"Bases: R Rule for determining if a user can view a project containing a deployment. Source code in data_models\\rules.py class CanViewProjectContainingDeployment(R): \"\"\" Rule for determining if a user can view a project containing a deployment. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.project.all().values_list(\"viewers__pk\", flat=True) def query(self, user): \"\"\" Construct a query for deployments in projects the user can view or annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__viewers=user) | Q(project__annotators=user) return final_query(accumulated_q)","title":"CanViewProjectContainingDeployment"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.check","text":"Check if the user can view the deployment's project.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the deployment's project. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user.pk in instance.project.all().values_list(\"viewers__pk\", flat=True)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.query","text":"Construct a query for deployments in projects the user can view or annotate.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDeployment.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for deployments in projects the user can view or annotate. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(project__viewers=user) | Q(project__annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice","text":"Bases: R Rule for determining if a user can view a project containing a device. Source code in data_models\\rules.py class CanViewProjectContainingDevice(R): \"\"\" Rule for determining if a user can view a project containing a device. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user can view the project via device deployments. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( \"deployments__project__viewers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__annotator__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__managers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__owner__pk\", flat=True) ) def query(self, user): \"\"\" Construct a query for devices in projects the user can access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q)","title":"CanViewProjectContainingDevice"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.check","text":"Check if the user can view the project via device deployments.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user can view the project via device deployments. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return ( user.pk in instance.values_list( \"deployments__project__viewers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__annotator__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__managers__pk\", flat=True) or user.pk in instance.values_list(\"deployments__project__owner__pk\", flat=True) )","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.query","text":"Construct a query for devices in projects the user can access.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.CanViewProjectContainingDevice.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for devices in projects the user can access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = ( Q(deployments__project__viewers=user) | Q(deployments__project__annotators=user) | Q(deployments__project__managers=user) | Q(deployments__project__owner=user) ) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman","text":"Bases: R Rule for determining if a data file does not contain human-related data. Source code in data_models\\rules.py class DataFileHasNoHuman(R): \"\"\" Rule for determining if a data file does not contain human-related data. \"\"\" def check(self, user, instance=None): \"\"\" Check if the data file does not contain human data. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return not instance.has_human def query(self, user): \"\"\" Construct a query for data files without human data. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(has_human=False) return final_query(accumulated_q)","title":"DataFileHasNoHuman"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.check","text":"Check if the data file does not contain human data.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the data file does not contain human data. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return not instance.has_human","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.query","text":"Construct a query for data files without human data.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.DataFileHasNoHuman.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for data files without human data. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(has_human=False) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator","text":"Bases: R Rule for checking if a user is an annotator for a given instance. Source code in data_models\\rules.py class IsAnnotator(R): \"\"\" Rule for checking if a user is an annotator for a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is an annotator, False otherwise. \"\"\" return user in instance.annotators.all() def query(self, user): \"\"\" Construct a query to filter instances where user is an annotator. Parameters ---------- user : User Returns ------- Q Query for filtering instances. \"\"\" accumulated_q = Q(annotators=user) return final_query(accumulated_q)","title":"IsAnnotator"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.check","text":"Check if the user is an annotator for the instance.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.check--parameters","text":"user : User The user to check. instance : Model, optional The instance to check against.","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.check--returns","text":"bool True if user is an annotator, False otherwise. Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is an annotator for the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is an annotator, False otherwise. \"\"\" return user in instance.annotators.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.query","text":"Construct a query to filter instances where user is an annotator.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsAnnotator.query--returns","text":"Q Query for filtering instances. Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query to filter instances where user is an annotator. Parameters ---------- user : User Returns ------- Q Query for filtering instances. \"\"\" accumulated_q = Q(annotators=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsManager","text":"Bases: R Rule for checking if a user is a manager of a given instance. Source code in data_models\\rules.py class IsManager(R): \"\"\" Rule for checking if a user is a manager of a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a manager of the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is a manager, False otherwise. \"\"\" return user in instance.managers.all() def query(self, user): \"\"\" Construct a query to filter instances managed by the user. Parameters ---------- user : User The user to filter by. Returns ------- Q Query for filtering managed instances. \"\"\" accumulated_q = Q(managers=user) return final_query(accumulated_q)","title":"IsManager"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.check","text":"Check if the user is a manager of the instance.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.check--parameters","text":"user : User The user to check. instance : Model, optional The instance to check against.","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.check--returns","text":"bool True if user is a manager, False otherwise. Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a manager of the instance. Parameters ---------- user : User The user to check. instance : Model, optional The instance to check against. Returns ------- bool True if user is a manager, False otherwise. \"\"\" return user in instance.managers.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.query","text":"Construct a query to filter instances managed by the user.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.query--parameters","text":"user : User The user to filter by.","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsManager.query--returns","text":"Q Query for filtering managed instances. Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query to filter instances managed by the user. Parameters ---------- user : User The user to filter by. Returns ------- Q Query for filtering managed instances. \"\"\" accumulated_q = Q(managers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer","text":"Bases: R Rule for checking if a user is a viewer of a given instance. Source code in data_models\\rules.py class IsViewer(R): \"\"\" Rule for checking if a user is a viewer of a given instance. \"\"\" def check(self, user, instance=None): \"\"\" Check if the user is a viewer for the instance. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.viewers.all() def query(self, user): \"\"\" Construct a query for viewer access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(viewers=user) return final_query(accumulated_q)","title":"IsViewer"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.check","text":"Check if the user is a viewer for the instance.","title":"check"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.check--parameters","text":"user : User instance : Model, optional","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.check--returns","text":"bool Source code in data_models\\rules.py def check(self, user, instance=None): \"\"\" Check if the user is a viewer for the instance. Parameters ---------- user : User instance : Model, optional Returns ------- bool \"\"\" return user in instance.viewers.all()","title":"Returns"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.query","text":"Construct a query for viewer access.","title":"query"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.query--parameters","text":"user : User","title":"Parameters"},{"location":"reference/data_models/rules/#data_models.rules.IsViewer.query--returns","text":"Q Source code in data_models\\rules.py def query(self, user): \"\"\" Construct a query for viewer access. Parameters ---------- user : User Returns ------- Q \"\"\" accumulated_q = Q(viewers=user) return final_query(accumulated_q)","title":"Returns"},{"location":"reference/data_models/serializers/","text":"DataFileCheckSerializer Bases: Serializer Serializer for checking if files exist in the system. Source code in data_models\\serializers.py class DataFileCheckSerializer(serializers.Serializer): \"\"\" Serializer for checking if files exist in the system. \"\"\" file_names = serializers.ListField( child=serializers.CharField(), required=False ) original_names = serializers.ListField( child=serializers.CharField(), required=False ) DataFileSerializer Bases: CreatedModifiedMixIn , ModelSerializer Serializer for DataFile model, including deployment, file type, and user-specific flags. Source code in data_models\\serializers.py class DataFileSerializer(CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for DataFile model, including deployment, file type, and user-specific flags. \"\"\" deployment = serializers.SlugRelatedField( slug_field='deployment_device_ID', queryset=Deployment.objects.all(), required=False) deployment_ID = serializers.PrimaryKeyRelatedField(source=\"deployment\", queryset=Deployment.objects.all(), required=False) file_type = serializers.StringRelatedField() recording_dt = serializers.DateTimeField(default_timezone=djtimezone.utc) def to_representation(self, instance): \"\"\" Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. \"\"\" initial_rep = super(DataFileSerializer, self).to_representation(instance) if self.context.get('request'): request_user = self.context['request'].user initial_rep[\"favourite\"] = instance.favourite_of.all().filter( pk=request_user.pk).exists() initial_rep.pop('path') initial_rep[\"can_annotate\"] = perms['data_models.annotate_datafile'].check( request_user, instance) return initial_rep class Meta: model = DataFile exclude = [\"do_not_remove\", \"local_path\", \"favourite_of\", \"tar_file\"] def validate(self, data): \"\"\" Validate DataFile, ensuring recording date and deployment are consistent. \"\"\" data = super().validate(data) result, message = validators.data_file_in_deployment( data.get('recording_dt', self.instance.recording_dt), data.get('deployment', self.instance.deployment)) if not result: raise serializers.ValidationError(message) return data to_representation(instance) Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. \"\"\" initial_rep = super(DataFileSerializer, self).to_representation(instance) if self.context.get('request'): request_user = self.context['request'].user initial_rep[\"favourite\"] = instance.favourite_of.all().filter( pk=request_user.pk).exists() initial_rep.pop('path') initial_rep[\"can_annotate\"] = perms['data_models.annotate_datafile'].check( request_user, instance) return initial_rep validate(data) Validate DataFile, ensuring recording date and deployment are consistent. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate DataFile, ensuring recording date and deployment are consistent. \"\"\" data = super().validate(data) result, message = validators.data_file_in_deployment( data.get('recording_dt', self.instance.recording_dt), data.get('deployment', self.instance.deployment)) if not result: raise serializers.ValidationError(message) return data DataFileUploadSerializer Bases: Serializer Serializer for uploading data files with associated metadata and validation. Source code in data_models\\serializers.py class DataFileUploadSerializer(serializers.Serializer): \"\"\" Serializer for uploading data files with associated metadata and validation. \"\"\" device = serializers.CharField(required=False) device_ID = serializers.IntegerField(required=False) deployment = serializers.CharField(required=False) deployment_ID = serializers.IntegerField(required=False) files = serializers.ListField(child=serializers.FileField( allow_empty_file=False, max_length=None), required=True) file_names = serializers.ListField(child=serializers.CharField( ), required=False) extra_data = serializers.ListField( child=serializers.JSONField(binary=True), required=False) recording_dt = serializers.ListField( child=serializers.DateTimeField(), required=False) autoupdate = serializers.BooleanField(default=False) rename = serializers.BooleanField(default=True) check_filename = serializers.BooleanField(default=True) data_types = serializers.ListField(child=serializers.SlugRelatedField(slug_field='name', queryset=DataType.objects.all()), required=False) is_active = serializers.BooleanField( source=\"deployment.is_active\", read_only=True) def create(self, validated_data): \"\"\" Return validated upload data unchanged. \"\"\" return validated_data def validate(self, data): \"\"\" Validate upload data for existence, deployment/device, and recording_dt. \"\"\" data = super().validate(data) deployment = data.get('deployment') deployment_ID = data.get('deployment_ID') device = data.get('device') device_ID = data.get('device_ID') if deployment is None and deployment_ID is None and device is None and device_ID is None: raise serializers.ValidationError( \"A deployment or a device must be supplied\") if deployment or deployment_ID: try: deployment_object = Deployment.objects.get(Q(Q(deployment_device_ID=deployment) | Q(pk=deployment_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"deployment\": f\"Deployment {deployment} does not exist\", \"deployment_ID\": f\"Deployment ID {deployment_ID} does not exist\"}) data['deployment_object'] = deployment_object data['device_object'] = deployment_object.device elif device or device_ID: try: device_object = Device.objects.get( Q(Q(device_ID=device) | Q(pk=device_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"device\": f\"Device {device} does not exist\", \"device_ID\": f\"Device ID {device_ID} does not exist\"}) data['device_object'] = device_object files = data.get(\"files\") recording_dt = data.get('recording_dt') if files: if recording_dt is not None: if len(recording_dt) > 1 and len(recording_dt) != len(data.get('files')): raise serializers.ValidationError( {\"recording_dt\": \"More than one recording_dt was supplied, but the number does not match the number of files\"}) return data def update(self, validated_data): \"\"\" Not used. \"\"\" pass create(validated_data) Return validated upload data unchanged. Source code in data_models\\serializers.py def create(self, validated_data): \"\"\" Return validated upload data unchanged. \"\"\" return validated_data update(validated_data) Not used. Source code in data_models\\serializers.py def update(self, validated_data): \"\"\" Not used. \"\"\" pass validate(data) Validate upload data for existence, deployment/device, and recording_dt. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate upload data for existence, deployment/device, and recording_dt. \"\"\" data = super().validate(data) deployment = data.get('deployment') deployment_ID = data.get('deployment_ID') device = data.get('device') device_ID = data.get('device_ID') if deployment is None and deployment_ID is None and device is None and device_ID is None: raise serializers.ValidationError( \"A deployment or a device must be supplied\") if deployment or deployment_ID: try: deployment_object = Deployment.objects.get(Q(Q(deployment_device_ID=deployment) | Q(pk=deployment_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"deployment\": f\"Deployment {deployment} does not exist\", \"deployment_ID\": f\"Deployment ID {deployment_ID} does not exist\"}) data['deployment_object'] = deployment_object data['device_object'] = deployment_object.device elif device or device_ID: try: device_object = Device.objects.get( Q(Q(device_ID=device) | Q(pk=device_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"device\": f\"Device {device} does not exist\", \"device_ID\": f\"Device ID {device_ID} does not exist\"}) data['device_object'] = device_object files = data.get(\"files\") recording_dt = data.get('recording_dt') if files: if recording_dt is not None: if len(recording_dt) > 1 and len(recording_dt) != len(data.get('files')): raise serializers.ValidationError( {\"recording_dt\": \"More than one recording_dt was supplied, but the number does not match the number of files\"}) return data DataTypeSerializer Bases: ModelSerializer Serializer for DataType model (all fields). Source code in data_models\\serializers.py class DataTypeSerializer(serializers.ModelSerializer): \"\"\" Serializer for DataType model (all fields). \"\"\" class Meta: model = DataType fields = '__all__' DeploymentFieldsMixIn Bases: InstanceGetMixIn , OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Mixin serializer for deployment-related fields and logic. Provides device, project, site, and timezone relationships, as well as custom validation for deployments, permission checks, and GeoJSON output. Source code in data_models\\serializers.py class DeploymentFieldsMixIn(InstanceGetMixIn, OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Mixin serializer for deployment-related fields and logic. Provides device, project, site, and timezone relationships, as well as custom validation for deployments, permission checks, and GeoJSON output. \"\"\" device_type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False, allow_null=True) device_type_ID = serializers.PrimaryKeyRelatedField(source=\"device_type\", queryset=DataType.objects.all(), required=False, allow_null=True) device = serializers.SlugRelatedField( slug_field='device_ID', queryset=Device.objects.all(), required=False) device_ID = serializers.PrimaryKeyRelatedField(source=\"device\", queryset=Device.objects.all(), required=False) project = serializers.SlugRelatedField(many=True, slug_field='project_ID', queryset=Project.objects.all().exclude(name=settings.GLOBAL_PROJECT_ID), allow_null=True, required=False) project_ID = serializers.PrimaryKeyRelatedField(source=\"project\", many=True, queryset=Project.objects.all().exclude(name=settings.GLOBAL_PROJECT_ID), required=False, allow_null=True) site = SlugRelatedGetOrCreateField(slug_field='name', queryset=Site.objects.all(), required=False, allow_null=True) site_ID = serializers.PrimaryKeyRelatedField(source=\"site\", queryset=Site.objects.all(), required=False, allow_null=True) time_zone = TimeZoneSerializerField( use_pytz=True, default=settings.TIME_ZONE) deployment_start = serializers.DateTimeField( default=djtimezone.now(), default_timezone=djtimezone.utc) deployment_end = serializers.DateTimeField( default_timezone=djtimezone.utc, required=False, allow_null=True) def to_representation(self, instance): \"\"\" Customize serialized output for deployments, including GeoJSON and permissions. \"\"\" initial_rep = super(DeploymentFieldsMixIn, self).to_representation(instance) if initial_rep.get('properties') is not None: geojson_rep = initial_rep initial_rep = initial_rep.get('properties') else: geojson_rep = None device_model = instance.device.model device_type = instance.device_type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol projects_no_global = [(x, y) for x, y in zip( initial_rep[\"project\"], initial_rep[\"project_ID\"]) if x != settings.GLOBAL_PROJECT_ID] initial_rep[\"project\"], initial_rep[\"project_ID\"] = zip( *projects_no_global) if projects_no_global else ([], []) if geojson_rep is not None: geojson_rep['properties'] = initial_rep return initial_rep class Meta: model = Deployment exclude = ['last_image'] def __init__(self, *args, **kwargs): \"\"\" Initialize DeploymentFieldsMixIn, set management permission and flags. \"\"\" self.clear_project = False self.management_perm = 'data_models.change_deployment' super(DeploymentFieldsMixIn, self).__init__(*args, **kwargs) def create(self, *args, **kwargs): \"\"\" Create and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).create(*args, **kwargs) instance.save() return instance def update(self, *args, **kwargs): \"\"\" Update and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).update(*args, **kwargs) instance.save() return instance def validate(self, data): \"\"\" Validate deployment data for required relationships and constraints. Raises ValidationError on failure. \"\"\" if self.form_submission & (data.get('project') is None): data['project'] = [] data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'device', 'device_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message, data = check_two_keys( 'site', 'site_ID', data, Site, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_type(self.instance_get('device_type', data), self.instance_get('device', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_start_time_after_end_time(self.instance_get('deployment_start', data), self.instance_get('deployment_end', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_overlap(self.instance_get('deployment_start', data), self.instance_get( 'deployment_end', data), self.instance_get( 'device', data), self.instance_get('id', data)) if not result: raise serializers.ValidationError(message) return data __init__(*args, **kwargs) Initialize DeploymentFieldsMixIn, set management permission and flags. Source code in data_models\\serializers.py def __init__(self, *args, **kwargs): \"\"\" Initialize DeploymentFieldsMixIn, set management permission and flags. \"\"\" self.clear_project = False self.management_perm = 'data_models.change_deployment' super(DeploymentFieldsMixIn, self).__init__(*args, **kwargs) create(*args, **kwargs) Create and persist a deployment instance. Source code in data_models\\serializers.py def create(self, *args, **kwargs): \"\"\" Create and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).create(*args, **kwargs) instance.save() return instance to_representation(instance) Customize serialized output for deployments, including GeoJSON and permissions. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Customize serialized output for deployments, including GeoJSON and permissions. \"\"\" initial_rep = super(DeploymentFieldsMixIn, self).to_representation(instance) if initial_rep.get('properties') is not None: geojson_rep = initial_rep initial_rep = initial_rep.get('properties') else: geojson_rep = None device_model = instance.device.model device_type = instance.device_type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol projects_no_global = [(x, y) for x, y in zip( initial_rep[\"project\"], initial_rep[\"project_ID\"]) if x != settings.GLOBAL_PROJECT_ID] initial_rep[\"project\"], initial_rep[\"project_ID\"] = zip( *projects_no_global) if projects_no_global else ([], []) if geojson_rep is not None: geojson_rep['properties'] = initial_rep return initial_rep update(*args, **kwargs) Update and persist a deployment instance. Source code in data_models\\serializers.py def update(self, *args, **kwargs): \"\"\" Update and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).update(*args, **kwargs) instance.save() return instance validate(data) Validate deployment data for required relationships and constraints. Raises ValidationError on failure. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate deployment data for required relationships and constraints. Raises ValidationError on failure. \"\"\" if self.form_submission & (data.get('project') is None): data['project'] = [] data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'device', 'device_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message, data = check_two_keys( 'site', 'site_ID', data, Site, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_type(self.instance_get('device_type', data), self.instance_get('device', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_start_time_after_end_time(self.instance_get('deployment_start', data), self.instance_get('deployment_end', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_overlap(self.instance_get('deployment_start', data), self.instance_get( 'deployment_end', data), self.instance_get( 'device', data), self.instance_get('id', data)) if not result: raise serializers.ValidationError(message) return data DeploymentSerializer Bases: DeploymentFieldsMixIn , ModelSerializer Serializer for Deployment model for regular (non-GeoJSON) API endpoints. Source code in data_models\\serializers.py class DeploymentSerializer(DeploymentFieldsMixIn, serializers.ModelSerializer): \"\"\" Serializer for Deployment model for regular (non-GeoJSON) API endpoints. \"\"\" class Meta: model = Deployment exclude = DeploymentFieldsMixIn.Meta.exclude + ['point'] DeploymentSerializer_GeoJSON Bases: DeploymentFieldsMixIn , GeoFeatureModelSerializer Serializer for Deployment model in GeoJSON format. Uses the 'point' field as the geographic representation. Source code in data_models\\serializers.py class DeploymentSerializer_GeoJSON(DeploymentFieldsMixIn, geoserializers.GeoFeatureModelSerializer): \"\"\" Serializer for Deployment model in GeoJSON format. Uses the 'point' field as the geographic representation. \"\"\" def __init__(self, *args, **kwargs): self.Meta.geo_field = \"point\" super(DeploymentSerializer_GeoJSON, self).__init__(*args, **kwargs) DeviceModelSerializer Bases: CreatedModifiedMixIn , OwnerMixIn , ModelSerializer Serializer for DeviceModel with type fields, handler info, and ownership/timestamps. Source code in data_models\\serializers.py class DeviceModelSerializer(CreatedModifiedMixIn, OwnerMixIn, serializers.ModelSerializer): \"\"\" Serializer for DeviceModel with type fields, handler info, and ownership/timestamps. \"\"\" type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False) type_ID = serializers.PrimaryKeyRelatedField(source=\"type\", queryset=DataType.objects.all(), required=False) class Meta: model = DeviceModel exclude = [] def to_representation(self, instance): \"\"\" Add data handler info to serialized DeviceModel if available. \"\"\" initial_rep = super(DeviceModelSerializer, self).to_representation(instance) handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep to_representation(instance) Add data handler info to serialized DeviceModel if available. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Add data handler info to serialized DeviceModel if available. \"\"\" initial_rep = super(DeviceModelSerializer, self).to_representation(instance) handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep DeviceSerializer Bases: OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Serializer for Device model with type/model, user fields, and permission-based filtering. Source code in data_models\\serializers.py class DeviceSerializer(OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Serializer for Device model with type/model, user fields, and permission-based filtering. \"\"\" type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False) type_ID = serializers.PrimaryKeyRelatedField(source=\"type\", queryset=DataType.objects.all(), required=False) model = serializers.SlugRelatedField( slug_field='name', queryset=DeviceModel.objects.all(), required=False) model_ID = serializers.PrimaryKeyRelatedField(source=\"model\", queryset=DeviceModel.objects.all(), required=False) username = serializers.CharField(required=False) password = serializers.CharField(required=False, write_only=True) is_active = serializers.BooleanField(read_only=True) class Meta: model = Device exclude = [] def __init__(self, *args, **kwargs): self.management_perm = 'data_models.change_device' super(DeviceSerializer, self).__init__(*args, **kwargs) def to_representation(self, instance): \"\"\" Remove sensitive fields, filter by permissions, and add handler/color/symbol info. \"\"\" initial_rep = super(DeviceSerializer, self).to_representation(instance) fields_to_pop = [\"username\"] fields_to_always_pop = [\"password\"] [initial_rep.pop(field, '') for field in fields_to_always_pop] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] device_model = instance.model device_type = instance.type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.model.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep def validate(self, data): \"\"\" Validate device data, ensuring a model is attached and fields are consistent. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'model', 'model_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) return data to_representation(instance) Remove sensitive fields, filter by permissions, and add handler/color/symbol info. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Remove sensitive fields, filter by permissions, and add handler/color/symbol info. \"\"\" initial_rep = super(DeviceSerializer, self).to_representation(instance) fields_to_pop = [\"username\"] fields_to_always_pop = [\"password\"] [initial_rep.pop(field, '') for field in fields_to_always_pop] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] device_model = instance.model device_type = instance.type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.model.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep validate(data) Validate device data, ensuring a model is attached and fields are consistent. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate device data, ensuring a model is attached and fields are consistent. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'model', 'model_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) return data GenericJobSerializer Bases: Serializer Serializer for displaying generic jobs. Attributes: id ( int ) \u2013 Numeric job ID. name ( str ) \u2013 Name of the job. task_name ( str ) \u2013 Celery task name. data_type ( str ) \u2013 Expected data type. admin_only ( bool ) \u2013 If superuser is required. max_items ( int ) \u2013 Max items for the job. default_args ( dict ) \u2013 Default arguments as JSON. Source code in data_models\\serializers.py class GenericJobSerializer(serializers.Serializer): \"\"\" Serializer for displaying generic jobs. Attributes: id (int): Numeric job ID. name (str): Name of the job. task_name (str): Celery task name. data_type (str): Expected data type. admin_only (bool): If superuser is required. max_items (int): Max items for the job. default_args (dict): Default arguments as JSON. \"\"\" id = serializers.IntegerField() name = serializers.CharField() task_name = serializers.CharField() data_type = serializers.CharField() admin_only = serializers.BooleanField() max_items = serializers.IntegerField() default_args = serializers.JSONField() ProjectSerializer Bases: OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , ModelSerializer Serializer for Project model with ownership, management, and timestamps. Source code in data_models\\serializers.py class ProjectSerializer(OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for Project model with ownership, management, and timestamps. \"\"\" is_active = serializers.BooleanField(read_only=True) class Meta: model = Project exclude = [\"data_storages\", \"automated_tasks\", \"archive\"] def __init__(self, *args, **kwargs): self.management_perm = 'data_models.change_project' super(ProjectSerializer, self).__init__(*args, **kwargs) SiteSerializer Bases: ModelSerializer Serializer for Site model (all fields). Source code in data_models\\serializers.py class SiteSerializer(serializers.ModelSerializer): \"\"\" Serializer for Site model (all fields). \"\"\" class Meta: model = Site fields = '__all__'","title":"serializers"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileCheckSerializer","text":"Bases: Serializer Serializer for checking if files exist in the system. Source code in data_models\\serializers.py class DataFileCheckSerializer(serializers.Serializer): \"\"\" Serializer for checking if files exist in the system. \"\"\" file_names = serializers.ListField( child=serializers.CharField(), required=False ) original_names = serializers.ListField( child=serializers.CharField(), required=False )","title":"DataFileCheckSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileSerializer","text":"Bases: CreatedModifiedMixIn , ModelSerializer Serializer for DataFile model, including deployment, file type, and user-specific flags. Source code in data_models\\serializers.py class DataFileSerializer(CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for DataFile model, including deployment, file type, and user-specific flags. \"\"\" deployment = serializers.SlugRelatedField( slug_field='deployment_device_ID', queryset=Deployment.objects.all(), required=False) deployment_ID = serializers.PrimaryKeyRelatedField(source=\"deployment\", queryset=Deployment.objects.all(), required=False) file_type = serializers.StringRelatedField() recording_dt = serializers.DateTimeField(default_timezone=djtimezone.utc) def to_representation(self, instance): \"\"\" Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. \"\"\" initial_rep = super(DataFileSerializer, self).to_representation(instance) if self.context.get('request'): request_user = self.context['request'].user initial_rep[\"favourite\"] = instance.favourite_of.all().filter( pk=request_user.pk).exists() initial_rep.pop('path') initial_rep[\"can_annotate\"] = perms['data_models.annotate_datafile'].check( request_user, instance) return initial_rep class Meta: model = DataFile exclude = [\"do_not_remove\", \"local_path\", \"favourite_of\", \"tar_file\"] def validate(self, data): \"\"\" Validate DataFile, ensuring recording date and deployment are consistent. \"\"\" data = super().validate(data) result, message = validators.data_file_in_deployment( data.get('recording_dt', self.instance.recording_dt), data.get('deployment', self.instance.deployment)) if not result: raise serializers.ValidationError(message) return data","title":"DataFileSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileSerializer.to_representation","text":"Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Add 'favourite' and 'can_annotate' to serialized DataFile based on request user. \"\"\" initial_rep = super(DataFileSerializer, self).to_representation(instance) if self.context.get('request'): request_user = self.context['request'].user initial_rep[\"favourite\"] = instance.favourite_of.all().filter( pk=request_user.pk).exists() initial_rep.pop('path') initial_rep[\"can_annotate\"] = perms['data_models.annotate_datafile'].check( request_user, instance) return initial_rep","title":"to_representation"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileSerializer.validate","text":"Validate DataFile, ensuring recording date and deployment are consistent. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate DataFile, ensuring recording date and deployment are consistent. \"\"\" data = super().validate(data) result, message = validators.data_file_in_deployment( data.get('recording_dt', self.instance.recording_dt), data.get('deployment', self.instance.deployment)) if not result: raise serializers.ValidationError(message) return data","title":"validate"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileUploadSerializer","text":"Bases: Serializer Serializer for uploading data files with associated metadata and validation. Source code in data_models\\serializers.py class DataFileUploadSerializer(serializers.Serializer): \"\"\" Serializer for uploading data files with associated metadata and validation. \"\"\" device = serializers.CharField(required=False) device_ID = serializers.IntegerField(required=False) deployment = serializers.CharField(required=False) deployment_ID = serializers.IntegerField(required=False) files = serializers.ListField(child=serializers.FileField( allow_empty_file=False, max_length=None), required=True) file_names = serializers.ListField(child=serializers.CharField( ), required=False) extra_data = serializers.ListField( child=serializers.JSONField(binary=True), required=False) recording_dt = serializers.ListField( child=serializers.DateTimeField(), required=False) autoupdate = serializers.BooleanField(default=False) rename = serializers.BooleanField(default=True) check_filename = serializers.BooleanField(default=True) data_types = serializers.ListField(child=serializers.SlugRelatedField(slug_field='name', queryset=DataType.objects.all()), required=False) is_active = serializers.BooleanField( source=\"deployment.is_active\", read_only=True) def create(self, validated_data): \"\"\" Return validated upload data unchanged. \"\"\" return validated_data def validate(self, data): \"\"\" Validate upload data for existence, deployment/device, and recording_dt. \"\"\" data = super().validate(data) deployment = data.get('deployment') deployment_ID = data.get('deployment_ID') device = data.get('device') device_ID = data.get('device_ID') if deployment is None and deployment_ID is None and device is None and device_ID is None: raise serializers.ValidationError( \"A deployment or a device must be supplied\") if deployment or deployment_ID: try: deployment_object = Deployment.objects.get(Q(Q(deployment_device_ID=deployment) | Q(pk=deployment_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"deployment\": f\"Deployment {deployment} does not exist\", \"deployment_ID\": f\"Deployment ID {deployment_ID} does not exist\"}) data['deployment_object'] = deployment_object data['device_object'] = deployment_object.device elif device or device_ID: try: device_object = Device.objects.get( Q(Q(device_ID=device) | Q(pk=device_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"device\": f\"Device {device} does not exist\", \"device_ID\": f\"Device ID {device_ID} does not exist\"}) data['device_object'] = device_object files = data.get(\"files\") recording_dt = data.get('recording_dt') if files: if recording_dt is not None: if len(recording_dt) > 1 and len(recording_dt) != len(data.get('files')): raise serializers.ValidationError( {\"recording_dt\": \"More than one recording_dt was supplied, but the number does not match the number of files\"}) return data def update(self, validated_data): \"\"\" Not used. \"\"\" pass","title":"DataFileUploadSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileUploadSerializer.create","text":"Return validated upload data unchanged. Source code in data_models\\serializers.py def create(self, validated_data): \"\"\" Return validated upload data unchanged. \"\"\" return validated_data","title":"create"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileUploadSerializer.update","text":"Not used. Source code in data_models\\serializers.py def update(self, validated_data): \"\"\" Not used. \"\"\" pass","title":"update"},{"location":"reference/data_models/serializers/#data_models.serializers.DataFileUploadSerializer.validate","text":"Validate upload data for existence, deployment/device, and recording_dt. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate upload data for existence, deployment/device, and recording_dt. \"\"\" data = super().validate(data) deployment = data.get('deployment') deployment_ID = data.get('deployment_ID') device = data.get('device') device_ID = data.get('device_ID') if deployment is None and deployment_ID is None and device is None and device_ID is None: raise serializers.ValidationError( \"A deployment or a device must be supplied\") if deployment or deployment_ID: try: deployment_object = Deployment.objects.get(Q(Q(deployment_device_ID=deployment) | Q(pk=deployment_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"deployment\": f\"Deployment {deployment} does not exist\", \"deployment_ID\": f\"Deployment ID {deployment_ID} does not exist\"}) data['deployment_object'] = deployment_object data['device_object'] = deployment_object.device elif device or device_ID: try: device_object = Device.objects.get( Q(Q(device_ID=device) | Q(pk=device_ID))) except ObjectDoesNotExist: raise serializers.ValidationError({\"device\": f\"Device {device} does not exist\", \"device_ID\": f\"Device ID {device_ID} does not exist\"}) data['device_object'] = device_object files = data.get(\"files\") recording_dt = data.get('recording_dt') if files: if recording_dt is not None: if len(recording_dt) > 1 and len(recording_dt) != len(data.get('files')): raise serializers.ValidationError( {\"recording_dt\": \"More than one recording_dt was supplied, but the number does not match the number of files\"}) return data","title":"validate"},{"location":"reference/data_models/serializers/#data_models.serializers.DataTypeSerializer","text":"Bases: ModelSerializer Serializer for DataType model (all fields). Source code in data_models\\serializers.py class DataTypeSerializer(serializers.ModelSerializer): \"\"\" Serializer for DataType model (all fields). \"\"\" class Meta: model = DataType fields = '__all__'","title":"DataTypeSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn","text":"Bases: InstanceGetMixIn , OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Mixin serializer for deployment-related fields and logic. Provides device, project, site, and timezone relationships, as well as custom validation for deployments, permission checks, and GeoJSON output. Source code in data_models\\serializers.py class DeploymentFieldsMixIn(InstanceGetMixIn, OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Mixin serializer for deployment-related fields and logic. Provides device, project, site, and timezone relationships, as well as custom validation for deployments, permission checks, and GeoJSON output. \"\"\" device_type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False, allow_null=True) device_type_ID = serializers.PrimaryKeyRelatedField(source=\"device_type\", queryset=DataType.objects.all(), required=False, allow_null=True) device = serializers.SlugRelatedField( slug_field='device_ID', queryset=Device.objects.all(), required=False) device_ID = serializers.PrimaryKeyRelatedField(source=\"device\", queryset=Device.objects.all(), required=False) project = serializers.SlugRelatedField(many=True, slug_field='project_ID', queryset=Project.objects.all().exclude(name=settings.GLOBAL_PROJECT_ID), allow_null=True, required=False) project_ID = serializers.PrimaryKeyRelatedField(source=\"project\", many=True, queryset=Project.objects.all().exclude(name=settings.GLOBAL_PROJECT_ID), required=False, allow_null=True) site = SlugRelatedGetOrCreateField(slug_field='name', queryset=Site.objects.all(), required=False, allow_null=True) site_ID = serializers.PrimaryKeyRelatedField(source=\"site\", queryset=Site.objects.all(), required=False, allow_null=True) time_zone = TimeZoneSerializerField( use_pytz=True, default=settings.TIME_ZONE) deployment_start = serializers.DateTimeField( default=djtimezone.now(), default_timezone=djtimezone.utc) deployment_end = serializers.DateTimeField( default_timezone=djtimezone.utc, required=False, allow_null=True) def to_representation(self, instance): \"\"\" Customize serialized output for deployments, including GeoJSON and permissions. \"\"\" initial_rep = super(DeploymentFieldsMixIn, self).to_representation(instance) if initial_rep.get('properties') is not None: geojson_rep = initial_rep initial_rep = initial_rep.get('properties') else: geojson_rep = None device_model = instance.device.model device_type = instance.device_type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol projects_no_global = [(x, y) for x, y in zip( initial_rep[\"project\"], initial_rep[\"project_ID\"]) if x != settings.GLOBAL_PROJECT_ID] initial_rep[\"project\"], initial_rep[\"project_ID\"] = zip( *projects_no_global) if projects_no_global else ([], []) if geojson_rep is not None: geojson_rep['properties'] = initial_rep return initial_rep class Meta: model = Deployment exclude = ['last_image'] def __init__(self, *args, **kwargs): \"\"\" Initialize DeploymentFieldsMixIn, set management permission and flags. \"\"\" self.clear_project = False self.management_perm = 'data_models.change_deployment' super(DeploymentFieldsMixIn, self).__init__(*args, **kwargs) def create(self, *args, **kwargs): \"\"\" Create and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).create(*args, **kwargs) instance.save() return instance def update(self, *args, **kwargs): \"\"\" Update and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).update(*args, **kwargs) instance.save() return instance def validate(self, data): \"\"\" Validate deployment data for required relationships and constraints. Raises ValidationError on failure. \"\"\" if self.form_submission & (data.get('project') is None): data['project'] = [] data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'device', 'device_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message, data = check_two_keys( 'site', 'site_ID', data, Site, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_type(self.instance_get('device_type', data), self.instance_get('device', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_start_time_after_end_time(self.instance_get('deployment_start', data), self.instance_get('deployment_end', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_overlap(self.instance_get('deployment_start', data), self.instance_get( 'deployment_end', data), self.instance_get( 'device', data), self.instance_get('id', data)) if not result: raise serializers.ValidationError(message) return data","title":"DeploymentFieldsMixIn"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn.__init__","text":"Initialize DeploymentFieldsMixIn, set management permission and flags. Source code in data_models\\serializers.py def __init__(self, *args, **kwargs): \"\"\" Initialize DeploymentFieldsMixIn, set management permission and flags. \"\"\" self.clear_project = False self.management_perm = 'data_models.change_deployment' super(DeploymentFieldsMixIn, self).__init__(*args, **kwargs)","title":"__init__"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn.create","text":"Create and persist a deployment instance. Source code in data_models\\serializers.py def create(self, *args, **kwargs): \"\"\" Create and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).create(*args, **kwargs) instance.save() return instance","title":"create"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn.to_representation","text":"Customize serialized output for deployments, including GeoJSON and permissions. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Customize serialized output for deployments, including GeoJSON and permissions. \"\"\" initial_rep = super(DeploymentFieldsMixIn, self).to_representation(instance) if initial_rep.get('properties') is not None: geojson_rep = initial_rep initial_rep = initial_rep.get('properties') else: geojson_rep = None device_model = instance.device.model device_type = instance.device_type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol projects_no_global = [(x, y) for x, y in zip( initial_rep[\"project\"], initial_rep[\"project_ID\"]) if x != settings.GLOBAL_PROJECT_ID] initial_rep[\"project\"], initial_rep[\"project_ID\"] = zip( *projects_no_global) if projects_no_global else ([], []) if geojson_rep is not None: geojson_rep['properties'] = initial_rep return initial_rep","title":"to_representation"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn.update","text":"Update and persist a deployment instance. Source code in data_models\\serializers.py def update(self, *args, **kwargs): \"\"\" Update and persist a deployment instance. \"\"\" instance = super(DeploymentFieldsMixIn, self).update(*args, **kwargs) instance.save() return instance","title":"update"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentFieldsMixIn.validate","text":"Validate deployment data for required relationships and constraints. Raises ValidationError on failure. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate deployment data for required relationships and constraints. Raises ValidationError on failure. \"\"\" if self.form_submission & (data.get('project') is None): data['project'] = [] data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'device', 'device_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message, data = check_two_keys( 'site', 'site_ID', data, Site, self.form_submission ) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_type(self.instance_get('device_type', data), self.instance_get('device', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_start_time_after_end_time(self.instance_get('deployment_start', data), self.instance_get('deployment_end', data)) if not result: raise serializers.ValidationError(message) result, message = validators.deployment_check_overlap(self.instance_get('deployment_start', data), self.instance_get( 'deployment_end', data), self.instance_get( 'device', data), self.instance_get('id', data)) if not result: raise serializers.ValidationError(message) return data","title":"validate"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentSerializer","text":"Bases: DeploymentFieldsMixIn , ModelSerializer Serializer for Deployment model for regular (non-GeoJSON) API endpoints. Source code in data_models\\serializers.py class DeploymentSerializer(DeploymentFieldsMixIn, serializers.ModelSerializer): \"\"\" Serializer for Deployment model for regular (non-GeoJSON) API endpoints. \"\"\" class Meta: model = Deployment exclude = DeploymentFieldsMixIn.Meta.exclude + ['point']","title":"DeploymentSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DeploymentSerializer_GeoJSON","text":"Bases: DeploymentFieldsMixIn , GeoFeatureModelSerializer Serializer for Deployment model in GeoJSON format. Uses the 'point' field as the geographic representation. Source code in data_models\\serializers.py class DeploymentSerializer_GeoJSON(DeploymentFieldsMixIn, geoserializers.GeoFeatureModelSerializer): \"\"\" Serializer for Deployment model in GeoJSON format. Uses the 'point' field as the geographic representation. \"\"\" def __init__(self, *args, **kwargs): self.Meta.geo_field = \"point\" super(DeploymentSerializer_GeoJSON, self).__init__(*args, **kwargs)","title":"DeploymentSerializer_GeoJSON"},{"location":"reference/data_models/serializers/#data_models.serializers.DeviceModelSerializer","text":"Bases: CreatedModifiedMixIn , OwnerMixIn , ModelSerializer Serializer for DeviceModel with type fields, handler info, and ownership/timestamps. Source code in data_models\\serializers.py class DeviceModelSerializer(CreatedModifiedMixIn, OwnerMixIn, serializers.ModelSerializer): \"\"\" Serializer for DeviceModel with type fields, handler info, and ownership/timestamps. \"\"\" type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False) type_ID = serializers.PrimaryKeyRelatedField(source=\"type\", queryset=DataType.objects.all(), required=False) class Meta: model = DeviceModel exclude = [] def to_representation(self, instance): \"\"\" Add data handler info to serialized DeviceModel if available. \"\"\" initial_rep = super(DeviceModelSerializer, self).to_representation(instance) handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep","title":"DeviceModelSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DeviceModelSerializer.to_representation","text":"Add data handler info to serialized DeviceModel if available. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Add data handler info to serialized DeviceModel if available. \"\"\" initial_rep = super(DeviceModelSerializer, self).to_representation(instance) handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep","title":"to_representation"},{"location":"reference/data_models/serializers/#data_models.serializers.DeviceSerializer","text":"Bases: OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Serializer for Device model with type/model, user fields, and permission-based filtering. Source code in data_models\\serializers.py class DeviceSerializer(OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Serializer for Device model with type/model, user fields, and permission-based filtering. \"\"\" type = serializers.SlugRelatedField( slug_field='name', queryset=DataType.objects.all(), required=False) type_ID = serializers.PrimaryKeyRelatedField(source=\"type\", queryset=DataType.objects.all(), required=False) model = serializers.SlugRelatedField( slug_field='name', queryset=DeviceModel.objects.all(), required=False) model_ID = serializers.PrimaryKeyRelatedField(source=\"model\", queryset=DeviceModel.objects.all(), required=False) username = serializers.CharField(required=False) password = serializers.CharField(required=False, write_only=True) is_active = serializers.BooleanField(read_only=True) class Meta: model = Device exclude = [] def __init__(self, *args, **kwargs): self.management_perm = 'data_models.change_device' super(DeviceSerializer, self).__init__(*args, **kwargs) def to_representation(self, instance): \"\"\" Remove sensitive fields, filter by permissions, and add handler/color/symbol info. \"\"\" initial_rep = super(DeviceSerializer, self).to_representation(instance) fields_to_pop = [\"username\"] fields_to_always_pop = [\"password\"] [initial_rep.pop(field, '') for field in fields_to_always_pop] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] device_model = instance.model device_type = instance.type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.model.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep def validate(self, data): \"\"\" Validate device data, ensuring a model is attached and fields are consistent. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'model', 'model_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) return data","title":"DeviceSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.DeviceSerializer.to_representation","text":"Remove sensitive fields, filter by permissions, and add handler/color/symbol info. Source code in data_models\\serializers.py def to_representation(self, instance): \"\"\" Remove sensitive fields, filter by permissions, and add handler/color/symbol info. \"\"\" initial_rep = super(DeviceSerializer, self).to_representation(instance) fields_to_pop = [\"username\"] fields_to_always_pop = [\"password\"] [initial_rep.pop(field, '') for field in fields_to_always_pop] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] device_model = instance.model device_type = instance.type initial_rep[\"colour\"] = device_model.colour if device_model.colour != \"\" else device_type.colour initial_rep[\"symbol\"] = device_model.symbol if device_model.symbol != \"\" else device_type.symbol handler = settings.DATA_HANDLERS.get_handler( instance.type.name, instance.model.name) if handler: initial_rep[\"data_handler\"] = handler.full_name initial_rep[\"data_handler_id\"] = handler.id else: initial_rep[\"data_handler\"] = None initial_rep[\"data_handler_id\"] = None return initial_rep","title":"to_representation"},{"location":"reference/data_models/serializers/#data_models.serializers.DeviceSerializer.validate","text":"Validate device data, ensuring a model is attached and fields are consistent. Source code in data_models\\serializers.py def validate(self, data): \"\"\" Validate device data, ensuring a model is attached and fields are consistent. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'model', 'model_ID', data, Device, self.form_submission ) if not result: raise serializers.ValidationError(message) return data","title":"validate"},{"location":"reference/data_models/serializers/#data_models.serializers.GenericJobSerializer","text":"Bases: Serializer Serializer for displaying generic jobs. Attributes: id ( int ) \u2013 Numeric job ID. name ( str ) \u2013 Name of the job. task_name ( str ) \u2013 Celery task name. data_type ( str ) \u2013 Expected data type. admin_only ( bool ) \u2013 If superuser is required. max_items ( int ) \u2013 Max items for the job. default_args ( dict ) \u2013 Default arguments as JSON. Source code in data_models\\serializers.py class GenericJobSerializer(serializers.Serializer): \"\"\" Serializer for displaying generic jobs. Attributes: id (int): Numeric job ID. name (str): Name of the job. task_name (str): Celery task name. data_type (str): Expected data type. admin_only (bool): If superuser is required. max_items (int): Max items for the job. default_args (dict): Default arguments as JSON. \"\"\" id = serializers.IntegerField() name = serializers.CharField() task_name = serializers.CharField() data_type = serializers.CharField() admin_only = serializers.BooleanField() max_items = serializers.IntegerField() default_args = serializers.JSONField()","title":"GenericJobSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.ProjectSerializer","text":"Bases: OwnerMixIn , ManagerMixIn , CreatedModifiedMixIn , ModelSerializer Serializer for Project model with ownership, management, and timestamps. Source code in data_models\\serializers.py class ProjectSerializer(OwnerMixIn, ManagerMixIn, CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for Project model with ownership, management, and timestamps. \"\"\" is_active = serializers.BooleanField(read_only=True) class Meta: model = Project exclude = [\"data_storages\", \"automated_tasks\", \"archive\"] def __init__(self, *args, **kwargs): self.management_perm = 'data_models.change_project' super(ProjectSerializer, self).__init__(*args, **kwargs)","title":"ProjectSerializer"},{"location":"reference/data_models/serializers/#data_models.serializers.SiteSerializer","text":"Bases: ModelSerializer Serializer for Site model (all fields). Source code in data_models\\serializers.py class SiteSerializer(serializers.ModelSerializer): \"\"\" Serializer for Site model (all fields). \"\"\" class Meta: model = Site fields = '__all__'","title":"SiteSerializer"},{"location":"reference/data_models/signals/","text":"post_remove_file(sender, instance, **kwargs) Post delete signal for DataFile model to update the deployment's thumbnail URL after a file is deleted. Source code in data_models\\signals.py @receiver(post_delete, sender=DataFile) def post_remove_file(sender, instance: DataFile, **kwargs): \"\"\" Post delete signal for DataFile model to update the deployment's thumbnail URL after a file is deleted. \"\"\" if instance.deployment.thumb_url is not None and instance.deployment.thumb_url != \"\": instance.deployment.set_thumb_url() instance.deployment.save() post_save_deploy(sender, instance, created, **kwargs) Post save signal for Deployment model to ensure that the global project is always associated with the deployment instance. Also cascade the permissions Source code in data_models\\signals.py @receiver(post_save, sender=Deployment) def post_save_deploy(sender, instance: Deployment, created, **kwargs): \"\"\" Post save signal for Deployment model to ensure that the global project is always associated with the deployment instance. Also cascade the permissions \"\"\" global_project, added = Project.objects.get_or_create( project_ID=settings.GLOBAL_PROJECT_ID) if global_project not in instance.project.all(): instance.project.add(global_project) post_save_file(sender, instance, created, **kwargs) Post save signal for DataFile model to update the deployment's thumbnail URL. Source code in data_models\\signals.py @receiver(post_save, sender=DataFile) def post_save_file(sender, instance: DataFile, created, **kwargs): \"\"\" Post save signal for DataFile model to update the deployment's thumbnail URL. \"\"\" if instance.deployment.thumb_url is not None and instance.deployment.thumb_url != \"\": instance.deployment.set_thumb_url() instance.deployment.save() post_save_permission_cascade(sender, instance, created, **kwargs) Post save signal for projects and devices, ensuring that they cascade their permissions and that their attached deployments update their permissions. Source code in data_models\\signals.py @receiver(post_save, sender=Project) @receiver(post_save, sender=Device) def post_save_permission_cascade(sender, instance: Device | Project, created, **kwargs): \"\"\" Post save signal for projects and devices, ensuring that they cascade their permissions and that their attached deployments update their permissions. \"\"\" cascade_permissions(instance) for deployment in instance.deployments.all(): try: deployment.save() except Exception as e: logger.error(e) pre_remove_file(sender, instance, **kwargs) Pre delete signal for DataFile model to clean up the attached file before deletion. Source code in data_models\\signals.py @receiver(pre_delete, sender=DataFile) def pre_remove_file(sender, instance: DataFile, **kwargs): \"\"\" Pre delete signal for DataFile model to clean up the attached file before deletion. \"\"\" # deletes the attached file from data storage success = instance.clean_file(True) if not success: raise ( Exception(f'Unable to delete datafile object {instance.file_name}')) update_combo_project(sender, instance, action, reverse, *args, **kwargs) Signal to update the deployment's project when the many-to-many relationship changes. This ensures that the combination project field is updated. Source code in data_models\\signals.py @receiver(m2m_changed, sender=Deployment.project.through) def update_combo_project(sender, instance, action, reverse, *args, **kwargs): \"\"\" Signal to update the deployment's project when the many-to-many relationship changes. This ensures that the combination project field is updated. \"\"\" if (action == 'post_add' or action == 'post_remove') and not reverse: instance.save()","title":"signals"},{"location":"reference/data_models/signals/#data_models.signals.post_remove_file","text":"Post delete signal for DataFile model to update the deployment's thumbnail URL after a file is deleted. Source code in data_models\\signals.py @receiver(post_delete, sender=DataFile) def post_remove_file(sender, instance: DataFile, **kwargs): \"\"\" Post delete signal for DataFile model to update the deployment's thumbnail URL after a file is deleted. \"\"\" if instance.deployment.thumb_url is not None and instance.deployment.thumb_url != \"\": instance.deployment.set_thumb_url() instance.deployment.save()","title":"post_remove_file"},{"location":"reference/data_models/signals/#data_models.signals.post_save_deploy","text":"Post save signal for Deployment model to ensure that the global project is always associated with the deployment instance. Also cascade the permissions Source code in data_models\\signals.py @receiver(post_save, sender=Deployment) def post_save_deploy(sender, instance: Deployment, created, **kwargs): \"\"\" Post save signal for Deployment model to ensure that the global project is always associated with the deployment instance. Also cascade the permissions \"\"\" global_project, added = Project.objects.get_or_create( project_ID=settings.GLOBAL_PROJECT_ID) if global_project not in instance.project.all(): instance.project.add(global_project)","title":"post_save_deploy"},{"location":"reference/data_models/signals/#data_models.signals.post_save_file","text":"Post save signal for DataFile model to update the deployment's thumbnail URL. Source code in data_models\\signals.py @receiver(post_save, sender=DataFile) def post_save_file(sender, instance: DataFile, created, **kwargs): \"\"\" Post save signal for DataFile model to update the deployment's thumbnail URL. \"\"\" if instance.deployment.thumb_url is not None and instance.deployment.thumb_url != \"\": instance.deployment.set_thumb_url() instance.deployment.save()","title":"post_save_file"},{"location":"reference/data_models/signals/#data_models.signals.post_save_permission_cascade","text":"Post save signal for projects and devices, ensuring that they cascade their permissions and that their attached deployments update their permissions. Source code in data_models\\signals.py @receiver(post_save, sender=Project) @receiver(post_save, sender=Device) def post_save_permission_cascade(sender, instance: Device | Project, created, **kwargs): \"\"\" Post save signal for projects and devices, ensuring that they cascade their permissions and that their attached deployments update their permissions. \"\"\" cascade_permissions(instance) for deployment in instance.deployments.all(): try: deployment.save() except Exception as e: logger.error(e)","title":"post_save_permission_cascade"},{"location":"reference/data_models/signals/#data_models.signals.pre_remove_file","text":"Pre delete signal for DataFile model to clean up the attached file before deletion. Source code in data_models\\signals.py @receiver(pre_delete, sender=DataFile) def pre_remove_file(sender, instance: DataFile, **kwargs): \"\"\" Pre delete signal for DataFile model to clean up the attached file before deletion. \"\"\" # deletes the attached file from data storage success = instance.clean_file(True) if not success: raise ( Exception(f'Unable to delete datafile object {instance.file_name}'))","title":"pre_remove_file"},{"location":"reference/data_models/signals/#data_models.signals.update_combo_project","text":"Signal to update the deployment's project when the many-to-many relationship changes. This ensures that the combination project field is updated. Source code in data_models\\signals.py @receiver(m2m_changed, sender=Deployment.project.through) def update_combo_project(sender, instance, action, reverse, *args, **kwargs): \"\"\" Signal to update the deployment's project when the many-to-many relationship changes. This ensures that the combination project field is updated. \"\"\" if (action == 'post_add' or action == 'post_remove') and not reverse: instance.save()","title":"update_combo_project"},{"location":"reference/data_models/tasks/","text":"check_deployment_active() Update the active status of deployments based on their start and end times. Activates deployments that should be active (start time has passed, end time not yet reached). Deactivates deployments that should be inactive (start time not yet reached, or end time passed). This task ensures the 'is_active' status and 'modified_on' timestamp are up-to-date. Source code in data_models\\tasks.py @app.task() def check_deployment_active(): \"\"\" Update the active status of deployments based on their start and end times. - Activates deployments that should be active (start time has passed, end time not yet reached). - Deactivates deployments that should be inactive (start time not yet reached, or end time passed). This task ensures the 'is_active' status and 'modified_on' timestamp are up-to-date. \"\"\" # Get all deployments that are inactive that should be active make_active = Deployment.objects.filter( is_active=False, deployment_start__lte=timezone.now(), ).filter(Q(deployment_end__isnull=True) | Q(deployment_end__gte=timezone.now())) make_active.update(is_active=True, modified_on=timezone.now()) # Get all deployments that are active that should be inactive make_inactive = Deployment.objects.filter( is_active=True, ).filter(Q(deployment_start__gte=timezone.now()) | Q(deployment_end__lte=timezone.now())) make_inactive.update(is_active=False, modified_on=timezone.now()) check_device_status() Check whether devices have transmitted data within their allotted update interval and notify managers if not. For each device with active deployments and autoupdate enabled, this task checks the age of the most recent data file. If a device has not transmitted within its expected update period, managers responsible for the device receive an email notification. Source code in data_models\\tasks.py @app.task() def check_device_status(): \"\"\" Check whether devices have transmitted data within their allotted update interval and notify managers if not. For each device with active deployments and autoupdate enabled, this task checks the age of the most recent data file. If a device has not transmitted within its expected update period, managers responsible for the device receive an email notification. \"\"\" logger.info(\"Checking device status...\") auto_devices = Device.objects.filter( deployments__is_active=True, autoupdate=True, ) logger.info(f\"Checking device status for {auto_devices.count()}\") bad_devices_pks = [] bad_devices_values = [] for device in auto_devices: logger.info(f\"Device {device.device_ID} checking...\") # get the last file time for each device last_file_time = device.deployments.filter(is_active=True).aggregate( Max('files__recording_dt')).get('files__recording_dt__max') if last_file_time is None: logger.info(f\"Device {device.device_ID} has no files.\") bad_devices_pks.append(device.pk) bad_devices_values.append({ 'device_ID': device.device_ID, 'name': device.name, 'file_hours': None }) continue # calculate the age of the last file file_age = timezone.now() - last_file_time # check if the file age is greater than the update time if file_age > timedelta(hours=device.update_time): logger.info( f\"Device {device.device_ID} has not transmitted in the allotted time: {file_age.total_seconds() / 3600} hours.\") # add the device to the list of bad devices bad_devices_pks.append(device.pk) bad_devices_values.append({ 'pk': device.pk, 'device_ID': device.device_ID, 'name': device.name, 'file_hours': file_age.total_seconds() / 3600 # convert to hours }) bad_devices = Device.objects.filter(pk__in=bad_devices_pks) logger.info(f\"Found {len(bad_devices_pks)} bad devices.\") # get all unique managers all_bad_device_users = User.objects.filter( deviceuser__isnull=True, is_active=True).filter( Q(managed_projects__deployments__device__in=bad_devices) | Q(managed_devices__in=bad_devices) ).distinct() for user in all_bad_device_users: logger.info(f\"Getting bad devices for {user.username}\") # for each manager, get their bad devices users_bad_devices = perms['data_models.change_device'].filter( user, bad_devices).distinct() logger.info(f\"Got bad devices for {user.username}\") if not users_bad_devices.exists(): continue # get PKs user_bad_device_pks = users_bad_devices.values_list('pk', flat=True) device_list = [ f'{x.get(\"device_ID\")} - {x.get(\"name\")} - {x.get(\"file_hours\")}' for x in bad_devices_values if x.get('pk') in user_bad_device_pks] device_list_string = \" \\n\".join(device_list) logger.info(f\"Got bad device info for {user.username}\") # send them an email email_body = f\"\"\" Dear {user.first_name} {user.last_name},\\n \\n The following devices which you manage have not transmitted in their allotted time: \\n {device_list_string} \"\"\" send_email_to_user( user, subject=f\"{Site.objects.get_current().name} - {users_bad_devices.count()} devices have not transmitted in allotted time\", body=email_body ) clean_all_files() Remove files from local storage that are archived and have not been modified within the project's clean time. This task iterates through all projects with an archive, identifies eligible files based on modification date and various flags, and removes them using the DataFile.clean_file() method. Source code in data_models\\tasks.py @app.task() def clean_all_files(): \"\"\" Remove files from local storage that are archived and have not been modified within the project's clean time. This task iterates through all projects with an archive, identifies eligible files based on modification date and various flags, and removes them using the DataFile.clean_file() method. \"\"\" projects_to_clean = Project.objects.filter(archive__isnull=False) logger.info(f\"Found {projects_to_clean.count()} projects to clean.\") for project in projects_to_clean: clean_time = project.clean_time logger.info( f\"Cleaning project: {project.name} with clean time: {clean_time} days.\") files_to_clean = DataFile.objects.filter( local_storage=True, archived=True, do_not_remove=False, favourite_of__isnull=True, deployment_last_image__isnull=True ) files_to_clean = files_to_clean.annotate(file_age=ExpressionWrapper( timezone.now().date() - F('modified_on__date'), output_field=DurationField())) files_to_clean = files_to_clean.filter( file_age__gt=timedelta(days=clean_time)) logger.info( f\"Found {files_to_clean.count()} files to clean for project: {project.name}.\") for file in files_to_clean: try: logger.info(f\"Cleaning file: {file.file_name} (ID: {file.pk})\") file.clean_file() except Exception as e: logger.info( f\"Error cleaning file {file.file_name} (ID: {file.pk}): {e}\") end_deployments(deployment_pks, no_delete=False, **kwargs) Mark the specified deployments as ended. Parameters: deployment_pks ( List [ int ] ) \u2013 List of primary keys for Deployment objects to update. no_delete ( bool , default: False ) \u2013 Unused in this function. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). This function sets the 'deployment_end' field to the current time and marks the deployments as inactive. Source code in data_models\\tasks.py @app.task(name=\"end_deployments\") @register_job(\"End deployments\", \"end_deployments\", \"deployment\", True, default_args={}) def end_deployments(deployment_pks: List[int], no_delete: bool = False, **kwargs): \"\"\" Mark the specified deployments as ended. Args: deployment_pks (List[int]): List of primary keys for Deployment objects to update. no_delete (bool, optional): Unused in this function. Defaults to False. **kwargs: Additional keyword arguments (unused). This function sets the 'deployment_end' field to the current time and marks the deployments as inactive. \"\"\" deployment_objs = Deployment.objects.filter( pk__in=deployment_pks, deployment_end=None) logger.info(deployment_objs.count()) deployment_objs.update(deployment_end=timezone.now(), is_active=False) flag_humans(datafile_pks, has_human=False, **kwargs) Set or unset the 'has_human' flag for specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. has_human ( bool , default: False ) \u2013 Value to set for the 'has_human' flag. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'has_human' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"flag_humans\") @register_job(\"Change human flag\", \"flag_humans\", \"datafile\", True, default_args={\"has_human\": False}) def flag_humans(datafile_pks: List[int], has_human: bool = False, **kwargs): \"\"\" Set or unset the 'has_human' flag for specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. has_human (bool, optional): Value to set for the 'has_human' flag. Defaults to False. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'has_human' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(has_human=has_human) flag_no_delete(datafile_pks, no_delete=False, **kwargs) Set or unset the 'do_not_remove' flag for specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. no_delete ( bool , default: False ) \u2013 Value to set for the 'do_not_remove' flag. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"flag_no_delete\") @register_job(\"Change no delete flag\", \"flag_no_delete\", \"datafile\", True, default_args={\"no_delete\": True}) def flag_no_delete(datafile_pks: List[int], no_delete: bool = False, **kwargs): \"\"\" Set or unset the 'do_not_remove' flag for specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. no_delete (bool, optional): Value to set for the 'do_not_remove' flag. Defaults to False. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(do_not_remove=no_delete) set_tag_task(datafile_pks, new_tag='', **kwargs) Set a tag on specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. new_tag ( str , default: '' ) \u2013 Value to set for the 'tag' field. Defaults to an empty string. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"set_tag\") @register_job(\"Set tag\", \"set_tag\", \"datafile\", True, default_args={\"new_tag\": \"\"}) def set_tag_task(datafile_pks: List[int], new_tag: str = \"\", **kwargs): \"\"\" Set a tag on specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. new_tag (str): Value to set for the 'tag' field. Defaults to an empty string. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(tag=new_tag)","title":"tasks"},{"location":"reference/data_models/tasks/#data_models.tasks.check_deployment_active","text":"Update the active status of deployments based on their start and end times. Activates deployments that should be active (start time has passed, end time not yet reached). Deactivates deployments that should be inactive (start time not yet reached, or end time passed). This task ensures the 'is_active' status and 'modified_on' timestamp are up-to-date. Source code in data_models\\tasks.py @app.task() def check_deployment_active(): \"\"\" Update the active status of deployments based on their start and end times. - Activates deployments that should be active (start time has passed, end time not yet reached). - Deactivates deployments that should be inactive (start time not yet reached, or end time passed). This task ensures the 'is_active' status and 'modified_on' timestamp are up-to-date. \"\"\" # Get all deployments that are inactive that should be active make_active = Deployment.objects.filter( is_active=False, deployment_start__lte=timezone.now(), ).filter(Q(deployment_end__isnull=True) | Q(deployment_end__gte=timezone.now())) make_active.update(is_active=True, modified_on=timezone.now()) # Get all deployments that are active that should be inactive make_inactive = Deployment.objects.filter( is_active=True, ).filter(Q(deployment_start__gte=timezone.now()) | Q(deployment_end__lte=timezone.now())) make_inactive.update(is_active=False, modified_on=timezone.now())","title":"check_deployment_active"},{"location":"reference/data_models/tasks/#data_models.tasks.check_device_status","text":"Check whether devices have transmitted data within their allotted update interval and notify managers if not. For each device with active deployments and autoupdate enabled, this task checks the age of the most recent data file. If a device has not transmitted within its expected update period, managers responsible for the device receive an email notification. Source code in data_models\\tasks.py @app.task() def check_device_status(): \"\"\" Check whether devices have transmitted data within their allotted update interval and notify managers if not. For each device with active deployments and autoupdate enabled, this task checks the age of the most recent data file. If a device has not transmitted within its expected update period, managers responsible for the device receive an email notification. \"\"\" logger.info(\"Checking device status...\") auto_devices = Device.objects.filter( deployments__is_active=True, autoupdate=True, ) logger.info(f\"Checking device status for {auto_devices.count()}\") bad_devices_pks = [] bad_devices_values = [] for device in auto_devices: logger.info(f\"Device {device.device_ID} checking...\") # get the last file time for each device last_file_time = device.deployments.filter(is_active=True).aggregate( Max('files__recording_dt')).get('files__recording_dt__max') if last_file_time is None: logger.info(f\"Device {device.device_ID} has no files.\") bad_devices_pks.append(device.pk) bad_devices_values.append({ 'device_ID': device.device_ID, 'name': device.name, 'file_hours': None }) continue # calculate the age of the last file file_age = timezone.now() - last_file_time # check if the file age is greater than the update time if file_age > timedelta(hours=device.update_time): logger.info( f\"Device {device.device_ID} has not transmitted in the allotted time: {file_age.total_seconds() / 3600} hours.\") # add the device to the list of bad devices bad_devices_pks.append(device.pk) bad_devices_values.append({ 'pk': device.pk, 'device_ID': device.device_ID, 'name': device.name, 'file_hours': file_age.total_seconds() / 3600 # convert to hours }) bad_devices = Device.objects.filter(pk__in=bad_devices_pks) logger.info(f\"Found {len(bad_devices_pks)} bad devices.\") # get all unique managers all_bad_device_users = User.objects.filter( deviceuser__isnull=True, is_active=True).filter( Q(managed_projects__deployments__device__in=bad_devices) | Q(managed_devices__in=bad_devices) ).distinct() for user in all_bad_device_users: logger.info(f\"Getting bad devices for {user.username}\") # for each manager, get their bad devices users_bad_devices = perms['data_models.change_device'].filter( user, bad_devices).distinct() logger.info(f\"Got bad devices for {user.username}\") if not users_bad_devices.exists(): continue # get PKs user_bad_device_pks = users_bad_devices.values_list('pk', flat=True) device_list = [ f'{x.get(\"device_ID\")} - {x.get(\"name\")} - {x.get(\"file_hours\")}' for x in bad_devices_values if x.get('pk') in user_bad_device_pks] device_list_string = \" \\n\".join(device_list) logger.info(f\"Got bad device info for {user.username}\") # send them an email email_body = f\"\"\" Dear {user.first_name} {user.last_name},\\n \\n The following devices which you manage have not transmitted in their allotted time: \\n {device_list_string} \"\"\" send_email_to_user( user, subject=f\"{Site.objects.get_current().name} - {users_bad_devices.count()} devices have not transmitted in allotted time\", body=email_body )","title":"check_device_status"},{"location":"reference/data_models/tasks/#data_models.tasks.clean_all_files","text":"Remove files from local storage that are archived and have not been modified within the project's clean time. This task iterates through all projects with an archive, identifies eligible files based on modification date and various flags, and removes them using the DataFile.clean_file() method. Source code in data_models\\tasks.py @app.task() def clean_all_files(): \"\"\" Remove files from local storage that are archived and have not been modified within the project's clean time. This task iterates through all projects with an archive, identifies eligible files based on modification date and various flags, and removes them using the DataFile.clean_file() method. \"\"\" projects_to_clean = Project.objects.filter(archive__isnull=False) logger.info(f\"Found {projects_to_clean.count()} projects to clean.\") for project in projects_to_clean: clean_time = project.clean_time logger.info( f\"Cleaning project: {project.name} with clean time: {clean_time} days.\") files_to_clean = DataFile.objects.filter( local_storage=True, archived=True, do_not_remove=False, favourite_of__isnull=True, deployment_last_image__isnull=True ) files_to_clean = files_to_clean.annotate(file_age=ExpressionWrapper( timezone.now().date() - F('modified_on__date'), output_field=DurationField())) files_to_clean = files_to_clean.filter( file_age__gt=timedelta(days=clean_time)) logger.info( f\"Found {files_to_clean.count()} files to clean for project: {project.name}.\") for file in files_to_clean: try: logger.info(f\"Cleaning file: {file.file_name} (ID: {file.pk})\") file.clean_file() except Exception as e: logger.info( f\"Error cleaning file {file.file_name} (ID: {file.pk}): {e}\")","title":"clean_all_files"},{"location":"reference/data_models/tasks/#data_models.tasks.end_deployments","text":"Mark the specified deployments as ended. Parameters: deployment_pks ( List [ int ] ) \u2013 List of primary keys for Deployment objects to update. no_delete ( bool , default: False ) \u2013 Unused in this function. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). This function sets the 'deployment_end' field to the current time and marks the deployments as inactive. Source code in data_models\\tasks.py @app.task(name=\"end_deployments\") @register_job(\"End deployments\", \"end_deployments\", \"deployment\", True, default_args={}) def end_deployments(deployment_pks: List[int], no_delete: bool = False, **kwargs): \"\"\" Mark the specified deployments as ended. Args: deployment_pks (List[int]): List of primary keys for Deployment objects to update. no_delete (bool, optional): Unused in this function. Defaults to False. **kwargs: Additional keyword arguments (unused). This function sets the 'deployment_end' field to the current time and marks the deployments as inactive. \"\"\" deployment_objs = Deployment.objects.filter( pk__in=deployment_pks, deployment_end=None) logger.info(deployment_objs.count()) deployment_objs.update(deployment_end=timezone.now(), is_active=False)","title":"end_deployments"},{"location":"reference/data_models/tasks/#data_models.tasks.flag_humans","text":"Set or unset the 'has_human' flag for specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. has_human ( bool , default: False ) \u2013 Value to set for the 'has_human' flag. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'has_human' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"flag_humans\") @register_job(\"Change human flag\", \"flag_humans\", \"datafile\", True, default_args={\"has_human\": False}) def flag_humans(datafile_pks: List[int], has_human: bool = False, **kwargs): \"\"\" Set or unset the 'has_human' flag for specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. has_human (bool, optional): Value to set for the 'has_human' flag. Defaults to False. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'has_human' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(has_human=has_human)","title":"flag_humans"},{"location":"reference/data_models/tasks/#data_models.tasks.flag_no_delete","text":"Set or unset the 'do_not_remove' flag for specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. no_delete ( bool , default: False ) \u2013 Value to set for the 'do_not_remove' flag. Defaults to False. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"flag_no_delete\") @register_job(\"Change no delete flag\", \"flag_no_delete\", \"datafile\", True, default_args={\"no_delete\": True}) def flag_no_delete(datafile_pks: List[int], no_delete: bool = False, **kwargs): \"\"\" Set or unset the 'do_not_remove' flag for specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. no_delete (bool, optional): Value to set for the 'do_not_remove' flag. Defaults to False. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(do_not_remove=no_delete)","title":"flag_no_delete"},{"location":"reference/data_models/tasks/#data_models.tasks.set_tag_task","text":"Set a tag on specified DataFile objects. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects to update. new_tag ( str , default: '' ) \u2013 Value to set for the 'tag' field. Defaults to an empty string. **kwargs \u2013 Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. Source code in data_models\\tasks.py @app.task(name=\"set_tag\") @register_job(\"Set tag\", \"set_tag\", \"datafile\", True, default_args={\"new_tag\": \"\"}) def set_tag_task(datafile_pks: List[int], new_tag: str = \"\", **kwargs): \"\"\" Set a tag on specified DataFile objects. Args: datafile_pks (List[int]): List of primary keys for DataFile objects to update. new_tag (str): Value to set for the 'tag' field. Defaults to an empty string. **kwargs: Additional keyword arguments (unused). Updates each DataFile's 'do_not_remove' attribute to the provided value. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) logger.info(file_objs.count()) file_objs.update(tag=new_tag)","title":"set_tag_task"},{"location":"reference/data_models/validators/","text":"data_file_in_deployment(recording_dt, deployment) Check if a date falls within a deployment's date range. Parameters: recording_dt ( datetime ) \u2013 Recording date time to check deployment ( Deployment ) \u2013 Deployment object to check Returns: tuple [ bool , dict ] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def data_file_in_deployment(recording_dt: datetime, deployment: \"Deployment\") -> tuple[bool, dict]: \"\"\" Check if a date falls within a deployment's date range. Args: recording_dt (datetime): Recording date time to check deployment (Deployment): Deployment object to check Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if deployment.deployment_end is None: deployment_end = \"\" else: deployment_end = f\" - {str(deployment.deployment_end)}\" valid_recording_dt_list = deployment.check_dates([recording_dt]) if all(valid_recording_dt_list): return True, \"\" error_message = {\"recording_dt\": f\"recording_dt not in deployment {deployment.deployment_device_ID} date time range \" f\"{str(deployment.deployment_start)}{deployment_end}\"} return False, error_message deployment_check_overlap(start_dt, end_dt, device, deployment_pk=None) Check if a new deployment of a device would overlap with existing deployments. Parameters: start_dt ( datetime ) \u2013 start datetime of new deployment end_dt ( datetime ) \u2013 end datetime of new deployment device ( Device ) \u2013 Device of new deployment deployment_pk ( int , default: None ) \u2013 pk of a deployment to be ignored when considering overlaps. Returns: Tuple [ bool , Dict [ str , Any ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_check_overlap(start_dt: datetime, end_dt: datetime, device: \"Device\", deployment_pk: Optional[int] = None) -> Tuple[bool, Dict[str, Any]]: \"\"\" Check if a new deployment of a device would overlap with existing deployments. Args: start_dt (datetime): start datetime of new deployment end_dt (datetime): end datetime of new deployment device (Device): Device of new deployment deployment_pk (int): pk of a deployment to be ignored when considering overlaps. Include if editing an existing deployment to avoid checking for overlap with itself. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" overlapping_deployments = device.check_overlap( start_dt, end_dt, deployment_pk) if len(overlapping_deployments) == 0: return True, \"\" error_message = { \"deployment_start\": f\"this deployment of {device.device_ID} \" f\"would overlap with {','.join(overlapping_deployments)}\" } return False, error_message deployment_check_type(device_type, device) Check if a deployment matches it's device type. Parameters: device_type ( DataType ) \u2013 New deployment device type. device ( Device ) \u2013 Device of new deployment. Returns: Tuple [ bool , Dict [ str , str ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_check_type(device_type: \"DataType\", device: \"Device\") -> Tuple[bool, Dict[str, str]]: \"\"\" Check if a deployment matches it's device type. Args: device_type (DataType): New deployment device type. device (Device): Device of new deployment. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if device_type is None or device.type == device_type: return True, \"\" error_message = { 'device': f\"{device.device_ID} is not a {device_type.name} device\"} return False, error_message deployment_start_time_after_end_time(start_dt, end_dt=None) Check if end time is after start time Parameters: start_dt ( datetime ) \u2013 Start time to check end_dt ( datetime , default: None ) \u2013 End time to check Returns: tuple [ bool , dict ] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_start_time_after_end_time(start_dt: datetime, end_dt: Optional[datetime] = None) -> tuple[bool, dict]: \"\"\" Check if end time is after start time Args: start_dt (datetime): Start time to check end_dt (datetime): End time to check Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if (end_dt is None) or (end_dt > start_dt): return True, \"\" error_message = { \"deployment_end\": f\"End time {end_dt} must be after start time f{start_dt}\"} return False, error_message device_check_type(device_type, device_model) summary Check if a device matches it's device model type. Parameters: device_type ( DataType ) \u2013 New device type. device_model ( DeviceModel ) \u2013 DeviceModel of new device. Returns: Tuple [ bool , Dict [ str , str ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def device_check_type(device_type: \"DataType\", device_model: \"DeviceModel\") -> Tuple[bool, Dict[str, str]]: \"\"\"_summary_ Check if a device matches it's device model type. Args: device_type (DataType): New device type. device_model (DeviceModel): DeviceModel of new device. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if device_type is None or device_model.type == device_type: return True, \"EMPTY STRING\" error_message = { 'model': f\"{device_model.name} is not a {device_type.name} device\"} return False, error_message","title":"validators"},{"location":"reference/data_models/validators/#data_models.validators.data_file_in_deployment","text":"Check if a date falls within a deployment's date range. Parameters: recording_dt ( datetime ) \u2013 Recording date time to check deployment ( Deployment ) \u2013 Deployment object to check Returns: tuple [ bool , dict ] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def data_file_in_deployment(recording_dt: datetime, deployment: \"Deployment\") -> tuple[bool, dict]: \"\"\" Check if a date falls within a deployment's date range. Args: recording_dt (datetime): Recording date time to check deployment (Deployment): Deployment object to check Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if deployment.deployment_end is None: deployment_end = \"\" else: deployment_end = f\" - {str(deployment.deployment_end)}\" valid_recording_dt_list = deployment.check_dates([recording_dt]) if all(valid_recording_dt_list): return True, \"\" error_message = {\"recording_dt\": f\"recording_dt not in deployment {deployment.deployment_device_ID} date time range \" f\"{str(deployment.deployment_start)}{deployment_end}\"} return False, error_message","title":"data_file_in_deployment"},{"location":"reference/data_models/validators/#data_models.validators.deployment_check_overlap","text":"Check if a new deployment of a device would overlap with existing deployments. Parameters: start_dt ( datetime ) \u2013 start datetime of new deployment end_dt ( datetime ) \u2013 end datetime of new deployment device ( Device ) \u2013 Device of new deployment deployment_pk ( int , default: None ) \u2013 pk of a deployment to be ignored when considering overlaps. Returns: Tuple [ bool , Dict [ str , Any ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_check_overlap(start_dt: datetime, end_dt: datetime, device: \"Device\", deployment_pk: Optional[int] = None) -> Tuple[bool, Dict[str, Any]]: \"\"\" Check if a new deployment of a device would overlap with existing deployments. Args: start_dt (datetime): start datetime of new deployment end_dt (datetime): end datetime of new deployment device (Device): Device of new deployment deployment_pk (int): pk of a deployment to be ignored when considering overlaps. Include if editing an existing deployment to avoid checking for overlap with itself. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" overlapping_deployments = device.check_overlap( start_dt, end_dt, deployment_pk) if len(overlapping_deployments) == 0: return True, \"\" error_message = { \"deployment_start\": f\"this deployment of {device.device_ID} \" f\"would overlap with {','.join(overlapping_deployments)}\" } return False, error_message","title":"deployment_check_overlap"},{"location":"reference/data_models/validators/#data_models.validators.deployment_check_type","text":"Check if a deployment matches it's device type. Parameters: device_type ( DataType ) \u2013 New deployment device type. device ( Device ) \u2013 Device of new deployment. Returns: Tuple [ bool , Dict [ str , str ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_check_type(device_type: \"DataType\", device: \"Device\") -> Tuple[bool, Dict[str, str]]: \"\"\" Check if a deployment matches it's device type. Args: device_type (DataType): New deployment device type. device (Device): Device of new deployment. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if device_type is None or device.type == device_type: return True, \"\" error_message = { 'device': f\"{device.device_ID} is not a {device_type.name} device\"} return False, error_message","title":"deployment_check_type"},{"location":"reference/data_models/validators/#data_models.validators.deployment_start_time_after_end_time","text":"Check if end time is after start time Parameters: start_dt ( datetime ) \u2013 Start time to check end_dt ( datetime , default: None ) \u2013 End time to check Returns: tuple [ bool , dict ] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def deployment_start_time_after_end_time(start_dt: datetime, end_dt: Optional[datetime] = None) -> tuple[bool, dict]: \"\"\" Check if end time is after start time Args: start_dt (datetime): Start time to check end_dt (datetime): End time to check Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if (end_dt is None) or (end_dt > start_dt): return True, \"\" error_message = { \"deployment_end\": f\"End time {end_dt} must be after start time f{start_dt}\"} return False, error_message","title":"deployment_start_time_after_end_time"},{"location":"reference/data_models/validators/#data_models.validators.device_check_type","text":"summary Check if a device matches it's device model type. Parameters: device_type ( DataType ) \u2013 New device type. device_model ( DeviceModel ) \u2013 DeviceModel of new device. Returns: Tuple [ bool , Dict [ str , str ]] \u2013 success (boolean), error message (dict where the key is the associated field name) Source code in data_models\\validators.py def device_check_type(device_type: \"DataType\", device_model: \"DeviceModel\") -> Tuple[bool, Dict[str, str]]: \"\"\"_summary_ Check if a device matches it's device model type. Args: device_type (DataType): New device type. device_model (DeviceModel): DeviceModel of new device. Returns: success (boolean), error message (dict where the key is the associated field name) \"\"\" if device_type is None or device_model.type == device_type: return True, \"EMPTY STRING\" error_message = { 'model': f\"{device_model.name} is not a {device_type.name} device\"} return False, error_message","title":"device_check_type"},{"location":"reference/data_models/viewsets/","text":"DataFileViewSet Bases: CheckAttachmentViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing DataFile objects. Offers comprehensive CRUD operations, advanced filtering, search, and many custom actions. Includes endpoints for checking file existence, favorites, project/deployment/device-specific data files, and starting jobs on sets of files. Main Features List, retrieve, create (multi-upload), update, delete DataFiles. Advanced filter/search, including by taxon and tag. Favorite/unfavorite files, and list favorites for user or all users. Retrieve associated observations. Bulk operations (count, start jobs) on filtered/queryset results. Project, deployment, and device-specific filtering. Custom Actions check_existing: Check which files already exist. ids_count, queryset_count, start_job: Bulk operations. observations: List observations for a datafile. favourite_file: Toggle favorite status. deployment_datafiles, project_datafiles, device_datafiles: Scoped file queries. user_favourite_datafiles, favourited_datafiles: Favorites endpoints. Source code in data_models\\viewsets.py @extend_schema(summary=\"Data files\", description=\"Files recorded by sensors.\", tags=[\"DataFiles\"], methods=[\"get\", \"post\", \"patch\", \"delete\"], responses=DummyDataFileSerializer ) @extend_schema_view( list=extend_schema(summary='List datafiles.', parameters=[ctdp_parameter], ), retrieve=extend_schema(summary='Get a single datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to get.\")]), partial_update=extend_schema(summary='Partially update a datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to update.\")]), create=extend_schema(summary='Upload datafiles', request=DummyDataFileUploadSerializer, responses=inline_upload_response_serializer, description=\"Upload multiple datafiles or part of a lrger datafile\"), destroy=extend_schema(summary='Delete a datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered devices\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), check_existing=extend_schema(summary=\"Check a list of filenames for files already in the database\", filters=True, request=DataFileCheckSerializer, responses=serializers.ListSerializer( child=serializers.CharField(default=\"myfile.jpg\"), many=False), ), project_datafiles=extend_schema(summary=\"Datafiles from project\", description=\"Get datafiles from a specific project.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get datafiles.\")]), deployment_datafiles=extend_schema(summary=\"Datafiles from deployment\", description=\"Get datafiles from a specific deployment.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment from which to get datafiles.\")]), device_datafiles=extend_schema(summary=\"Datafiles from device\", description=\"Get datafiles from a specific device.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get datafiles.\")]), user_favourite_datafiles=extend_schema(summary=\"User favourite datafiles\", description=\"Get datafiles favourited by the current user.\", filters=True, parameters=[ ctdp_parameter, ]), favourited_datafiles=extend_schema(summary=\"Favourited datafiles\", description=\"Get datafiles favourited by users.\", filters=True, parameters=[ ctdp_parameter, ]), favourite_file=extend_schema(exclude=True), observations=extend_schema(exclude=True), deployment_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Deployment\", description=\"Return the count of DataFile objects for a given deployment. Returns an integer.\", parameters=[ OpenApiParameter(\"deployment_pk\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Deployment ID\"), ], responses=OpenApiTypes.INT ), deployment_datafiles_start_job=extend_schema( summary=\"Start a Job on Deployment DataFiles\", description=\"Start a job on DataFiles for a given deployment.\", parameters=[ OpenApiParameter(\"deployment_pk\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Deployment ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], # or whatever serializer describes your POST body request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), project_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Project\", description=\"Return the count of DataFile objects for a given project. Returns an integer.\", parameters=[ OpenApiParameter(\"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Project ID\"), ], responses=OpenApiTypes.INT ), project_datafiles_start_job=extend_schema( summary=\"Start a Job on Project DataFiles\", description=\"Start a job on DataFiles for a given project.\", parameters=[ OpenApiParameter(\"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Project ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), device_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Device\", description=\"Return the count of DataFile objects for a given device. Returns an integer.\", parameters=[ OpenApiParameter(\"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Device ID\"), ], responses=OpenApiTypes.INT ), device_datafiles_start_job=extend_schema( summary=\"Start a Job on Device DataFiles\", description=\"Start a job on DataFiles for a given device.\", parameters=[ OpenApiParameter(\"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Device ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), user_favourite_datafiles_queryset_count=extend_schema( summary=\"Count User Favourite DataFiles\", description=\"Return the count of DataFile objects favourited by the current user. Returns an integer.\", responses=OpenApiTypes.INT ), user_favourite_datafiles_start_job=extend_schema( summary=\"Start a Job on User Favourite DataFiles\", description=\"Start a job on DataFiles favourited by the current user.\", parameters=[ OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), favourited_datafiles_queryset_count=extend_schema( summary=\"Count Favourited DataFiles\", description=\"Return the count of DataFile objects favourited by any user. Returns an integer.\", responses=OpenApiTypes.INT ), favourited_datafiles_start_job=extend_schema( summary=\"Start a Job on Favourited DataFiles\", description=\"Start a job on DataFiles favourited by any user.\", parameters=[ OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), ) class DataFileViewSet(CheckAttachmentViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing DataFile objects. Offers comprehensive CRUD operations, advanced filtering, search, and many custom actions. Includes endpoints for checking file existence, favorites, project/deployment/device-specific data files, and starting jobs on sets of files. Main Features: - List, retrieve, create (multi-upload), update, delete DataFiles. - Advanced filter/search, including by taxon and tag. - Favorite/unfavorite files, and list favorites for user or all users. - Retrieve associated observations. - Bulk operations (count, start jobs) on filtered/queryset results. - Project, deployment, and device-specific filtering. Custom Actions: - check_existing: Check which files already exist. - ids_count, queryset_count, start_job: Bulk operations. - observations: List observations for a datafile. - favourite_file: Toggle favorite status. - deployment_datafiles, project_datafiles, device_datafiles: Scoped file queries. - user_favourite_datafiles, favourited_datafiles: Favorites endpoints. \"\"\" http_method_names = ['get', 'patch', 'delete', 'post', 'head'] filterset_class = DataFileFilter search_fields = ['=tag', 'file_name', 'observations__taxon__species_name', 'observations__taxon__species_common_name'] def get_serializer_class(self): if self.action == 'create': return DataFileUploadSerializer else: if 'ctdp' in self.request.GET.keys(): return DataFileSerializerCTDP else: return DataFileSerializer def get_queryset(self): qs = DataFile.objects.prefetch_related(\"deployment\", \"observations__taxon\").all().distinct() if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_media_qs(qs) return qs @action(detail=False, methods=['post'], pagination_class=None) def check_existing(self, request, *args, **kwargs): queryset = perms['data_models.view_datafile'].filter( request.user, self.get_queryset()) serializer = DataFileCheckSerializer(data=request.data) if not serializer.is_valid(): return Response({\"detail\": serializer.errors}, status=status.HTTP_400_BAD_REQUEST) filter_params = request.data or request.GET if filter_params: queryfilter = self.filterset_class( filter_params, queryset=queryset) queryset = queryfilter.qs if (original_names := serializer.validated_data.get('original_names')): existing_names = queryset.filter( original_name__in=original_names).values_list('original_name', flat=True) existing_names = list(existing_names) missing_names = [ x for x in original_names if x not in existing_names] elif (file_names := serializer.validated_data.get('file_names')): existing_names = queryset.filter( file_name__in=file_names).values_list('file_name', flat=True) missing_names = [ x for x in original_names if x not in existing_names] else: return Response({\"detail\": \"Either 'original_names' or 'file_names' must be provided.\"}, status=status.HTTP_400_BAD_REQUEST) return Response(missing_names, status=status.HTTP_200_OK) @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = DataFile.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def observations(self, request, pk=None): data_file = DataFile.objects.get(pk=pk) # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files=data_file) # Paginate the queryset logger.info(self.request) page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = ObservationSerializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = ObservationSerializer( observation_qs, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK) @action(detail=True, methods=['post']) def favourite_file(self, request, pk=None): data_file = self.get_object() user = request.user if user: if data_file.favourite_of.all().filter(pk=user.pk).exists(): data_file.favourite_of.remove(user) else: data_file.favourite_of.add(user) return Response({}, status=status.HTTP_200_OK) else: return Response(status=status.HTTP_403_FORBIDDEN) def check_attachment(self, serializer): deployment_object = serializer.validated_data.get( 'deployment', serializer.instance.deployment) if not self.request.user.has_perm('data_models.change_deployment', deployment_object): raise PermissionDenied(f\"You don't have permission to add a datafile\" f\" to {deployment_object.deployment_device_ID}\") def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) headers = self.get_success_headers(serializer.validated_data) instance = serializer.validated_data files = instance.get('files') recording_dt = instance.get('recording_dt') extra_data = instance.get('extra_data') deployment_object = instance.get('deployment_object') device_object = instance.get('device_object') data_types = instance.get('data_types') check_filename = instance.get('check_filename') multipart = 'HTTP_CONTENT_RANGE' in request.META with transaction.atomic(), connection.cursor() as cursor: # Remove db limits during this function. cursor.execute('SET LOCAL statement_timeout TO 0;') uploaded_files, invalid_files, existing_files, status_code = create_file_objects( files, check_filename, recording_dt, extra_data, deployment_object, device_object, data_types, self.request.user, multipart) logger.info( f\"Uploaded files: {uploaded_files}, Invalid files: {invalid_files}, Existing files: {existing_files}, Status code: {status_code}\") if len(uploaded_files) > 0: returned_data = DataFileSerializer(data=uploaded_files, many=True) returned_data.is_valid() uploaded_files = returned_data.data return Response({\"uploaded_files\": uploaded_files, \"invalid_files\": invalid_files, \"existing_files\": existing_files}, status=status_code, headers=headers) # --- Deployment DataFiles --- @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_pk>\\w+)', url_name=\"deployment_datafiles\") def deployment_datafiles(self, request, deployment_pk=None): # Filter data files based on the deployment primary key (deployment_pk) data_file_qs = DataFile.objects.filter(deployment__pk=deployment_pk) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_pk>\\w+)/queryset_count') def deployment_datafiles_queryset_count(self, request, deployment_pk=None): queryset = self.filter_queryset( DataFile.objects.filter(deployment__pk=deployment_pk)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'deployment/(?P<deployment_pk>\\w+)/start_job/(?P<job_name>\\w+)') def deployment_datafiles_start_job(self, request, deployment_pk=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(deployment__pk=deployment_pk)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Project DataFiles --- @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)', url_name=\"project_datafiles\") def project_datafiles(self, request, project_id=None): # Filter data files based on the project primary key (project_id) through deployments data_file_qs = DataFile.objects.filter( deployment__project__pk=project_id) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)/queryset_count') def project_datafiles_queryset_count(self, request, project_id=None): queryset = self.filter_queryset( DataFile.objects.filter(project__pk=project_id)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'project/(?P<project_id>\\w+)/start_job/(?P<job_name>\\w+)') def project_datafiles_start_job(self, request, project_id=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(project__pk=project_id)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Device DataFiles --- @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)', url_name=\"device_datafiles\") def device_datafiles(self, request, device_id=None): # Filter data files based on the device primary key (device_id) through deployments data_file_qs = DataFile.objects.filter( deployment__device__pk=device_id) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)/queryset_count') def device_datafiles_queryset_count(self, request, device_id=None): queryset = self.filter_queryset( DataFile.objects.filter(device__pk=device_id)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'device/(?P<device_id>\\w+)/start_job/(?P<job_name>\\w+)') def device_datafiles_start_job(self, request, device_id=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(device__pk=device_id)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- User Favourite DataFiles --- @action(detail=False, methods=['get'], url_path=r'user', url_name=\"user_favourite_datafiles\") def user_favourite_datafiles(self, request): # Get the data files favorited by the current user user = request.user if not user.is_authenticated: return Response({\"detail\": \"Authentication credentials were not provided.\"}, status=status.HTTP_401_UNAUTHORIZED) data_file_qs = DataFile.objects.filter(favourite_of=user) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path='user_favourite/queryset_count') def user_favourite_datafiles_queryset_count(self, request): queryset = self.filter_queryset( DataFile.objects.filter(favourites=request.user)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path='user_favourite/start_job/(?P<job_name>\\w+)') def user_favourite_datafiles_start_job(self, request, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(favourites=request.user)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Favourited DataFiles (by any user) --- @action(detail=False, methods=['get'], url_path=r'highlights', url_name=\"highlight_datafiles\") def favourited_datafiles(self, request): user = request.user if not user.is_authenticated: return Response({\"detail\": \"Authentication credentials were not provided.\"}, status=status.HTTP_401_UNAUTHORIZED) # Get the data files where the favourite_of column is not null data_file_qs = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(favourite_of__isnull=False)) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path='favourited/queryset_count') def favourited_datafiles_queryset_count(self, request): queryset = self.filter_queryset( DataFile.objects.filter(favourites__isnull=False).distinct()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path='favourited/start_job/(?P<job_name>\\w+)') def favourited_datafiles_start_job(self, request, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(favourites__isnull=False).distinct()) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) DataTypeViewSet Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for DataType objects. Allows searching and filtering of data types (for devices or files). Results are paginated and cached for performance. Source code in data_models\\viewsets.py @extend_schema(summary=\"Data type\", description=\"Type of devices or of datafiles.\", tags=[\"Data type\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List data types.', ), retrieve=extend_schema(summary='Get a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to get.\")]), partial_update=extend_schema(summary='Partially update a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to update.\")]), create=extend_schema(summary='Create data type', description=\"Add a data type\"), destroy=extend_schema(summary='Delete a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to delete.\")]), ) class DataTypeViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for DataType objects. Allows searching and filtering of data types (for devices or files). Results are paginated and cached for performance. \"\"\" serializer_class = DataTypeSerializer queryset = DataType.objects.all().distinct() search_fields = ['name'] filterset_class = DataTypeFilter @method_decorator(cache_page(60 * 60 * 2)) def list(self, request): return super().list(request) DeploymentViewSet Bases: CheckAttachmentViewSetMixIn , AddOwnerViewSetMixIn , CheckFormViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Deployment objects. This viewset provides CRUD operations and custom actions for deployments of devices in the field. It includes filtering, searching, pagination, and project/device-specific endpoints. Main Features Filter, search, and order deployments. Retrieve deployments associated with specific projects or devices. Count deployments and start jobs on filtered sets. Retrieve metrics for a deployment. Enforces permission checks for project and device attachment. Custom Actions ids_count: Count deployments by list of IDs. queryset_count: Count deployments in filtered queryset. start_job: Start a job for deployments. metrics: Get deployment metrics. project/device-specific list, count, and job actions. Source code in data_models\\viewsets.py @extend_schema(summary=\"Deployments\", description=\"Deployments of devices in the field, with certain settings.\", tags=[\"Deployments\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyDeploymentSerializer, request=DummyDeploymentSerializer, ) @extend_schema_view( list=extend_schema(summary='List deployments.', parameters=[ctdp_parameter, geoJSON_parameter], ), retrieve=extend_schema(summary='Get a single deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to get.\")]), update=extend_schema(summary='Update an deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deplyment to update.\")]), partial_update=extend_schema(summary='Partially update a deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to update.\")]), create=extend_schema(summary='Create a deployment'), destroy=extend_schema(summary='Delete a deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to delete.\")]), project_deployments=extend_schema(summary=\"Deployments from project\", description=\"Get deployments from a specific project.\", filters=True, parameters=[ctdp_parameter, geoJSON_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), device_deployments=extend_schema(summary=\"Deployments from device\", description=\"Get deployments of a specific device.\", filters=True, parameters=[ctdp_parameter, geoJSON_parameter, OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), project_queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer, parameters=[OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), project_start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer, parameters=[OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), device_queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer, parameters=[OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]), device_start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer, parameters=[OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]) ) class DeploymentViewSet(CheckAttachmentViewSetMixIn, AddOwnerViewSetMixIn, CheckFormViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Deployment objects. This viewset provides CRUD operations and custom actions for deployments of devices in the field. It includes filtering, searching, pagination, and project/device-specific endpoints. Main Features: - Filter, search, and order deployments. - Retrieve deployments associated with specific projects or devices. - Count deployments and start jobs on filtered sets. - Retrieve metrics for a deployment. - Enforces permission checks for project and device attachment. Custom Actions: - ids_count: Count deployments by list of IDs. - queryset_count: Count deployments in filtered queryset. - start_job: Start a job for deployments. - metrics: Get deployment metrics. - project/device-specific list, count, and job actions. \"\"\" search_fields = ['deployment_device_ID', 'device__name', 'device__device_ID', 'extra_data'] ordering_fields = ordering = [ 'deployment_device_ID', 'created_on', 'device_type'] queryset = Deployment.objects.all().distinct() filterset_class = DeploymentFilter filter_backends = viewsets.ModelViewSet.filter_backends + \\ [filters_gis.InBBoxFilter] def get_queryset(self): qs = Deployment.objects.all().distinct() if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_deployment_qs(qs) return qs def get_serializer_class(self): if 'geojson' in self.request.GET.keys(): return DeploymentSerializer_GeoJSON elif 'ctdp' in self.request.GET.keys(): return DeploymentSerializerCTDP else: return DeploymentSerializer def check_attachment(self, serializer): project_objects = serializer.validated_data.get('project') if project_objects is not None: for project_object in project_objects: if (not self.request.user.has_perm('data_models.change_project', project_object)) and \\ (project_object.name != settings.GLOBAL_PROJECT_ID): raise PermissionDenied( f\"You don't have permission to add a deployment to {project_object.project_ID}\") device_object = serializer.validated_data.get('device') if device_object is not None: if not self.request.user.has_perm('data_models.change_device', device_object): raise PermissionDenied( f\"You don't have permission to deploy {device_object.device_ID}\") @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Deployment.objects.filter(pk__in=request.data.get(\"ids\")) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): deployment = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, deployment.files.all()) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files) return Response(file_metric_dicts, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)', url_name=\"project_deployments\") def project_deployments(self, request, project_id=None): # Filter deployments based on the project primary key (project_id) deployment_qs = Deployment.objects.filter( project__pk=project_id) deployment_qs = self.filter_queryset(deployment_qs) if 'ctdp' in request.GET.keys(): deployment_qs = get_ctdp_deployment_qs(deployment_qs) # Paginate the queryset page = self.paginate_queryset(deployment_qs) if page is not None: deployment_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(deployment_serializer.data) # If no pagination, serialize all data deployment_serializer = self.get_serializer( deployment_qs, many=True, context={'request': request}) return Response(deployment_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)/queryset_count') def project_queryset_count(self, request, project_id, *args, **kwargs): queryset = Deployment.objects.filter( project__pk=project_id) queryset = self.filter_queryset(queryset) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'project/(?P<project_id>\\w+)/start_job/(?P<job_name>\\w+)') def project_start_job(self, request, project_id, job_name, *args, **kwargs): queryset = Deployment.objects.filter( project__pk=project_id) queryset = self.filter_queryset(queryset) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)', url_name=\"device_deployments\") def device_deployments(self, request, device_id=None): # Filter deployments based on the device primary key (device_id) deployment_qs = Deployment.objects.filter( device__pk=device_id) deployment_qs = self.filter_queryset(deployment_qs) if 'ctdp' in request.GET.keys(): deployment_qs = get_ctdp_deployment_qs(deployment_qs) # Paginate the queryset page = self.paginate_queryset(deployment_qs) if page is not None: deployment_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(deployment_serializer.data) # If no pagination, serialize all data deployment_serializer = self.get_serializer( deployment_qs, many=True, context={'request': request}) return Response(deployment_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)/queryset_count') def device_queryset_count(self, request, device_id, *args, **kwargs): queryset = Deployment.objects.filter( device__pk=device_id) queryset = self.filter_queryset(queryset) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'device/(?P<device_id>\\w+)/start_job/(?P<job_name>\\w+)') def device_start_job(self, request, device_id, job_name, *args, **kwargs): queryset = Deployment.objects.filter( device__pk=device_id) queryset = self.filter_queryset(queryset) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) DeviceModelViewSet Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for DeviceModel objects. Lists and retrieves sensor/device models, supporting search and filter. Results are paginated and cached for performance. Source code in data_models\\viewsets.py @extend_schema(summary=\"Device Model\", description=\"Models of sensors, which determine how data is handled.\", tags=[\"Device models\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List device models.', ), retrieve=extend_schema(summary='Get a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to get.\")]), partial_update=extend_schema(summary='Partially update a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to update.\")]), create=extend_schema(summary='Create device model', description=\"Add a device model\"), destroy=extend_schema(summary='Delete a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to delete.\")]), ) class DeviceModelViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for DeviceModel objects. Lists and retrieves sensor/device models, supporting search and filter. Results are paginated and cached for performance. \"\"\" serializer_class = DeviceModelSerializer queryset = DeviceModel.objects.all().distinct() search_fields = ['name'] filterset_class = DeviceModelFilter @method_decorator(cache_page(60 * 60 * 2)) def list(self, request): return super().list(request) DeviceViewSet Bases: AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Device objects. Supports CRUD operations, filtering, searching, and custom device-related actions. Enables bulk operations, job execution, and retrieval of device-level metrics. Main Features List, retrieve, and manage devices. Bulk count and job execution for device sets. Retrieve metrics for individual devices. Custom Actions ids_count, queryset_count, start_job: Bulk operations. metrics: Get metrics for a device. Source code in data_models\\viewsets.py @extend_schema(summary=\"Devices\", description=\"Database representation of sensors.\", tags=[\"Devices\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyDeviceSerializer, request=DummyDeviceSerializer ) @extend_schema_view( list=extend_schema(summary='List devices.', ), retrieve=extend_schema(summary='Get a single device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to get.\")]), update=extend_schema(summary='Update a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to update.\")]), partial_update=extend_schema(summary='Partially update a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to update.\")]), create=extend_schema(summary='Create a device'), destroy=extend_schema(summary='Delete a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered devices\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer) ) class DeviceViewSet(AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Device objects. Supports CRUD operations, filtering, searching, and custom device-related actions. Enables bulk operations, job execution, and retrieval of device-level metrics. Main Features: - List, retrieve, and manage devices. - Bulk count and job execution for device sets. - Retrieve metrics for individual devices. Custom Actions: - ids_count, queryset_count, start_job: Bulk operations. - metrics: Get metrics for a device. \"\"\" serializer_class = DeviceSerializer queryset = Device.objects.all().distinct() filterset_class = DeviceFilter search_fields = ['device_ID', 'name', 'model__name'] @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Device.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"device\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): device = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__device=device)) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files) return Response(file_metric_dicts, status=status.HTTP_200_OK) GenericJobViewSet Bases: ViewSet API endpoint for listing and retrieving available generic jobs. Allows users to list available jobs (with staff/admin filtering) and get job details. Source code in data_models\\viewsets.py @extend_schema(summary=\"Jobs\", description=\"Generic jobs that can be run on objects.\", tags=[\"Jobs\"], methods=[\"get\"], ) class GenericJobViewSet(viewsets.ViewSet): \"\"\" API endpoint for listing and retrieving available generic jobs. Allows users to list available jobs (with staff/admin filtering) and get job details. \"\"\" # Required for the Browsable API renderer to have a nice form. serializer_class = GenericJobSerializer permission_classes = [IsAuthenticated] def get_queryset(self): user = self.request.user job_list = settings.GENERIC_JOBS.values() if not user.is_staff: job_list = [x for x in job_list if not x[\"admin_only\"]] data_type = self.request.query_params.get('data_type') if data_type is not None: job_list = [x for x in job_list if x[\"data_type\"] == data_type] return job_list # @method_decorator(cache_page(60 * 60 * 2)) # @method_decorator(vary_on_cookie) def list(self, request): serializer = self.serializer_class( instance=self.get_queryset(), many=True) return Response(serializer.data) # @method_decorator(cache_page(60 * 60 * 2)) # @method_decorator(vary_on_cookie) def retrieve(self, request, pk=None): try: job_dict = list(settings.GENERIC_JOBS.values())[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(job_dict) return Response(serializer.data) ProjectViewSet Bases: AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Project objects. Provides CRUD operations, filtering, search, and specialized endpoints for projects. Supports counting, job execution, and retrieval of associated metrics and species lists. Main Features List, retrieve, and manage projects. Count files or deployments related to projects. Start jobs for selected projects. List unique species found in a project's data files. Retrieve file metrics for a project. Custom Actions ids_count, queryset_count, start_job: For bulk operations. species_list: Get unique species in project. metrics: Get project-level file metrics. Source code in data_models\\viewsets.py @extend_schema(summary=\"Projects\", description=\"Projects to organise collections of deployments and scientific work.\", tags=[\"Projects\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List projects.' ), retrieve=extend_schema(summary='Get a single project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to get.\")]), update=extend_schema(summary='Update an deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to update.\")]), partial_update=extend_schema(summary='Partially update a project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to update.\")]), create=extend_schema(summary='Create a project'), destroy=extend_schema(summary='Delete a project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer), species_list=extend_schema( summary=\"Get list of species\", responses=serializers.ListSerializer( child=serializers.CharField(default=\"Bufo bufo\"), many=False), filters=False, ), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), ) class ProjectViewSet(AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Project objects. Provides CRUD operations, filtering, search, and specialized endpoints for projects. Supports counting, job execution, and retrieval of associated metrics and species lists. Main Features: - List, retrieve, and manage projects. - Count files or deployments related to projects. - Start jobs for selected projects. - List unique species found in a project's data files. - Retrieve file metrics for a project. Custom Actions: - ids_count, queryset_count, start_job: For bulk operations. - species_list: Get unique species in project. - metrics: Get project-level file metrics. \"\"\" serializer_class = ProjectSerializer queryset = Project.objects.all().distinct().exclude( name=settings.GLOBAL_PROJECT_ID) filterset_class = ProjectFilter search_fields = ['project_ID', 'name', 'organisation'] @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Project.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"project\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get'], pagination_class=None) def species_list(self, request, pk=None): project = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__project=project)) if not data_files.exists(): return Response([], status=status.HTTP_200_OK) obs_obj = Observation.objects.filter(data_files__in=data_files) species_list = list(obs_obj.values_list( \"taxon__species_name\", flat=True).distinct()) return Response(species_list, status=status.HTTP_200_OK) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): project = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__project=project)) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files, False) return Response(file_metric_dicts, status=status.HTTP_200_OK) SiteViewSet Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for Site objects. Provides paginated, cached access to site/location definitions for deployments. Source code in data_models\\viewsets.py @extend_schema(summary=\"Site\", description=\"Locations where devices are deployed.\", tags=[\"Sites\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List sites.', ), retrieve=extend_schema(summary='Get a single site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to get.\")]), partial_update=extend_schema(summary='Partially update a site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to update.\")]), create=extend_schema(summary='Create site', description=\"Add a site\"), destroy=extend_schema(summary='Delete a site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to delete.\")]), ) class SiteViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for Site objects. Provides paginated, cached access to site/location definitions for deployments. \"\"\" serializer_class = SiteSerializer queryset = Site.objects.all().distinct() search_fields = ['name', 'short_name'] def list(self, request): return super().list(request)","title":"viewsets"},{"location":"reference/data_models/viewsets/#data_models.viewsets.DataFileViewSet","text":"Bases: CheckAttachmentViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing DataFile objects. Offers comprehensive CRUD operations, advanced filtering, search, and many custom actions. Includes endpoints for checking file existence, favorites, project/deployment/device-specific data files, and starting jobs on sets of files. Main Features List, retrieve, create (multi-upload), update, delete DataFiles. Advanced filter/search, including by taxon and tag. Favorite/unfavorite files, and list favorites for user or all users. Retrieve associated observations. Bulk operations (count, start jobs) on filtered/queryset results. Project, deployment, and device-specific filtering. Custom Actions check_existing: Check which files already exist. ids_count, queryset_count, start_job: Bulk operations. observations: List observations for a datafile. favourite_file: Toggle favorite status. deployment_datafiles, project_datafiles, device_datafiles: Scoped file queries. user_favourite_datafiles, favourited_datafiles: Favorites endpoints. Source code in data_models\\viewsets.py @extend_schema(summary=\"Data files\", description=\"Files recorded by sensors.\", tags=[\"DataFiles\"], methods=[\"get\", \"post\", \"patch\", \"delete\"], responses=DummyDataFileSerializer ) @extend_schema_view( list=extend_schema(summary='List datafiles.', parameters=[ctdp_parameter], ), retrieve=extend_schema(summary='Get a single datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to get.\")]), partial_update=extend_schema(summary='Partially update a datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to update.\")]), create=extend_schema(summary='Upload datafiles', request=DummyDataFileUploadSerializer, responses=inline_upload_response_serializer, description=\"Upload multiple datafiles or part of a lrger datafile\"), destroy=extend_schema(summary='Delete a datafile', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered devices\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), check_existing=extend_schema(summary=\"Check a list of filenames for files already in the database\", filters=True, request=DataFileCheckSerializer, responses=serializers.ListSerializer( child=serializers.CharField(default=\"myfile.jpg\"), many=False), ), project_datafiles=extend_schema(summary=\"Datafiles from project\", description=\"Get datafiles from a specific project.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get datafiles.\")]), deployment_datafiles=extend_schema(summary=\"Datafiles from deployment\", description=\"Get datafiles from a specific deployment.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment from which to get datafiles.\")]), device_datafiles=extend_schema(summary=\"Datafiles from device\", description=\"Get datafiles from a specific device.\", filters=True, parameters=[ ctdp_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get datafiles.\")]), user_favourite_datafiles=extend_schema(summary=\"User favourite datafiles\", description=\"Get datafiles favourited by the current user.\", filters=True, parameters=[ ctdp_parameter, ]), favourited_datafiles=extend_schema(summary=\"Favourited datafiles\", description=\"Get datafiles favourited by users.\", filters=True, parameters=[ ctdp_parameter, ]), favourite_file=extend_schema(exclude=True), observations=extend_schema(exclude=True), deployment_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Deployment\", description=\"Return the count of DataFile objects for a given deployment. Returns an integer.\", parameters=[ OpenApiParameter(\"deployment_pk\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Deployment ID\"), ], responses=OpenApiTypes.INT ), deployment_datafiles_start_job=extend_schema( summary=\"Start a Job on Deployment DataFiles\", description=\"Start a job on DataFiles for a given deployment.\", parameters=[ OpenApiParameter(\"deployment_pk\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Deployment ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], # or whatever serializer describes your POST body request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), project_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Project\", description=\"Return the count of DataFile objects for a given project. Returns an integer.\", parameters=[ OpenApiParameter(\"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Project ID\"), ], responses=OpenApiTypes.INT ), project_datafiles_start_job=extend_schema( summary=\"Start a Job on Project DataFiles\", description=\"Start a job on DataFiles for a given project.\", parameters=[ OpenApiParameter(\"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Project ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), device_datafiles_queryset_count=extend_schema( summary=\"Count DataFiles for Device\", description=\"Return the count of DataFile objects for a given device. Returns an integer.\", parameters=[ OpenApiParameter(\"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Device ID\"), ], responses=OpenApiTypes.INT ), device_datafiles_start_job=extend_schema( summary=\"Start a Job on Device DataFiles\", description=\"Start a job on DataFiles for a given device.\", parameters=[ OpenApiParameter(\"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Device ID\"), OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), user_favourite_datafiles_queryset_count=extend_schema( summary=\"Count User Favourite DataFiles\", description=\"Return the count of DataFile objects favourited by the current user. Returns an integer.\", responses=OpenApiTypes.INT ), user_favourite_datafiles_start_job=extend_schema( summary=\"Start a Job on User Favourite DataFiles\", description=\"Start a job on DataFiles favourited by the current user.\", parameters=[ OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), favourited_datafiles_queryset_count=extend_schema( summary=\"Count Favourited DataFiles\", description=\"Return the count of DataFile objects favourited by any user. Returns an integer.\", responses=OpenApiTypes.INT ), favourited_datafiles_start_job=extend_schema( summary=\"Start a Job on Favourited DataFiles\", description=\"Start a job on DataFiles favourited by any user.\", parameters=[ OpenApiParameter(\"job_name\", OpenApiTypes.STR, OpenApiParameter.PATH, description=\"Job name\"), ], request=inline_id_serializer_optional, responses=inline_job_start_serializer, ), ) class DataFileViewSet(CheckAttachmentViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing DataFile objects. Offers comprehensive CRUD operations, advanced filtering, search, and many custom actions. Includes endpoints for checking file existence, favorites, project/deployment/device-specific data files, and starting jobs on sets of files. Main Features: - List, retrieve, create (multi-upload), update, delete DataFiles. - Advanced filter/search, including by taxon and tag. - Favorite/unfavorite files, and list favorites for user or all users. - Retrieve associated observations. - Bulk operations (count, start jobs) on filtered/queryset results. - Project, deployment, and device-specific filtering. Custom Actions: - check_existing: Check which files already exist. - ids_count, queryset_count, start_job: Bulk operations. - observations: List observations for a datafile. - favourite_file: Toggle favorite status. - deployment_datafiles, project_datafiles, device_datafiles: Scoped file queries. - user_favourite_datafiles, favourited_datafiles: Favorites endpoints. \"\"\" http_method_names = ['get', 'patch', 'delete', 'post', 'head'] filterset_class = DataFileFilter search_fields = ['=tag', 'file_name', 'observations__taxon__species_name', 'observations__taxon__species_common_name'] def get_serializer_class(self): if self.action == 'create': return DataFileUploadSerializer else: if 'ctdp' in self.request.GET.keys(): return DataFileSerializerCTDP else: return DataFileSerializer def get_queryset(self): qs = DataFile.objects.prefetch_related(\"deployment\", \"observations__taxon\").all().distinct() if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_media_qs(qs) return qs @action(detail=False, methods=['post'], pagination_class=None) def check_existing(self, request, *args, **kwargs): queryset = perms['data_models.view_datafile'].filter( request.user, self.get_queryset()) serializer = DataFileCheckSerializer(data=request.data) if not serializer.is_valid(): return Response({\"detail\": serializer.errors}, status=status.HTTP_400_BAD_REQUEST) filter_params = request.data or request.GET if filter_params: queryfilter = self.filterset_class( filter_params, queryset=queryset) queryset = queryfilter.qs if (original_names := serializer.validated_data.get('original_names')): existing_names = queryset.filter( original_name__in=original_names).values_list('original_name', flat=True) existing_names = list(existing_names) missing_names = [ x for x in original_names if x not in existing_names] elif (file_names := serializer.validated_data.get('file_names')): existing_names = queryset.filter( file_name__in=file_names).values_list('file_name', flat=True) missing_names = [ x for x in original_names if x not in existing_names] else: return Response({\"detail\": \"Either 'original_names' or 'file_names' must be provided.\"}, status=status.HTTP_400_BAD_REQUEST) return Response(missing_names, status=status.HTTP_200_OK) @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = DataFile.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def observations(self, request, pk=None): data_file = DataFile.objects.get(pk=pk) # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files=data_file) # Paginate the queryset logger.info(self.request) page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = ObservationSerializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = ObservationSerializer( observation_qs, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK) @action(detail=True, methods=['post']) def favourite_file(self, request, pk=None): data_file = self.get_object() user = request.user if user: if data_file.favourite_of.all().filter(pk=user.pk).exists(): data_file.favourite_of.remove(user) else: data_file.favourite_of.add(user) return Response({}, status=status.HTTP_200_OK) else: return Response(status=status.HTTP_403_FORBIDDEN) def check_attachment(self, serializer): deployment_object = serializer.validated_data.get( 'deployment', serializer.instance.deployment) if not self.request.user.has_perm('data_models.change_deployment', deployment_object): raise PermissionDenied(f\"You don't have permission to add a datafile\" f\" to {deployment_object.deployment_device_ID}\") def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) headers = self.get_success_headers(serializer.validated_data) instance = serializer.validated_data files = instance.get('files') recording_dt = instance.get('recording_dt') extra_data = instance.get('extra_data') deployment_object = instance.get('deployment_object') device_object = instance.get('device_object') data_types = instance.get('data_types') check_filename = instance.get('check_filename') multipart = 'HTTP_CONTENT_RANGE' in request.META with transaction.atomic(), connection.cursor() as cursor: # Remove db limits during this function. cursor.execute('SET LOCAL statement_timeout TO 0;') uploaded_files, invalid_files, existing_files, status_code = create_file_objects( files, check_filename, recording_dt, extra_data, deployment_object, device_object, data_types, self.request.user, multipart) logger.info( f\"Uploaded files: {uploaded_files}, Invalid files: {invalid_files}, Existing files: {existing_files}, Status code: {status_code}\") if len(uploaded_files) > 0: returned_data = DataFileSerializer(data=uploaded_files, many=True) returned_data.is_valid() uploaded_files = returned_data.data return Response({\"uploaded_files\": uploaded_files, \"invalid_files\": invalid_files, \"existing_files\": existing_files}, status=status_code, headers=headers) # --- Deployment DataFiles --- @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_pk>\\w+)', url_name=\"deployment_datafiles\") def deployment_datafiles(self, request, deployment_pk=None): # Filter data files based on the deployment primary key (deployment_pk) data_file_qs = DataFile.objects.filter(deployment__pk=deployment_pk) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_pk>\\w+)/queryset_count') def deployment_datafiles_queryset_count(self, request, deployment_pk=None): queryset = self.filter_queryset( DataFile.objects.filter(deployment__pk=deployment_pk)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'deployment/(?P<deployment_pk>\\w+)/start_job/(?P<job_name>\\w+)') def deployment_datafiles_start_job(self, request, deployment_pk=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(deployment__pk=deployment_pk)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Project DataFiles --- @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)', url_name=\"project_datafiles\") def project_datafiles(self, request, project_id=None): # Filter data files based on the project primary key (project_id) through deployments data_file_qs = DataFile.objects.filter( deployment__project__pk=project_id) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)/queryset_count') def project_datafiles_queryset_count(self, request, project_id=None): queryset = self.filter_queryset( DataFile.objects.filter(project__pk=project_id)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'project/(?P<project_id>\\w+)/start_job/(?P<job_name>\\w+)') def project_datafiles_start_job(self, request, project_id=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(project__pk=project_id)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Device DataFiles --- @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)', url_name=\"device_datafiles\") def device_datafiles(self, request, device_id=None): # Filter data files based on the device primary key (device_id) through deployments data_file_qs = DataFile.objects.filter( deployment__device__pk=device_id) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)/queryset_count') def device_datafiles_queryset_count(self, request, device_id=None): queryset = self.filter_queryset( DataFile.objects.filter(device__pk=device_id)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'device/(?P<device_id>\\w+)/start_job/(?P<job_name>\\w+)') def device_datafiles_start_job(self, request, device_id=None, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(device__pk=device_id)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- User Favourite DataFiles --- @action(detail=False, methods=['get'], url_path=r'user', url_name=\"user_favourite_datafiles\") def user_favourite_datafiles(self, request): # Get the data files favorited by the current user user = request.user if not user.is_authenticated: return Response({\"detail\": \"Authentication credentials were not provided.\"}, status=status.HTTP_401_UNAUTHORIZED) data_file_qs = DataFile.objects.filter(favourite_of=user) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path='user_favourite/queryset_count') def user_favourite_datafiles_queryset_count(self, request): queryset = self.filter_queryset( DataFile.objects.filter(favourites=request.user)) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path='user_favourite/start_job/(?P<job_name>\\w+)') def user_favourite_datafiles_start_job(self, request, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(favourites=request.user)) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) # --- Favourited DataFiles (by any user) --- @action(detail=False, methods=['get'], url_path=r'highlights', url_name=\"highlight_datafiles\") def favourited_datafiles(self, request): user = request.user if not user.is_authenticated: return Response({\"detail\": \"Authentication credentials were not provided.\"}, status=status.HTTP_401_UNAUTHORIZED) # Get the data files where the favourite_of column is not null data_file_qs = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(favourite_of__isnull=False)) # Apply filters from URL query parameters data_file_qs = self.filter_queryset(data_file_qs) if 'ctdp' in request.GET.keys(): data_file_qs = get_ctdp_media_qs(data_file_qs) # Paginate the queryset page = self.paginate_queryset(data_file_qs) if page is not None: serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(serializer.data) # If no pagination, serialize all data serializer = self.get_serializer( data_file_qs, many=True, context={'request': request}) return Response(serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path='favourited/queryset_count') def favourited_datafiles_queryset_count(self, request): queryset = self.filter_queryset( DataFile.objects.filter(favourites__isnull=False).distinct()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path='favourited/start_job/(?P<job_name>\\w+)') def favourited_datafiles_start_job(self, request, job_name=None): queryset = self.filter_queryset( DataFile.objects.filter(favourites__isnull=False).distinct()) user_pk = request.user.pk obj_pks = request.data.get(\"ids\") or list( queryset.values_list('pk', flat=True)) if \"ids\" in request.data: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"datafile\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status)","title":"DataFileViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.DataTypeViewSet","text":"Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for DataType objects. Allows searching and filtering of data types (for devices or files). Results are paginated and cached for performance. Source code in data_models\\viewsets.py @extend_schema(summary=\"Data type\", description=\"Type of devices or of datafiles.\", tags=[\"Data type\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List data types.', ), retrieve=extend_schema(summary='Get a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to get.\")]), partial_update=extend_schema(summary='Partially update a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to update.\")]), create=extend_schema(summary='Create data type', description=\"Add a data type\"), destroy=extend_schema(summary='Delete a data type', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of data type to delete.\")]), ) class DataTypeViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for DataType objects. Allows searching and filtering of data types (for devices or files). Results are paginated and cached for performance. \"\"\" serializer_class = DataTypeSerializer queryset = DataType.objects.all().distinct() search_fields = ['name'] filterset_class = DataTypeFilter @method_decorator(cache_page(60 * 60 * 2)) def list(self, request): return super().list(request)","title":"DataTypeViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.DeploymentViewSet","text":"Bases: CheckAttachmentViewSetMixIn , AddOwnerViewSetMixIn , CheckFormViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Deployment objects. This viewset provides CRUD operations and custom actions for deployments of devices in the field. It includes filtering, searching, pagination, and project/device-specific endpoints. Main Features Filter, search, and order deployments. Retrieve deployments associated with specific projects or devices. Count deployments and start jobs on filtered sets. Retrieve metrics for a deployment. Enforces permission checks for project and device attachment. Custom Actions ids_count: Count deployments by list of IDs. queryset_count: Count deployments in filtered queryset. start_job: Start a job for deployments. metrics: Get deployment metrics. project/device-specific list, count, and job actions. Source code in data_models\\viewsets.py @extend_schema(summary=\"Deployments\", description=\"Deployments of devices in the field, with certain settings.\", tags=[\"Deployments\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyDeploymentSerializer, request=DummyDeploymentSerializer, ) @extend_schema_view( list=extend_schema(summary='List deployments.', parameters=[ctdp_parameter, geoJSON_parameter], ), retrieve=extend_schema(summary='Get a single deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to get.\")]), update=extend_schema(summary='Update an deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deplyment to update.\")]), partial_update=extend_schema(summary='Partially update a deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to update.\")]), create=extend_schema(summary='Create a deployment'), destroy=extend_schema(summary='Delete a deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment to delete.\")]), project_deployments=extend_schema(summary=\"Deployments from project\", description=\"Get deployments from a specific project.\", filters=True, parameters=[ctdp_parameter, geoJSON_parameter, OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), device_deployments=extend_schema(summary=\"Deployments from device\", description=\"Get deployments of a specific device.\", filters=True, parameters=[ctdp_parameter, geoJSON_parameter, OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), project_queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer, parameters=[OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), project_start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer, parameters=[OpenApiParameter( \"project_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project from which to get deployments.\")]), device_queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer, parameters=[OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]), device_start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer, parameters=[OpenApiParameter( \"device_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device from which to get deployments.\")]) ) class DeploymentViewSet(CheckAttachmentViewSetMixIn, AddOwnerViewSetMixIn, CheckFormViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Deployment objects. This viewset provides CRUD operations and custom actions for deployments of devices in the field. It includes filtering, searching, pagination, and project/device-specific endpoints. Main Features: - Filter, search, and order deployments. - Retrieve deployments associated with specific projects or devices. - Count deployments and start jobs on filtered sets. - Retrieve metrics for a deployment. - Enforces permission checks for project and device attachment. Custom Actions: - ids_count: Count deployments by list of IDs. - queryset_count: Count deployments in filtered queryset. - start_job: Start a job for deployments. - metrics: Get deployment metrics. - project/device-specific list, count, and job actions. \"\"\" search_fields = ['deployment_device_ID', 'device__name', 'device__device_ID', 'extra_data'] ordering_fields = ordering = [ 'deployment_device_ID', 'created_on', 'device_type'] queryset = Deployment.objects.all().distinct() filterset_class = DeploymentFilter filter_backends = viewsets.ModelViewSet.filter_backends + \\ [filters_gis.InBBoxFilter] def get_queryset(self): qs = Deployment.objects.all().distinct() if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_deployment_qs(qs) return qs def get_serializer_class(self): if 'geojson' in self.request.GET.keys(): return DeploymentSerializer_GeoJSON elif 'ctdp' in self.request.GET.keys(): return DeploymentSerializerCTDP else: return DeploymentSerializer def check_attachment(self, serializer): project_objects = serializer.validated_data.get('project') if project_objects is not None: for project_object in project_objects: if (not self.request.user.has_perm('data_models.change_project', project_object)) and \\ (project_object.name != settings.GLOBAL_PROJECT_ID): raise PermissionDenied( f\"You don't have permission to add a deployment to {project_object.project_ID}\") device_object = serializer.validated_data.get('device') if device_object is not None: if not self.request.user.has_perm('data_models.change_device', device_object): raise PermissionDenied( f\"You don't have permission to deploy {device_object.device_ID}\") @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Deployment.objects.filter(pk__in=request.data.get(\"ids\")) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): deployment = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, deployment.files.all()) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files) return Response(file_metric_dicts, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)', url_name=\"project_deployments\") def project_deployments(self, request, project_id=None): # Filter deployments based on the project primary key (project_id) deployment_qs = Deployment.objects.filter( project__pk=project_id) deployment_qs = self.filter_queryset(deployment_qs) if 'ctdp' in request.GET.keys(): deployment_qs = get_ctdp_deployment_qs(deployment_qs) # Paginate the queryset page = self.paginate_queryset(deployment_qs) if page is not None: deployment_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(deployment_serializer.data) # If no pagination, serialize all data deployment_serializer = self.get_serializer( deployment_qs, many=True, context={'request': request}) return Response(deployment_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'project/(?P<project_id>\\w+)/queryset_count') def project_queryset_count(self, request, project_id, *args, **kwargs): queryset = Deployment.objects.filter( project__pk=project_id) queryset = self.filter_queryset(queryset) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'project/(?P<project_id>\\w+)/start_job/(?P<job_name>\\w+)') def project_start_job(self, request, project_id, job_name, *args, **kwargs): queryset = Deployment.objects.filter( project__pk=project_id) queryset = self.filter_queryset(queryset) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)', url_name=\"device_deployments\") def device_deployments(self, request, device_id=None): # Filter deployments based on the device primary key (device_id) deployment_qs = Deployment.objects.filter( device__pk=device_id) deployment_qs = self.filter_queryset(deployment_qs) if 'ctdp' in request.GET.keys(): deployment_qs = get_ctdp_deployment_qs(deployment_qs) # Paginate the queryset page = self.paginate_queryset(deployment_qs) if page is not None: deployment_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(deployment_serializer.data) # If no pagination, serialize all data deployment_serializer = self.get_serializer( deployment_qs, many=True, context={'request': request}) return Response(deployment_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'device/(?P<device_id>\\w+)/queryset_count') def device_queryset_count(self, request, device_id, *args, **kwargs): queryset = Deployment.objects.filter( device__pk=device_id) queryset = self.filter_queryset(queryset) return Response({\"object_n\": queryset.count()}, status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'device/(?P<device_id>\\w+)/start_job/(?P<job_name>\\w+)') def device_start_job(self, request, device_id, job_name, *args, **kwargs): queryset = Deployment.objects.filter( device__pk=device_id) queryset = self.filter_queryset(queryset) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"deployment\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status)","title":"DeploymentViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.DeviceModelViewSet","text":"Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for DeviceModel objects. Lists and retrieves sensor/device models, supporting search and filter. Results are paginated and cached for performance. Source code in data_models\\viewsets.py @extend_schema(summary=\"Device Model\", description=\"Models of sensors, which determine how data is handled.\", tags=[\"Device models\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List device models.', ), retrieve=extend_schema(summary='Get a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to get.\")]), partial_update=extend_schema(summary='Partially update a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to update.\")]), create=extend_schema(summary='Create device model', description=\"Add a device model\"), destroy=extend_schema(summary='Delete a device model', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device model to delete.\")]), ) class DeviceModelViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for DeviceModel objects. Lists and retrieves sensor/device models, supporting search and filter. Results are paginated and cached for performance. \"\"\" serializer_class = DeviceModelSerializer queryset = DeviceModel.objects.all().distinct() search_fields = ['name'] filterset_class = DeviceModelFilter @method_decorator(cache_page(60 * 60 * 2)) def list(self, request): return super().list(request)","title":"DeviceModelViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.DeviceViewSet","text":"Bases: AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Device objects. Supports CRUD operations, filtering, searching, and custom device-related actions. Enables bulk operations, job execution, and retrieval of device-level metrics. Main Features List, retrieve, and manage devices. Bulk count and job execution for device sets. Retrieve metrics for individual devices. Custom Actions ids_count, queryset_count, start_job: Bulk operations. metrics: Get metrics for a device. Source code in data_models\\viewsets.py @extend_schema(summary=\"Devices\", description=\"Database representation of sensors.\", tags=[\"Devices\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyDeviceSerializer, request=DummyDeviceSerializer ) @extend_schema_view( list=extend_schema(summary='List devices.', ), retrieve=extend_schema(summary='Get a single device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to get.\")]), update=extend_schema(summary='Update a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to update.\")]), partial_update=extend_schema(summary='Partially update a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to update.\")]), create=extend_schema(summary='Create a device'), destroy=extend_schema(summary='Delete a device', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of device to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered devices\", filters=True, responses=inline_count_serializer), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer) ) class DeviceViewSet(AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Device objects. Supports CRUD operations, filtering, searching, and custom device-related actions. Enables bulk operations, job execution, and retrieval of device-level metrics. Main Features: - List, retrieve, and manage devices. - Bulk count and job execution for device sets. - Retrieve metrics for individual devices. Custom Actions: - ids_count, queryset_count, start_job: Bulk operations. - metrics: Get metrics for a device. \"\"\" serializer_class = DeviceSerializer queryset = Device.objects.all().distinct() filterset_class = DeviceFilter search_fields = ['device_ID', 'name', 'model__name'] @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Device.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"device\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): device = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__device=device)) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files) return Response(file_metric_dicts, status=status.HTTP_200_OK)","title":"DeviceViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.GenericJobViewSet","text":"Bases: ViewSet API endpoint for listing and retrieving available generic jobs. Allows users to list available jobs (with staff/admin filtering) and get job details. Source code in data_models\\viewsets.py @extend_schema(summary=\"Jobs\", description=\"Generic jobs that can be run on objects.\", tags=[\"Jobs\"], methods=[\"get\"], ) class GenericJobViewSet(viewsets.ViewSet): \"\"\" API endpoint for listing and retrieving available generic jobs. Allows users to list available jobs (with staff/admin filtering) and get job details. \"\"\" # Required for the Browsable API renderer to have a nice form. serializer_class = GenericJobSerializer permission_classes = [IsAuthenticated] def get_queryset(self): user = self.request.user job_list = settings.GENERIC_JOBS.values() if not user.is_staff: job_list = [x for x in job_list if not x[\"admin_only\"]] data_type = self.request.query_params.get('data_type') if data_type is not None: job_list = [x for x in job_list if x[\"data_type\"] == data_type] return job_list # @method_decorator(cache_page(60 * 60 * 2)) # @method_decorator(vary_on_cookie) def list(self, request): serializer = self.serializer_class( instance=self.get_queryset(), many=True) return Response(serializer.data) # @method_decorator(cache_page(60 * 60 * 2)) # @method_decorator(vary_on_cookie) def retrieve(self, request, pk=None): try: job_dict = list(settings.GENERIC_JOBS.values())[int(pk)] except (IndexError, ValueError): return Response(status=404) serializer = self.serializer_class(job_dict) return Response(serializer.data)","title":"GenericJobViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.ProjectViewSet","text":"Bases: AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn API endpoint for managing Project objects. Provides CRUD operations, filtering, search, and specialized endpoints for projects. Supports counting, job execution, and retrieval of associated metrics and species lists. Main Features List, retrieve, and manage projects. Count files or deployments related to projects. Start jobs for selected projects. List unique species found in a project's data files. Retrieve file metrics for a project. Custom Actions ids_count, queryset_count, start_job: For bulk operations. species_list: Get unique species in project. metrics: Get project-level file metrics. Source code in data_models\\viewsets.py @extend_schema(summary=\"Projects\", description=\"Projects to organise collections of deployments and scientific work.\", tags=[\"Projects\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List projects.' ), retrieve=extend_schema(summary='Get a single project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to get.\")]), update=extend_schema(summary='Update an deployment', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to update.\")]), partial_update=extend_schema(summary='Partially update a project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to update.\")]), create=extend_schema(summary='Create a project'), destroy=extend_schema(summary='Delete a project', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of project to delete.\")]), metrics=extend_schema(summary=\"Metrics\", description=\"Get metrics of specific object.\", responses=inline_metric_serialiser ), ids_count=extend_schema(summary=\"Count selected IDs\", request=inline_id_serializer, responses=inline_count_serializer), queryset_count=extend_schema(summary=\"Count filtered deployments\", filters=True, responses=inline_count_serializer), species_list=extend_schema( summary=\"Get list of species\", responses=serializers.ListSerializer( child=serializers.CharField(default=\"Bufo bufo\"), many=False), filters=False, ), start_job=extend_schema(summary=\"Start a job from these objects\", filters=True, request=inline_id_serializer_optional, responses=inline_job_start_serializer), ) class ProjectViewSet(AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" API endpoint for managing Project objects. Provides CRUD operations, filtering, search, and specialized endpoints for projects. Supports counting, job execution, and retrieval of associated metrics and species lists. Main Features: - List, retrieve, and manage projects. - Count files or deployments related to projects. - Start jobs for selected projects. - List unique species found in a project's data files. - Retrieve file metrics for a project. Custom Actions: - ids_count, queryset_count, start_job: For bulk operations. - species_list: Get unique species in project. - metrics: Get project-level file metrics. \"\"\" serializer_class = ProjectSerializer queryset = Project.objects.all().distinct().exclude( name=settings.GLOBAL_PROJECT_ID) filterset_class = ProjectFilter search_fields = ['project_ID', 'name', 'organisation'] @action(detail=False, methods=['post']) def ids_count(self, request, *args, **kwargs): queryset = Project.objects.filter(pk__in=request.data.get(\"ids\")) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['get']) def queryset_count(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) return Response(queryset.file_count(), status=status.HTTP_200_OK) @action(detail=False, methods=['post'], url_path=r'start_job/(?P<job_name>\\w+)') def start_job(self, request, job_name, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) user_pk = request.user.pk if not (obj_pks := request.data.get(\"ids\")): obj_pks = list(queryset.values_list('pk', flat=True)) else: request.data.pop(\"ids\") job_args = request.data success, detail, job_status = start_job_from_name( job_name, \"project\", obj_pks, job_args, user_pk) return Response({\"detail\": detail}, status=job_status) @action(detail=True, methods=['get'], pagination_class=None) def species_list(self, request, pk=None): project = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__project=project)) if not data_files.exists(): return Response([], status=status.HTTP_200_OK) obs_obj = Observation.objects.filter(data_files__in=data_files) species_list = list(obs_obj.values_list( \"taxon__species_name\", flat=True).distinct()) return Response(species_list, status=status.HTTP_200_OK) @action(detail=True, methods=['get']) def metrics(self, request, pk=None): project = self.get_object() user = request.user data_files = perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(deployment__project=project)) if not data_files.exists(): return Response({}, status=status.HTTP_200_OK) file_metric_dicts = get_all_file_metric_dicts(data_files, False) return Response(file_metric_dicts, status=status.HTTP_200_OK)","title":"ProjectViewSet"},{"location":"reference/data_models/viewsets/#data_models.viewsets.SiteViewSet","text":"Bases: ReadOnlyModelViewSet , OptionalPaginationViewSetMixIn Read-only API endpoint for Site objects. Provides paginated, cached access to site/location definitions for deployments. Source code in data_models\\viewsets.py @extend_schema(summary=\"Site\", description=\"Locations where devices are deployed.\", tags=[\"Sites\"], methods=[\"get\", \"put\", \"post\", \"patch\", \"delete\"], ) @extend_schema_view( list=extend_schema(summary='List sites.', ), retrieve=extend_schema(summary='Get a single site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to get.\")]), partial_update=extend_schema(summary='Partially update a site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to update.\")]), create=extend_schema(summary='Create site', description=\"Add a site\"), destroy=extend_schema(summary='Delete a site', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of site to delete.\")]), ) class SiteViewSet(viewsets.ReadOnlyModelViewSet, OptionalPaginationViewSetMixIn): \"\"\" Read-only API endpoint for Site objects. Provides paginated, cached access to site/location definitions for deployments. \"\"\" serializer_class = SiteSerializer queryset = Site.objects.all().distinct() search_fields = ['name', 'short_name'] def list(self, request): return super().list(request)","title":"SiteViewSet"},{"location":"reference/data_packages/","text":"This module is responsible for creating and managing data packages within the sensor portal.","title":"data_packages"},{"location":"reference/data_packages/create_zip_functions/","text":"create_zip(zip_name, file_objs, metadata_type, includes_files) Creates a zip archive containing data or media files along with associated metadata. Parameters: zip_name ( str ) \u2013 The base name of the zip file. \".zip\" will be appended if not present. file_objs ( QuerySet ) \u2013 A Django QuerySet of file objects to include in the zip. metadata_type ( int ) \u2013 Type of metadata to include. 0 = Standard metadata and observation metadata (metadata.json, observations.json). 1 = Camtrap DP metadata (media.csv, observations.csv, deployments.csv, events.csv, datapackage.json). includes_files ( bool ) \u2013 If True, only include files with local storage. Returns: Tuple [ bool , str ] \u2013 Tuple[bool, str]: - Success status (True if the zip was created, False otherwise). - Path to the created package directory (empty string if unsuccessful). Source code in data_packages\\create_zip_functions.py def create_zip( zip_name: str, file_objs: QuerySet, metadata_type: int, includes_files: bool ) -> Tuple[bool, str]: \"\"\" Creates a zip archive containing data or media files along with associated metadata. Args: zip_name (str): The base name of the zip file. \".zip\" will be appended if not present. file_objs (QuerySet): A Django QuerySet of file objects to include in the zip. metadata_type (int): Type of metadata to include. 0 = Standard metadata and observation metadata (metadata.json, observations.json). 1 = Camtrap DP metadata (media.csv, observations.csv, deployments.csv, events.csv, datapackage.json). includes_files (bool): If True, only include files with local storage. Returns: Tuple[bool, str]: - Success status (True if the zip was created, False otherwise). - Path to the created package directory (empty string if unsuccessful). \"\"\" if includes_files: file_objs = file_objs.filter(local_storage=True) package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) os.makedirs(package_path, exist_ok=True) if \".zip\" not in zip_name: zip_name = f\"{zip_name}.zip\" file_objs = file_objs.full_paths() match metadata_type: case 0: file_objs = file_objs.annotate(zip_path=Concat( Value('data'), Value(os.sep), F('relative_path'))) case 1: file_objs = file_objs.annotate( zip_path=Concat(Value('media'), Value(os.sep), F('relative_path'))) case _: return False, \"\" with ZipFile(os.path.join(package_path, zip_name), 'w') as zip_file: if (includes_files): for file_obj in file_objs: zip_file.write(file_obj.full_path, file_obj.zip_path) match metadata_type: case 0: metadata_dict = create_metadata_dict(file_objs) observation_dict = create_obs_metadata_dict( Observation.objects.filter(data_files__in=file_objs)) metadata_dict[\"data_files\"] = [ {**x, \"path\": os.path.join(\"data\", x[\"path\"])} for x in metadata_dict[\"data_files\"]] with zip_file.open(\"metadata.json\", \"w\") as f: f.write(json.dumps(metadata_dict, indent=2).encode(\"utf-8\")) with zip_file.open(\"observations.json\", \"w\") as f: f.write(json.dumps(observation_dict, indent=2).encode(\"utf-8\")) case 1: uuid = zip_name.split(\"_\")[0] file_df, observation_df, deploy_df, event_df, metadata = create_camtrap_dp_metadata( file_objs, uuid, zip_name) file_df[\"filePath\"] = [os.path.join( \"media\", x) for x in file_df[\"filePath\"]] with zip_file.open(\"media.csv\", \"w\") as df: file_df.to_csv(df, index=False) with zip_file.open(\"observations.csv\", \"w\") as df: observation_df.to_csv(df, index=False) with zip_file.open(\"deployments.csv\", \"w\") as df: deploy_df.to_csv(df, index=False) with zip_file.open(\"events.csv\", \"w\") as df: event_df.to_csv(df, index=False) with zip_file.open(\"datapackage.json\", \"w\") as f: f.write(json.dumps(metadata, indent=2).encode(\"utf-8\")) case _: return False, \"\" return True, package_path","title":"create_zip_functions"},{"location":"reference/data_packages/create_zip_functions/#data_packages.create_zip_functions.create_zip","text":"Creates a zip archive containing data or media files along with associated metadata. Parameters: zip_name ( str ) \u2013 The base name of the zip file. \".zip\" will be appended if not present. file_objs ( QuerySet ) \u2013 A Django QuerySet of file objects to include in the zip. metadata_type ( int ) \u2013 Type of metadata to include. 0 = Standard metadata and observation metadata (metadata.json, observations.json). 1 = Camtrap DP metadata (media.csv, observations.csv, deployments.csv, events.csv, datapackage.json). includes_files ( bool ) \u2013 If True, only include files with local storage. Returns: Tuple [ bool , str ] \u2013 Tuple[bool, str]: - Success status (True if the zip was created, False otherwise). - Path to the created package directory (empty string if unsuccessful). Source code in data_packages\\create_zip_functions.py def create_zip( zip_name: str, file_objs: QuerySet, metadata_type: int, includes_files: bool ) -> Tuple[bool, str]: \"\"\" Creates a zip archive containing data or media files along with associated metadata. Args: zip_name (str): The base name of the zip file. \".zip\" will be appended if not present. file_objs (QuerySet): A Django QuerySet of file objects to include in the zip. metadata_type (int): Type of metadata to include. 0 = Standard metadata and observation metadata (metadata.json, observations.json). 1 = Camtrap DP metadata (media.csv, observations.csv, deployments.csv, events.csv, datapackage.json). includes_files (bool): If True, only include files with local storage. Returns: Tuple[bool, str]: - Success status (True if the zip was created, False otherwise). - Path to the created package directory (empty string if unsuccessful). \"\"\" if includes_files: file_objs = file_objs.filter(local_storage=True) package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) os.makedirs(package_path, exist_ok=True) if \".zip\" not in zip_name: zip_name = f\"{zip_name}.zip\" file_objs = file_objs.full_paths() match metadata_type: case 0: file_objs = file_objs.annotate(zip_path=Concat( Value('data'), Value(os.sep), F('relative_path'))) case 1: file_objs = file_objs.annotate( zip_path=Concat(Value('media'), Value(os.sep), F('relative_path'))) case _: return False, \"\" with ZipFile(os.path.join(package_path, zip_name), 'w') as zip_file: if (includes_files): for file_obj in file_objs: zip_file.write(file_obj.full_path, file_obj.zip_path) match metadata_type: case 0: metadata_dict = create_metadata_dict(file_objs) observation_dict = create_obs_metadata_dict( Observation.objects.filter(data_files__in=file_objs)) metadata_dict[\"data_files\"] = [ {**x, \"path\": os.path.join(\"data\", x[\"path\"])} for x in metadata_dict[\"data_files\"]] with zip_file.open(\"metadata.json\", \"w\") as f: f.write(json.dumps(metadata_dict, indent=2).encode(\"utf-8\")) with zip_file.open(\"observations.json\", \"w\") as f: f.write(json.dumps(observation_dict, indent=2).encode(\"utf-8\")) case 1: uuid = zip_name.split(\"_\")[0] file_df, observation_df, deploy_df, event_df, metadata = create_camtrap_dp_metadata( file_objs, uuid, zip_name) file_df[\"filePath\"] = [os.path.join( \"media\", x) for x in file_df[\"filePath\"]] with zip_file.open(\"media.csv\", \"w\") as df: file_df.to_csv(df, index=False) with zip_file.open(\"observations.csv\", \"w\") as df: observation_df.to_csv(df, index=False) with zip_file.open(\"deployments.csv\", \"w\") as df: deploy_df.to_csv(df, index=False) with zip_file.open(\"events.csv\", \"w\") as df: event_df.to_csv(df, index=False) with zip_file.open(\"datapackage.json\", \"w\") as f: f.write(json.dumps(metadata, indent=2).encode(\"utf-8\")) case _: return False, \"\" return True, package_path","title":"create_zip"},{"location":"reference/data_packages/models/","text":"DataPackage Bases: BaseModel Model representing a data package, containing files and metadata for download or processing. Source code in data_packages\\models.py class DataPackage(BaseModel): \"\"\" Model representing a data package, containing files and metadata for download or processing. \"\"\" name = models.CharField( max_length=200, help_text=\"The name of the data package.\" ) data_files = models.ManyToManyField( DataFile, related_name=\"data_bundles\", help_text=\"The data files included in this package.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"data_bundles\", on_delete=models.SET_NULL, null=True, help_text=\"The user who owns this data package.\" ) status = models.IntegerField( choices=status, default=0, help_text=\"The current status of the data package.\" ) metadata_type = models.IntegerField( choices=metadata_type, default=0, help_text=\"The type of metadata associated with this package.\" ) includes_files = models.BooleanField( default=True, help_text=\"Whether the package includes files.\" ) file_url = models.CharField( max_length=500, blank=True, null=True, help_text=\"URL to download the zipped data package.\" ) def set_file_url(self) -> None: \"\"\" Set the file URL if the data package is ready. \"\"\" if self.status == 3: zip_name = self.name if \"zip\" not in zip_name: zip_name += \".zip\" self.file_url = os.path.normpath( os.path.join( settings.FILE_STORAGE_URL, settings.PACKAGE_PATH, zip_name ) ).replace(\"\\\\\", \"/\") else: self.file_url = None def __str__(self) -> str: \"\"\" Return the name of the data package. \"\"\" return self.name def make_zip(self) -> None: \"\"\" Create a zip archive of the data files and update status. \"\"\" create_zip(self.name, self.data_files, self.metadata_type, self.includes_files) self.status = 3 self.save() def save(self, *args, **kwargs) -> None: \"\"\" Override save to always update the file URL before saving. \"\"\" self.set_file_url() super().save(*args, **kwargs) def clean_data_package(self) -> bool: \"\"\" Remove the data package file from storage if ready or failed. Returns: bool: True if deletion was successful or unnecessary, False otherwise. \"\"\" if self.status == 3: package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) try_remove_file_clean_dirs( os.path.join(package_path, self.name+\"zip\")) return True elif self.status == 4: return True else: return False __str__() Return the name of the data package. Source code in data_packages\\models.py def __str__(self) -> str: \"\"\" Return the name of the data package. \"\"\" return self.name clean_data_package() Remove the data package file from storage if ready or failed. Returns: bool ( bool ) \u2013 True if deletion was successful or unnecessary, False otherwise. Source code in data_packages\\models.py def clean_data_package(self) -> bool: \"\"\" Remove the data package file from storage if ready or failed. Returns: bool: True if deletion was successful or unnecessary, False otherwise. \"\"\" if self.status == 3: package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) try_remove_file_clean_dirs( os.path.join(package_path, self.name+\"zip\")) return True elif self.status == 4: return True else: return False make_zip() Create a zip archive of the data files and update status. Source code in data_packages\\models.py def make_zip(self) -> None: \"\"\" Create a zip archive of the data files and update status. \"\"\" create_zip(self.name, self.data_files, self.metadata_type, self.includes_files) self.status = 3 self.save() save(*args, **kwargs) Override save to always update the file URL before saving. Source code in data_packages\\models.py def save(self, *args, **kwargs) -> None: \"\"\" Override save to always update the file URL before saving. \"\"\" self.set_file_url() super().save(*args, **kwargs) set_file_url() Set the file URL if the data package is ready. Source code in data_packages\\models.py def set_file_url(self) -> None: \"\"\" Set the file URL if the data package is ready. \"\"\" if self.status == 3: zip_name = self.name if \"zip\" not in zip_name: zip_name += \".zip\" self.file_url = os.path.normpath( os.path.join( settings.FILE_STORAGE_URL, settings.PACKAGE_PATH, zip_name ) ).replace(\"\\\\\", \"/\") else: self.file_url = None pre_remove_bundle(sender, instance, **kwargs) Signal handler to remove related files when a DataPackage is deleted. Raises: Exception \u2013 If the data package could not be removed. Source code in data_packages\\models.py @receiver(pre_delete, sender=DataPackage) def pre_remove_bundle(sender, instance: DataPackage, **kwargs) -> None: \"\"\" Signal handler to remove related files when a DataPackage is deleted. Raises: Exception: If the data package could not be removed. \"\"\" success = instance.clean_data_package() if not success: raise Exception(f\"Could not remove data package {instance.name}\")","title":"models"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage","text":"Bases: BaseModel Model representing a data package, containing files and metadata for download or processing. Source code in data_packages\\models.py class DataPackage(BaseModel): \"\"\" Model representing a data package, containing files and metadata for download or processing. \"\"\" name = models.CharField( max_length=200, help_text=\"The name of the data package.\" ) data_files = models.ManyToManyField( DataFile, related_name=\"data_bundles\", help_text=\"The data files included in this package.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"data_bundles\", on_delete=models.SET_NULL, null=True, help_text=\"The user who owns this data package.\" ) status = models.IntegerField( choices=status, default=0, help_text=\"The current status of the data package.\" ) metadata_type = models.IntegerField( choices=metadata_type, default=0, help_text=\"The type of metadata associated with this package.\" ) includes_files = models.BooleanField( default=True, help_text=\"Whether the package includes files.\" ) file_url = models.CharField( max_length=500, blank=True, null=True, help_text=\"URL to download the zipped data package.\" ) def set_file_url(self) -> None: \"\"\" Set the file URL if the data package is ready. \"\"\" if self.status == 3: zip_name = self.name if \"zip\" not in zip_name: zip_name += \".zip\" self.file_url = os.path.normpath( os.path.join( settings.FILE_STORAGE_URL, settings.PACKAGE_PATH, zip_name ) ).replace(\"\\\\\", \"/\") else: self.file_url = None def __str__(self) -> str: \"\"\" Return the name of the data package. \"\"\" return self.name def make_zip(self) -> None: \"\"\" Create a zip archive of the data files and update status. \"\"\" create_zip(self.name, self.data_files, self.metadata_type, self.includes_files) self.status = 3 self.save() def save(self, *args, **kwargs) -> None: \"\"\" Override save to always update the file URL before saving. \"\"\" self.set_file_url() super().save(*args, **kwargs) def clean_data_package(self) -> bool: \"\"\" Remove the data package file from storage if ready or failed. Returns: bool: True if deletion was successful or unnecessary, False otherwise. \"\"\" if self.status == 3: package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) try_remove_file_clean_dirs( os.path.join(package_path, self.name+\"zip\")) return True elif self.status == 4: return True else: return False","title":"DataPackage"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage.__str__","text":"Return the name of the data package. Source code in data_packages\\models.py def __str__(self) -> str: \"\"\" Return the name of the data package. \"\"\" return self.name","title":"__str__"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage.clean_data_package","text":"Remove the data package file from storage if ready or failed. Returns: bool ( bool ) \u2013 True if deletion was successful or unnecessary, False otherwise. Source code in data_packages\\models.py def clean_data_package(self) -> bool: \"\"\" Remove the data package file from storage if ready or failed. Returns: bool: True if deletion was successful or unnecessary, False otherwise. \"\"\" if self.status == 3: package_path = os.path.join( settings.FILE_STORAGE_ROOT, settings.PACKAGE_PATH) try_remove_file_clean_dirs( os.path.join(package_path, self.name+\"zip\")) return True elif self.status == 4: return True else: return False","title":"clean_data_package"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage.make_zip","text":"Create a zip archive of the data files and update status. Source code in data_packages\\models.py def make_zip(self) -> None: \"\"\" Create a zip archive of the data files and update status. \"\"\" create_zip(self.name, self.data_files, self.metadata_type, self.includes_files) self.status = 3 self.save()","title":"make_zip"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage.save","text":"Override save to always update the file URL before saving. Source code in data_packages\\models.py def save(self, *args, **kwargs) -> None: \"\"\" Override save to always update the file URL before saving. \"\"\" self.set_file_url() super().save(*args, **kwargs)","title":"save"},{"location":"reference/data_packages/models/#data_packages.models.DataPackage.set_file_url","text":"Set the file URL if the data package is ready. Source code in data_packages\\models.py def set_file_url(self) -> None: \"\"\" Set the file URL if the data package is ready. \"\"\" if self.status == 3: zip_name = self.name if \"zip\" not in zip_name: zip_name += \".zip\" self.file_url = os.path.normpath( os.path.join( settings.FILE_STORAGE_URL, settings.PACKAGE_PATH, zip_name ) ).replace(\"\\\\\", \"/\") else: self.file_url = None","title":"set_file_url"},{"location":"reference/data_packages/models/#data_packages.models.pre_remove_bundle","text":"Signal handler to remove related files when a DataPackage is deleted. Raises: Exception \u2013 If the data package could not be removed. Source code in data_packages\\models.py @receiver(pre_delete, sender=DataPackage) def pre_remove_bundle(sender, instance: DataPackage, **kwargs) -> None: \"\"\" Signal handler to remove related files when a DataPackage is deleted. Raises: Exception: If the data package could not be removed. \"\"\" success = instance.clean_data_package() if not success: raise Exception(f\"Could not remove data package {instance.name}\")","title":"pre_remove_bundle"},{"location":"reference/data_packages/serializers/","text":"","title":"serializers"},{"location":"reference/data_packages/tasks/","text":"fail_data_package_task(all_package_pks) Task to mark DataPackages as failed. Parameters: all_package_pks ( List [ int ] ) \u2013 List of DataPackage primary keys to mark as failed. Source code in data_packages\\tasks.py @app.task() def fail_data_package_task(all_package_pks: List[int]) -> None: \"\"\" Task to mark DataPackages as failed. Args: all_package_pks (List[int]): List of DataPackage primary keys to mark as failed. \"\"\" bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=4) make_data_package_task(all_package_pks) Task to finalize DataPackage creation and trigger zipping. Parameters: all_package_pks ( List [ int ] ) \u2013 List of DataPackage primary keys to process. Source code in data_packages\\tasks.py @app.task() def make_data_package_task(all_package_pks: List[int]) -> None: \"\"\" Task to finalize DataPackage creation and trigger zipping. Args: all_package_pks (List[int]): List of DataPackage primary keys to process. \"\"\" bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=2) for bundle_obj in bundle_objs: bundle_obj.make_zip() start_make_data_package_task(datafile_pks, user_pk, metadata_type=0, include_files=True) Task to create data packages from selected DataFile primary keys. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. user_pk ( int ) \u2013 Primary key for the User initiating the task. metadata_type ( int , default: 0 ) \u2013 Type of metadata to include in the package. Defaults to 0. include_files ( bool , default: True ) \u2013 Whether to include files in the package. Defaults to True. Source code in data_packages\\tasks.py @app.task(name=\"create_data_package\") @register_job( \"Create data package\", \"create_data_package\", \"datafile\", False, default_args={\"metadata_type\": \"0\", \"include_files\": True}, ) def start_make_data_package_task( datafile_pks: List[int], user_pk: int, metadata_type: int = 0, include_files: bool = True ) -> None: \"\"\" Task to create data packages from selected DataFile primary keys. Args: datafile_pks (List[int]): List of primary keys for DataFile objects. user_pk (int): Primary key for the User initiating the task. metadata_type (int, optional): Type of metadata to include in the package. Defaults to 0. include_files (bool, optional): Whether to include files in the package. Defaults to True. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) user = User.objects.get(pk=user_pk) file_objs = perms['data_models.view_datafile'].filter(user, file_objs) min_date = file_objs.min_date() min_date_str = min_date.strftime(\"%Y%m%d\") max_date = file_objs.max_date() max_date_str = max_date.strftime(\"%Y%m%d\") creation_dt = datetime.now().strftime(\"%Y%m%d_%H%M%S\") all_package_pks: List[int] = [] # if files, Split file objs by size, create all bundles if include_files: file_splits = group_files_by_size(file_objs) for suffix, group in enumerate(file_splits): bundle_file_objs = file_objs.filter(pk__in=group.get('file_pks')) uuid = str(uuid4()) bundle_name = \"_\".join([ uuid, user.username, min_date_str, max_date_str, creation_dt, str(suffix) ]) # Create bundle object new_file_bundle = DataPackage.objects.create( name=bundle_name, owner=user, metadata_type=metadata_type ) new_file_bundle.data_files.set(bundle_file_objs) all_package_pks.append(new_file_bundle.pk) archived_files = file_objs.filter(local_storage=False, archived=True) if archived_files.exists() and ( (not settings.ONLY_SUPER_UNARCHIVE) or (settings.ONLY_SUPER_UNARCHIVE and user.is_superuser) ): bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=1) archive_callback = make_data_package_task.si(all_package_pks).on_error( fail_data_package_task.si(all_package_pks) ) get_files_from_archive_task(datafile_pks, archive_callback) return else: make_data_package_task(all_package_pks) return else: uuid = str(uuid4()) # Note: 'suffix' is not defined in this scope, consider using '0' or another value bundle_name = \"_\".join([ uuid, user.username, min_date_str, max_date_str, creation_dt, \"0\" ]) new_file_bundle = DataPackage.objects.create( name=bundle_name, data_files=file_objs, owner=user, metadata_type=metadata_type, include_files=False ) all_package_pks.append(new_file_bundle.pk) # go straight to finish bundle job make_data_package_task(all_package_pks) return","title":"tasks"},{"location":"reference/data_packages/tasks/#data_packages.tasks.fail_data_package_task","text":"Task to mark DataPackages as failed. Parameters: all_package_pks ( List [ int ] ) \u2013 List of DataPackage primary keys to mark as failed. Source code in data_packages\\tasks.py @app.task() def fail_data_package_task(all_package_pks: List[int]) -> None: \"\"\" Task to mark DataPackages as failed. Args: all_package_pks (List[int]): List of DataPackage primary keys to mark as failed. \"\"\" bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=4)","title":"fail_data_package_task"},{"location":"reference/data_packages/tasks/#data_packages.tasks.make_data_package_task","text":"Task to finalize DataPackage creation and trigger zipping. Parameters: all_package_pks ( List [ int ] ) \u2013 List of DataPackage primary keys to process. Source code in data_packages\\tasks.py @app.task() def make_data_package_task(all_package_pks: List[int]) -> None: \"\"\" Task to finalize DataPackage creation and trigger zipping. Args: all_package_pks (List[int]): List of DataPackage primary keys to process. \"\"\" bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=2) for bundle_obj in bundle_objs: bundle_obj.make_zip()","title":"make_data_package_task"},{"location":"reference/data_packages/tasks/#data_packages.tasks.start_make_data_package_task","text":"Task to create data packages from selected DataFile primary keys. Parameters: datafile_pks ( List [ int ] ) \u2013 List of primary keys for DataFile objects. user_pk ( int ) \u2013 Primary key for the User initiating the task. metadata_type ( int , default: 0 ) \u2013 Type of metadata to include in the package. Defaults to 0. include_files ( bool , default: True ) \u2013 Whether to include files in the package. Defaults to True. Source code in data_packages\\tasks.py @app.task(name=\"create_data_package\") @register_job( \"Create data package\", \"create_data_package\", \"datafile\", False, default_args={\"metadata_type\": \"0\", \"include_files\": True}, ) def start_make_data_package_task( datafile_pks: List[int], user_pk: int, metadata_type: int = 0, include_files: bool = True ) -> None: \"\"\" Task to create data packages from selected DataFile primary keys. Args: datafile_pks (List[int]): List of primary keys for DataFile objects. user_pk (int): Primary key for the User initiating the task. metadata_type (int, optional): Type of metadata to include in the package. Defaults to 0. include_files (bool, optional): Whether to include files in the package. Defaults to True. \"\"\" file_objs = DataFile.objects.filter(pk__in=datafile_pks) user = User.objects.get(pk=user_pk) file_objs = perms['data_models.view_datafile'].filter(user, file_objs) min_date = file_objs.min_date() min_date_str = min_date.strftime(\"%Y%m%d\") max_date = file_objs.max_date() max_date_str = max_date.strftime(\"%Y%m%d\") creation_dt = datetime.now().strftime(\"%Y%m%d_%H%M%S\") all_package_pks: List[int] = [] # if files, Split file objs by size, create all bundles if include_files: file_splits = group_files_by_size(file_objs) for suffix, group in enumerate(file_splits): bundle_file_objs = file_objs.filter(pk__in=group.get('file_pks')) uuid = str(uuid4()) bundle_name = \"_\".join([ uuid, user.username, min_date_str, max_date_str, creation_dt, str(suffix) ]) # Create bundle object new_file_bundle = DataPackage.objects.create( name=bundle_name, owner=user, metadata_type=metadata_type ) new_file_bundle.data_files.set(bundle_file_objs) all_package_pks.append(new_file_bundle.pk) archived_files = file_objs.filter(local_storage=False, archived=True) if archived_files.exists() and ( (not settings.ONLY_SUPER_UNARCHIVE) or (settings.ONLY_SUPER_UNARCHIVE and user.is_superuser) ): bundle_objs = DataPackage.objects.filter(pk__in=all_package_pks) bundle_objs.update(status=1) archive_callback = make_data_package_task.si(all_package_pks).on_error( fail_data_package_task.si(all_package_pks) ) get_files_from_archive_task(datafile_pks, archive_callback) return else: make_data_package_task(all_package_pks) return else: uuid = str(uuid4()) # Note: 'suffix' is not defined in this scope, consider using '0' or another value bundle_name = \"_\".join([ uuid, user.username, min_date_str, max_date_str, creation_dt, \"0\" ]) new_file_bundle = DataPackage.objects.create( name=bundle_name, data_files=file_objs, owner=user, metadata_type=metadata_type, include_files=False ) all_package_pks.append(new_file_bundle.pk) # go straight to finish bundle job make_data_package_task(all_package_pks) return","title":"start_make_data_package_task"},{"location":"reference/data_packages/viewsets/","text":"","title":"viewsets"},{"location":"reference/external_storage_import/","text":"This module is responsible for importing external storage data into the sensor portal. It includes the necessary imports and configurations to handle data from various external sources.","title":"external_storage_import"},{"location":"reference/external_storage_import/filtersets/","text":"DataStorageFilter Bases: GenericFilterMixIn This filterset is used to filter DataStorageInput objects based on various fields. Source code in external_storage_import\\filtersets.py class DataStorageFilter(GenericFilterMixIn): \"\"\" This filterset is used to filter DataStorageInput objects based on various fields. \"\"\" project_id = NumberFilter( field_name='linked_projects', label='Project ID', lookup_expr='exact') class Meta: model = DataStorageInput fields = GenericFilterMixIn.get_fields().copy()","title":"filtersets"},{"location":"reference/external_storage_import/filtersets/#external_storage_import.filtersets.DataStorageFilter","text":"Bases: GenericFilterMixIn This filterset is used to filter DataStorageInput objects based on various fields. Source code in external_storage_import\\filtersets.py class DataStorageFilter(GenericFilterMixIn): \"\"\" This filterset is used to filter DataStorageInput objects based on various fields. \"\"\" project_id = NumberFilter( field_name='linked_projects', label='Project ID', lookup_expr='exact') class Meta: model = DataStorageInput fields = GenericFilterMixIn.get_fields().copy()","title":"DataStorageFilter"},{"location":"reference/external_storage_import/models/","text":"DataStorageInput Bases: BaseModel Model representing an external data storage input for sensor data import. Manages connection credentials and provides utilities for user and file management. Source code in external_storage_import\\models.py class DataStorageInput(BaseModel): \"\"\" Model representing an external data storage input for sensor data import. Manages connection credentials and provides utilities for user and file management. \"\"\" name = models.CharField( max_length=20, unique=True, help_text=\"A unique name for this data storage input.\" ) username = models.CharField( max_length=50, unique=True, help_text=\"Username for accessing the external storage.\" ) password = EncryptedCharField( max_length=128, help_text=\"Encrypted password for the storage username.\" ) address = models.CharField( max_length=100, unique=True, help_text=\"Network address (IP or hostname) of the external storage.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_inputstorages\", on_delete=models.SET_NULL, null=True, help_text=\"User who owns this data storage input.\" ) def __str__(self) -> str: \"\"\" Return the string representation of the storage input. \"\"\" return self.name def init_ssh_client(self) -> 'SSH_client': \"\"\" Initialize and return an SSH client for the storage input. \"\"\" return SSH_client(self.username, self.password, self.address, 22) def check_users_input(self) -> None: \"\"\" Set up users and check input storage for the linked devices. \"\"\" self.setup_users() self.check_input() def setup_users(self) -> None: \"\"\" Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ssh() # Get list of all current users status_code, stdout, stderr = ssh_client.send_ssh_command( 'cut -d: -f1 /etc/passwd') existing_users = stdout all_devices = self.linked_devices.all() required_users = all_devices.exclude( username=\"\", username__isnull=True).values('username', 'password') missing_users = [x for x in required_users if x['username'] not in existing_users] user_group_name = \"ftpuser\" # check main user group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep {user_group_name} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd {user_group_name}\", True) for user in missing_users: username = user[\"username\"] user_password = user[\"password\"] status_code, stdout, stderr = ssh_client.send_ssh_command( f\"perl -e 'print crypt('{user_password}', 'password')'\") encrypted_user_password = stdout[0] logger.info(f\"{self.name} - add user {username}\") # Add new user, add to ftpuser group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"useradd -m -p {encrypted_user_password}-N -g {user_group_name} {username}\", True) # Create a group for this user status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) # Add service account to the user group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # Prevent other users viewing the contents status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) for user in required_users: username = user[\"username\"] logger.info(f\"{self.name} - check user {username} permissions\") # Check that service account is in user's group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"id {self.username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # check if group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep g{username} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) if not (self.username in group_users): # If service account is not in the group, add it. status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Try to list user home directory status_code, stdout, stderr = ssh_client.send_ssh_command( f\"ls -ld /home/{username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # If unable to list directory, make sure group permissions are correct stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and main user to own it stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # prevent other users viewing the contents stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) ssh_client.close_connection_to_ftp() def check_connection(self) -> tuple[bool, 'SSH_client | None']: \"\"\" Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) \"\"\" try: ssh_client = self.init_ssh_client() return True, ssh_client except Exception as e: logger.info(f\"{self.name} - error\") logger.info(repr(e)) return False, None def check_input(self, remove_bad: bool = False) -> None: \"\"\" Check input files and folders for all linked devices. Optionally remove invalid files. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ftp() all_devices = self.linked_devices.all() all_dirs_attributes = ssh_client.ftp_sftp.listdir_attr() file_names = [x.filename for x in all_dirs_attributes] for device in all_devices: logger.info(f\"{self.name} - {device.device_ID} - checking storage\") if device.username is None: logger.info( f\"{self.name} - {device.device_ID} - configured incorrectly\") continue # check for folder of this device if device.username not in file_names: logger.info( f\"{self.name} - {device.device_ID} - not found on external storage\") continue # check for files device_dir_attribute = ssh_client.ftp_sftp.listdir_attr( device.username) device_file_names = [ x.filename for x in device_dir_attribute if all([y != '' for y in splitext(x.filename)])] if len(device_file_names) == 0: logger.info( f\"{self.name} - {device.device_ID} - no files on external storage\") continue files = [] for filename in device_file_names: try: with ssh_client.ftp_sftp.open(join(device.username, filename), bufsize=32768) as f: f.set_pipelined f.prefetch() f_bytes = io.BytesIO(f.read()) file_object = File(f_bytes, name=filename) files.append(file_object) except Exception as e: logger.error(e) # import files downloaded_files, invalid_files, existing_files, status = create_file_objects( files, device_object=device) logger.info(f\"{self.name} - {device.device_ID} - {status}\") # delete files that are succesfully downloaded for file_obj in downloaded_files: logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} succesfully downloaded\") if not settings.DEVMODE: ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") for problem_file in invalid_files: logger.info( f\"{self.name} - {device.device_ID} - {problem_file}\") if remove_bad: mtime = ssh_client.ftp_sftp.stat( join(device.username, filename)).st_mtime last_modified = datetime.fromtimestamp(mtime) if (datetime.now() - last_modified) <= timedelta(days=7): ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") ssh_client.close_connection_to_ftp() __str__() Return the string representation of the storage input. Source code in external_storage_import\\models.py def __str__(self) -> str: \"\"\" Return the string representation of the storage input. \"\"\" return self.name check_connection() Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) Source code in external_storage_import\\models.py def check_connection(self) -> tuple[bool, 'SSH_client | None']: \"\"\" Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) \"\"\" try: ssh_client = self.init_ssh_client() return True, ssh_client except Exception as e: logger.info(f\"{self.name} - error\") logger.info(repr(e)) return False, None check_input(remove_bad=False) Check input files and folders for all linked devices. Optionally remove invalid files. Source code in external_storage_import\\models.py def check_input(self, remove_bad: bool = False) -> None: \"\"\" Check input files and folders for all linked devices. Optionally remove invalid files. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ftp() all_devices = self.linked_devices.all() all_dirs_attributes = ssh_client.ftp_sftp.listdir_attr() file_names = [x.filename for x in all_dirs_attributes] for device in all_devices: logger.info(f\"{self.name} - {device.device_ID} - checking storage\") if device.username is None: logger.info( f\"{self.name} - {device.device_ID} - configured incorrectly\") continue # check for folder of this device if device.username not in file_names: logger.info( f\"{self.name} - {device.device_ID} - not found on external storage\") continue # check for files device_dir_attribute = ssh_client.ftp_sftp.listdir_attr( device.username) device_file_names = [ x.filename for x in device_dir_attribute if all([y != '' for y in splitext(x.filename)])] if len(device_file_names) == 0: logger.info( f\"{self.name} - {device.device_ID} - no files on external storage\") continue files = [] for filename in device_file_names: try: with ssh_client.ftp_sftp.open(join(device.username, filename), bufsize=32768) as f: f.set_pipelined f.prefetch() f_bytes = io.BytesIO(f.read()) file_object = File(f_bytes, name=filename) files.append(file_object) except Exception as e: logger.error(e) # import files downloaded_files, invalid_files, existing_files, status = create_file_objects( files, device_object=device) logger.info(f\"{self.name} - {device.device_ID} - {status}\") # delete files that are succesfully downloaded for file_obj in downloaded_files: logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} succesfully downloaded\") if not settings.DEVMODE: ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") for problem_file in invalid_files: logger.info( f\"{self.name} - {device.device_ID} - {problem_file}\") if remove_bad: mtime = ssh_client.ftp_sftp.stat( join(device.username, filename)).st_mtime last_modified = datetime.fromtimestamp(mtime) if (datetime.now() - last_modified) <= timedelta(days=7): ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") ssh_client.close_connection_to_ftp() check_users_input() Set up users and check input storage for the linked devices. Source code in external_storage_import\\models.py def check_users_input(self) -> None: \"\"\" Set up users and check input storage for the linked devices. \"\"\" self.setup_users() self.check_input() init_ssh_client() Initialize and return an SSH client for the storage input. Source code in external_storage_import\\models.py def init_ssh_client(self) -> 'SSH_client': \"\"\" Initialize and return an SSH client for the storage input. \"\"\" return SSH_client(self.username, self.password, self.address, 22) setup_users() Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. Source code in external_storage_import\\models.py def setup_users(self) -> None: \"\"\" Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ssh() # Get list of all current users status_code, stdout, stderr = ssh_client.send_ssh_command( 'cut -d: -f1 /etc/passwd') existing_users = stdout all_devices = self.linked_devices.all() required_users = all_devices.exclude( username=\"\", username__isnull=True).values('username', 'password') missing_users = [x for x in required_users if x['username'] not in existing_users] user_group_name = \"ftpuser\" # check main user group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep {user_group_name} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd {user_group_name}\", True) for user in missing_users: username = user[\"username\"] user_password = user[\"password\"] status_code, stdout, stderr = ssh_client.send_ssh_command( f\"perl -e 'print crypt('{user_password}', 'password')'\") encrypted_user_password = stdout[0] logger.info(f\"{self.name} - add user {username}\") # Add new user, add to ftpuser group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"useradd -m -p {encrypted_user_password}-N -g {user_group_name} {username}\", True) # Create a group for this user status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) # Add service account to the user group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # Prevent other users viewing the contents status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) for user in required_users: username = user[\"username\"] logger.info(f\"{self.name} - check user {username} permissions\") # Check that service account is in user's group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"id {self.username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # check if group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep g{username} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) if not (self.username in group_users): # If service account is not in the group, add it. status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Try to list user home directory status_code, stdout, stderr = ssh_client.send_ssh_command( f\"ls -ld /home/{username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # If unable to list directory, make sure group permissions are correct stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and main user to own it stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # prevent other users viewing the contents stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) ssh_client.close_connection_to_ftp()","title":"models"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput","text":"Bases: BaseModel Model representing an external data storage input for sensor data import. Manages connection credentials and provides utilities for user and file management. Source code in external_storage_import\\models.py class DataStorageInput(BaseModel): \"\"\" Model representing an external data storage input for sensor data import. Manages connection credentials and provides utilities for user and file management. \"\"\" name = models.CharField( max_length=20, unique=True, help_text=\"A unique name for this data storage input.\" ) username = models.CharField( max_length=50, unique=True, help_text=\"Username for accessing the external storage.\" ) password = EncryptedCharField( max_length=128, help_text=\"Encrypted password for the storage username.\" ) address = models.CharField( max_length=100, unique=True, help_text=\"Network address (IP or hostname) of the external storage.\" ) owner = models.ForeignKey( settings.AUTH_USER_MODEL, blank=True, related_name=\"owned_inputstorages\", on_delete=models.SET_NULL, null=True, help_text=\"User who owns this data storage input.\" ) def __str__(self) -> str: \"\"\" Return the string representation of the storage input. \"\"\" return self.name def init_ssh_client(self) -> 'SSH_client': \"\"\" Initialize and return an SSH client for the storage input. \"\"\" return SSH_client(self.username, self.password, self.address, 22) def check_users_input(self) -> None: \"\"\" Set up users and check input storage for the linked devices. \"\"\" self.setup_users() self.check_input() def setup_users(self) -> None: \"\"\" Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ssh() # Get list of all current users status_code, stdout, stderr = ssh_client.send_ssh_command( 'cut -d: -f1 /etc/passwd') existing_users = stdout all_devices = self.linked_devices.all() required_users = all_devices.exclude( username=\"\", username__isnull=True).values('username', 'password') missing_users = [x for x in required_users if x['username'] not in existing_users] user_group_name = \"ftpuser\" # check main user group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep {user_group_name} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd {user_group_name}\", True) for user in missing_users: username = user[\"username\"] user_password = user[\"password\"] status_code, stdout, stderr = ssh_client.send_ssh_command( f\"perl -e 'print crypt('{user_password}', 'password')'\") encrypted_user_password = stdout[0] logger.info(f\"{self.name} - add user {username}\") # Add new user, add to ftpuser group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"useradd -m -p {encrypted_user_password}-N -g {user_group_name} {username}\", True) # Create a group for this user status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) # Add service account to the user group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # Prevent other users viewing the contents status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) for user in required_users: username = user[\"username\"] logger.info(f\"{self.name} - check user {username} permissions\") # Check that service account is in user's group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"id {self.username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # check if group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep g{username} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) if not (self.username in group_users): # If service account is not in the group, add it. status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Try to list user home directory status_code, stdout, stderr = ssh_client.send_ssh_command( f\"ls -ld /home/{username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # If unable to list directory, make sure group permissions are correct stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and main user to own it stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # prevent other users viewing the contents stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) ssh_client.close_connection_to_ftp() def check_connection(self) -> tuple[bool, 'SSH_client | None']: \"\"\" Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) \"\"\" try: ssh_client = self.init_ssh_client() return True, ssh_client except Exception as e: logger.info(f\"{self.name} - error\") logger.info(repr(e)) return False, None def check_input(self, remove_bad: bool = False) -> None: \"\"\" Check input files and folders for all linked devices. Optionally remove invalid files. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ftp() all_devices = self.linked_devices.all() all_dirs_attributes = ssh_client.ftp_sftp.listdir_attr() file_names = [x.filename for x in all_dirs_attributes] for device in all_devices: logger.info(f\"{self.name} - {device.device_ID} - checking storage\") if device.username is None: logger.info( f\"{self.name} - {device.device_ID} - configured incorrectly\") continue # check for folder of this device if device.username not in file_names: logger.info( f\"{self.name} - {device.device_ID} - not found on external storage\") continue # check for files device_dir_attribute = ssh_client.ftp_sftp.listdir_attr( device.username) device_file_names = [ x.filename for x in device_dir_attribute if all([y != '' for y in splitext(x.filename)])] if len(device_file_names) == 0: logger.info( f\"{self.name} - {device.device_ID} - no files on external storage\") continue files = [] for filename in device_file_names: try: with ssh_client.ftp_sftp.open(join(device.username, filename), bufsize=32768) as f: f.set_pipelined f.prefetch() f_bytes = io.BytesIO(f.read()) file_object = File(f_bytes, name=filename) files.append(file_object) except Exception as e: logger.error(e) # import files downloaded_files, invalid_files, existing_files, status = create_file_objects( files, device_object=device) logger.info(f\"{self.name} - {device.device_ID} - {status}\") # delete files that are succesfully downloaded for file_obj in downloaded_files: logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} succesfully downloaded\") if not settings.DEVMODE: ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") for problem_file in invalid_files: logger.info( f\"{self.name} - {device.device_ID} - {problem_file}\") if remove_bad: mtime = ssh_client.ftp_sftp.stat( join(device.username, filename)).st_mtime last_modified = datetime.fromtimestamp(mtime) if (datetime.now() - last_modified) <= timedelta(days=7): ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") ssh_client.close_connection_to_ftp()","title":"DataStorageInput"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.__str__","text":"Return the string representation of the storage input. Source code in external_storage_import\\models.py def __str__(self) -> str: \"\"\" Return the string representation of the storage input. \"\"\" return self.name","title":"__str__"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.check_connection","text":"Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) Source code in external_storage_import\\models.py def check_connection(self) -> tuple[bool, 'SSH_client | None']: \"\"\" Attempt to initialize an SSH client for this storage input. Returns a tuple: (success, SSH_client or None) \"\"\" try: ssh_client = self.init_ssh_client() return True, ssh_client except Exception as e: logger.info(f\"{self.name} - error\") logger.info(repr(e)) return False, None","title":"check_connection"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.check_input","text":"Check input files and folders for all linked devices. Optionally remove invalid files. Source code in external_storage_import\\models.py def check_input(self, remove_bad: bool = False) -> None: \"\"\" Check input files and folders for all linked devices. Optionally remove invalid files. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ftp() all_devices = self.linked_devices.all() all_dirs_attributes = ssh_client.ftp_sftp.listdir_attr() file_names = [x.filename for x in all_dirs_attributes] for device in all_devices: logger.info(f\"{self.name} - {device.device_ID} - checking storage\") if device.username is None: logger.info( f\"{self.name} - {device.device_ID} - configured incorrectly\") continue # check for folder of this device if device.username not in file_names: logger.info( f\"{self.name} - {device.device_ID} - not found on external storage\") continue # check for files device_dir_attribute = ssh_client.ftp_sftp.listdir_attr( device.username) device_file_names = [ x.filename for x in device_dir_attribute if all([y != '' for y in splitext(x.filename)])] if len(device_file_names) == 0: logger.info( f\"{self.name} - {device.device_ID} - no files on external storage\") continue files = [] for filename in device_file_names: try: with ssh_client.ftp_sftp.open(join(device.username, filename), bufsize=32768) as f: f.set_pipelined f.prefetch() f_bytes = io.BytesIO(f.read()) file_object = File(f_bytes, name=filename) files.append(file_object) except Exception as e: logger.error(e) # import files downloaded_files, invalid_files, existing_files, status = create_file_objects( files, device_object=device) logger.info(f\"{self.name} - {device.device_ID} - {status}\") # delete files that are succesfully downloaded for file_obj in downloaded_files: logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} succesfully downloaded\") if not settings.DEVMODE: ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") for problem_file in invalid_files: logger.info( f\"{self.name} - {device.device_ID} - {problem_file}\") if remove_bad: mtime = ssh_client.ftp_sftp.stat( join(device.username, filename)).st_mtime last_modified = datetime.fromtimestamp(mtime) if (datetime.now() - last_modified) <= timedelta(days=7): ssh_client.ftp_sftp.remove( join(device.username, file_obj.original_name)) logger.info( f\"{self.name} - {device.device_ID} - {file_obj.original_name} removed\") ssh_client.close_connection_to_ftp()","title":"check_input"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.check_users_input","text":"Set up users and check input storage for the linked devices. Source code in external_storage_import\\models.py def check_users_input(self) -> None: \"\"\" Set up users and check input storage for the linked devices. \"\"\" self.setup_users() self.check_input()","title":"check_users_input"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.init_ssh_client","text":"Initialize and return an SSH client for the storage input. Source code in external_storage_import\\models.py def init_ssh_client(self) -> 'SSH_client': \"\"\" Initialize and return an SSH client for the storage input. \"\"\" return SSH_client(self.username, self.password, self.address, 22)","title":"init_ssh_client"},{"location":"reference/external_storage_import/models/#external_storage_import.models.DataStorageInput.setup_users","text":"Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. Source code in external_storage_import\\models.py def setup_users(self) -> None: \"\"\" Ensure all required users for devices exist on the external storage. Set up necessary user accounts and permissions. \"\"\" connection_success, ssh_client = self.check_connection() if not connection_success: logger.info(f\"{self.name} - unable to connect\") return ssh_client.connect_to_ssh() # Get list of all current users status_code, stdout, stderr = ssh_client.send_ssh_command( 'cut -d: -f1 /etc/passwd') existing_users = stdout all_devices = self.linked_devices.all() required_users = all_devices.exclude( username=\"\", username__isnull=True).values('username', 'password') missing_users = [x for x in required_users if x['username'] not in existing_users] user_group_name = \"ftpuser\" # check main user group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep {user_group_name} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd {user_group_name}\", True) for user in missing_users: username = user[\"username\"] user_password = user[\"password\"] status_code, stdout, stderr = ssh_client.send_ssh_command( f\"perl -e 'print crypt('{user_password}', 'password')'\") encrypted_user_password = stdout[0] logger.info(f\"{self.name} - add user {username}\") # Add new user, add to ftpuser group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"useradd -m -p {encrypted_user_password}-N -g {user_group_name} {username}\", True) # Create a group for this user status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) # Add service account to the user group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and service account to own it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # Prevent other users viewing the contents status_code, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) for user in required_users: username = user[\"username\"] logger.info(f\"{self.name} - check user {username} permissions\") # Check that service account is in user's group status_code, stdout, stderr = ssh_client.send_ssh_command( f\"id {self.username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # check if group exists status_code, stdout, stderr = ssh_client.send_ssh_command( f\"grep g{username} /etc/group\") group_users = stdout if len(group_users) == 0: # If group does not exist, create it status_code, stdout, stderr = ssh_client.send_ssh_command( f\"groupadd g{username}\", True) if not (self.username in group_users): # If service account is not in the group, add it. status_code, stdout, stderr = ssh_client.send_ssh_command( f\"usermod -a -G g{username} {self.username}\", True) # Try to list user home directory status_code, stdout, stderr = ssh_client.send_ssh_command( f\"ls -ld /home/{username} | grep -c g{username}\") grep_result = int(stdout[0]) if grep_result == 0: # If unable to list directory, make sure group permissions are correct stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chown {username}:g{username} /home/{username}\", True) # allow user and main user to own it stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R g+srwx /home/{username}\", True) # prevent other users viewing the contents stdin, stdout, stderr = ssh_client.send_ssh_command( f\"chmod -R o-rwx /home/{username}\", True) ssh_client.close_connection_to_ftp()","title":"setup_users"},{"location":"reference/external_storage_import/serializers/","text":"DataStorageInputSerializer Bases: OwnerMixIn , CreatedModifiedMixIn , ModelSerializer Serializer for DataStorageInput model. Handles serialization and deserialization of DataStorageInput objects, including custom field visibility based on user permissions. Source code in external_storage_import\\serializers.py class DataStorageInputSerializer(OwnerMixIn, CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for DataStorageInput model. Handles serialization and deserialization of DataStorageInput objects, including custom field visibility based on user permissions. \"\"\" username = serializers.CharField( required=False, help_text=\"Optional username for accessing the external data storage.\" ) password = serializers.CharField( required=False, write_only=True, help_text=\"Optional password for accessing the external data storage. Write-only for security.\" ) class Meta: \"\"\" Meta information for DataStorageInputSerializer. \"\"\" model = DataStorageInput exclude = [] def __init__(self, *args, **kwargs): \"\"\" Initializes the serializer and sets the required management permission. \"\"\" self.management_perm = 'data_models.change_datastorageinput' super().__init__(*args, **kwargs) def to_representation(self, instance): \"\"\" Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. \"\"\" initial_rep = super().to_representation(instance) fields_to_pop = [ \"username\", \"address\" ] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep Meta Meta information for DataStorageInputSerializer. Source code in external_storage_import\\serializers.py class Meta: \"\"\" Meta information for DataStorageInputSerializer. \"\"\" model = DataStorageInput exclude = [] __init__(*args, **kwargs) Initializes the serializer and sets the required management permission. Source code in external_storage_import\\serializers.py def __init__(self, *args, **kwargs): \"\"\" Initializes the serializer and sets the required management permission. \"\"\" self.management_perm = 'data_models.change_datastorageinput' super().__init__(*args, **kwargs) to_representation(instance) Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. Source code in external_storage_import\\serializers.py def to_representation(self, instance): \"\"\" Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. \"\"\" initial_rep = super().to_representation(instance) fields_to_pop = [ \"username\", \"address\" ] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep","title":"serializers"},{"location":"reference/external_storage_import/serializers/#external_storage_import.serializers.DataStorageInputSerializer","text":"Bases: OwnerMixIn , CreatedModifiedMixIn , ModelSerializer Serializer for DataStorageInput model. Handles serialization and deserialization of DataStorageInput objects, including custom field visibility based on user permissions. Source code in external_storage_import\\serializers.py class DataStorageInputSerializer(OwnerMixIn, CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for DataStorageInput model. Handles serialization and deserialization of DataStorageInput objects, including custom field visibility based on user permissions. \"\"\" username = serializers.CharField( required=False, help_text=\"Optional username for accessing the external data storage.\" ) password = serializers.CharField( required=False, write_only=True, help_text=\"Optional password for accessing the external data storage. Write-only for security.\" ) class Meta: \"\"\" Meta information for DataStorageInputSerializer. \"\"\" model = DataStorageInput exclude = [] def __init__(self, *args, **kwargs): \"\"\" Initializes the serializer and sets the required management permission. \"\"\" self.management_perm = 'data_models.change_datastorageinput' super().__init__(*args, **kwargs) def to_representation(self, instance): \"\"\" Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. \"\"\" initial_rep = super().to_representation(instance) fields_to_pop = [ \"username\", \"address\" ] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep","title":"DataStorageInputSerializer"},{"location":"reference/external_storage_import/serializers/#external_storage_import.serializers.DataStorageInputSerializer.Meta","text":"Meta information for DataStorageInputSerializer. Source code in external_storage_import\\serializers.py class Meta: \"\"\" Meta information for DataStorageInputSerializer. \"\"\" model = DataStorageInput exclude = []","title":"Meta"},{"location":"reference/external_storage_import/serializers/#external_storage_import.serializers.DataStorageInputSerializer.__init__","text":"Initializes the serializer and sets the required management permission. Source code in external_storage_import\\serializers.py def __init__(self, *args, **kwargs): \"\"\" Initializes the serializer and sets the required management permission. \"\"\" self.management_perm = 'data_models.change_datastorageinput' super().__init__(*args, **kwargs)","title":"__init__"},{"location":"reference/external_storage_import/serializers/#external_storage_import.serializers.DataStorageInputSerializer.to_representation","text":"Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. Source code in external_storage_import\\serializers.py def to_representation(self, instance): \"\"\" Customize the representation of the DataStorageInput instance. Removes sensitive fields (username, address) from the output unless the user has management permissions. \"\"\" initial_rep = super().to_representation(instance) fields_to_pop = [ \"username\", \"address\" ] if self.context.get('request'): user_is_manager = self.context['request'].user.has_perm( self.management_perm, obj=instance) else: user_is_manager = False if not user_is_manager: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep","title":"to_representation"},{"location":"reference/external_storage_import/tasks/","text":"check_external_storage_all_task(storage_pk) Celery task to perform a full check on a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance to check. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_all_task(storage_pk: int): \"\"\" Celery task to perform a full check on a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance to check. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.check() check_external_storage_input_task(storage_pk) Celery task to check the input for a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance to check. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_input_task(storage_pk: int): \"\"\" Celery task to check the input for a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance to check. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.check_input() check_external_storage_users_task(storage_pk) Celery task to set up users for a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance for user setup. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_users_task(storage_pk: int): \"\"\" Celery task to set up users for a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance for user setup. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.setup_users()","title":"tasks"},{"location":"reference/external_storage_import/tasks/#external_storage_import.tasks.check_external_storage_all_task","text":"Celery task to perform a full check on a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance to check. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_all_task(storage_pk: int): \"\"\" Celery task to perform a full check on a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance to check. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.check()","title":"check_external_storage_all_task"},{"location":"reference/external_storage_import/tasks/#external_storage_import.tasks.check_external_storage_input_task","text":"Celery task to check the input for a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance to check. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_input_task(storage_pk: int): \"\"\" Celery task to check the input for a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance to check. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.check_input()","title":"check_external_storage_input_task"},{"location":"reference/external_storage_import/tasks/#external_storage_import.tasks.check_external_storage_users_task","text":"Celery task to set up users for a specific DataStorageInput instance. Parameters: storage_pk ( int ) \u2013 Primary key of the DataStorageInput instance for user setup. Source code in external_storage_import\\tasks.py @app.task() def check_external_storage_users_task(storage_pk: int): \"\"\" Celery task to set up users for a specific DataStorageInput instance. Args: storage_pk (int): Primary key of the DataStorageInput instance for user setup. \"\"\" from .models import DataStorageInput storage = DataStorageInput.objects.get(pk=storage_pk) storage.setup_users()","title":"check_external_storage_users_task"},{"location":"reference/external_storage_import/viewsets/","text":"DataStorageInputViewSet Bases: AddOwnerViewSetMixIn A ViewSet for managing DataStorageInput instances, allowing users to list, retrieve, and search for external storage inputs. Source code in external_storage_import\\viewsets.py @extend_schema(exclude=True) class DataStorageInputViewSet(AddOwnerViewSetMixIn): \"\"\" A ViewSet for managing DataStorageInput instances, allowing users to list, retrieve, and search for external storage inputs. \"\"\" serializer_class = DataStorageInputSerializer queryset = DataStorageInput.objects.all().distinct() search_fields = ['name'] filterset_class = DataStorageFilter","title":"viewsets"},{"location":"reference/external_storage_import/viewsets/#external_storage_import.viewsets.DataStorageInputViewSet","text":"Bases: AddOwnerViewSetMixIn A ViewSet for managing DataStorageInput instances, allowing users to list, retrieve, and search for external storage inputs. Source code in external_storage_import\\viewsets.py @extend_schema(exclude=True) class DataStorageInputViewSet(AddOwnerViewSetMixIn): \"\"\" A ViewSet for managing DataStorageInput instances, allowing users to list, retrieve, and search for external storage inputs. \"\"\" serializer_class = DataStorageInputSerializer queryset = DataStorageInput.objects.all().distinct() search_fields = ['name'] filterset_class = DataStorageFilter","title":"DataStorageInputViewSet"},{"location":"reference/observation_editor/","text":"This module provides functionality for creating observations and their unnerlyng taxonomy in the sensor portal.","title":"observation_editor"},{"location":"reference/observation_editor/GBIF_functions/","text":"GBIF_get_or_create_taxon_object_from_taxon_code(taxon_code) Retrieve or create a Taxon model object from a GBIF taxon code. Parameters: taxon_code ( int ) \u2013 Integer GBIF taxon code. Returns: Tuple [ Any , bool ] \u2013 Tuple of (Taxon object, boolean indicating if it was created). Source code in observation_editor\\GBIF_functions.py def GBIF_get_or_create_taxon_object_from_taxon_code( taxon_code: int ) -> Tuple[Any, bool]: \"\"\" Retrieve or create a Taxon model object from a GBIF taxon code. Args: taxon_code: Integer GBIF taxon code. Returns: Tuple of (Taxon object, boolean indicating if it was created). \"\"\" from .models import Taxon species_data = GBIF_get_species(taxon_code) all_taxon_codes = GBIF_get_taxoncodes(species_data) try: # Try to get or create the Taxon object taxon_obj, created = Taxon.objects.get_or_create( species_name=species_data.get(\"canonicalName\"), species_common_name=species_data.get(\"vernacularName\", \"\"), taxon_source=1, taxon_code=all_taxon_codes[0][0], taxonomic_level=all_taxon_codes[0][1] ) except MultipleObjectsReturned: # If multiple objects exist, just return the first one taxon_obj = Taxon.objects.filter( species_name=species_data.get(\"canonicalName\"), species_common_name=species_data.get(\"vernacularName\", \"\"), taxon_source=1, taxon_code=all_taxon_codes[0][0], taxonomic_level=all_taxon_codes[0][1] ).first() created = False return taxon_obj, created GBIF_get_species(species_key) Retrieve a species' detailed metadata from GBIF by its key. Parameters: species_key ( int ) \u2013 Integer GBIF species key. Returns: Dict [ str , Any ] \u2013 Dictionary of species metadata returned by GBIF. Source code in observation_editor\\GBIF_functions.py def GBIF_get_species( species_key: int ) -> Dict[str, Any]: \"\"\" Retrieve a species' detailed metadata from GBIF by its key. Args: species_key: Integer GBIF species key. Returns: Dictionary of species metadata returned by GBIF. \"\"\" gbif_response = requests.get( f\"https://api.gbif.org/v1/species/{species_key}\") gbif_data = gbif_response.json() return gbif_data GBIF_get_taxoncodes(gbif_data) Extract all available taxonomic codes and their levels from a GBIF result. Parameters: gbif_data ( Dict [ str , Any ] ) \u2013 Dictionary containing GBIF taxon data. Returns: List [ Tuple [ int , int ]] \u2013 List of (taxon code, taxonomic level) tuples, ordered from species up to kingdom. Source code in observation_editor\\GBIF_functions.py def GBIF_get_taxoncodes( gbif_data: Dict[str, Any] ) -> List[Tuple[int, int]]: \"\"\" Extract all available taxonomic codes and their levels from a GBIF result. Args: gbif_data: Dictionary containing GBIF taxon data. Returns: List of (taxon code, taxonomic level) tuples, ordered from species up to kingdom. \"\"\" # List of GBIF taxonomic ranks, ordered from most specific to most general rank_keys = [ 'speciesKey', 'genusKey', 'familyKey', 'orderKey', 'classKey', 'phylumKey', 'kingdomKey' ] all_keys: List[Tuple[int, int]] = [] for taxonomic_level, rank_key in enumerate(rank_keys): curr_key = gbif_data.get(rank_key, None) if curr_key is not None: # Validate the key by checking for major issues in GBIF data species_data = GBIF_get_species(curr_key) issues = species_data.get('issues', []) if any([x in issues for x in ['NO_SPECIES']]): continue all_keys.append((curr_key, taxonomic_level)) # Handle special case where the result is a subspecies: use its own key if gbif_data.get('rank') == 'SUBSPECIES' and all_keys: all_keys[0] = (gbif_data.get('key'), 0) return all_keys GBIF_result_match(search_string, gbif_result) Perform fuzzy matching between a search string and a single GBIF API result. Parameters: search_string ( str ) \u2013 User input string to search for (e.g., species name). gbif_result ( Dict [ str , Any ] ) \u2013 A dictionary representing a single species result from GBIF. Returns: Dict [ str , Optional [ Union [ int , float ]]] \u2013 A dictionary containing max scientific and vernacular name scores, Dict [ str , Optional [ Union [ int , float ]]] \u2013 median score over all name types, and count of scores > 50. Source code in observation_editor\\GBIF_functions.py def GBIF_result_match( search_string: str, gbif_result: Dict[str, Any] ) -> Dict[str, Optional[Union[int, float]]]: \"\"\" Perform fuzzy matching between a search string and a single GBIF API result. Args: search_string: User input string to search for (e.g., species name). gbif_result: A dictionary representing a single species result from GBIF. Returns: A dictionary containing max scientific and vernacular name scores, median score over all name types, and count of scores > 50. \"\"\" # Normalize the search string for more robust comparison search_string = search_string.lower() keys_to_search = ['scientificName', 'canonicalName'] # Fuzzy match against both scientific and canonical names scores = [ fuzz.ratio(search_string, gbif_result.get(x, \"\").lower()) for x in keys_to_search ] # Fuzzy match against all vernacular (common) names vernacular_scores = [ fuzz.ratio(search_string, x['vernacularName'].lower()) for x in gbif_result.get('vernacularNames', []) ] # Find the highest vernacular score, or None if there are no vernaculars try: max_vernacular_score = max(vernacular_scores) except ValueError: max_vernacular_score = None # Compute the max scientific score maximum_score = max(scores) # The median across all scores (scientific, canonical, vernacular) if vernacular_scores + scores: median_score = statistics.median(vernacular_scores + scores) else: median_score = 0 # Count how many scores are above the threshold of 50 n_scores_gt_50 = len([x for x in scores + vernacular_scores if x > 50]) return { \"max_scientific_score\": maximum_score, \"max_vernacular_score\": max_vernacular_score, \"median_score\": median_score, \"n_scores_gt_50\": n_scores_gt_50 } GBIF_species_search(search_string) Query the GBIF API for species matching the search string, scoring and ranking results. Parameters: search_string ( str ) \u2013 The string (species name) to search for. Returns: Tuple [ List [ Dict [ str , Any ]], List [ float ]] \u2013 Tuple containing: - List of sorted GBIF result dictionaries (best match first) - List of overall scores for each result (matching order) Source code in observation_editor\\GBIF_functions.py def GBIF_species_search( search_string: str ) -> Tuple[List[Dict[str, Any]], List[float]]: \"\"\" Query the GBIF API for species matching the search string, scoring and ranking results. Args: search_string: The string (species name) to search for. Returns: Tuple containing: - List of sorted GBIF result dictionaries (best match first) - List of overall scores for each result (matching order) \"\"\" gbif_response = requests.get( \"https://api.gbif.org/v1/species/search\", params={ \"q\": search_string, \"datasetKey\": \"d7dddbf4-2cf0-4f39-9b2a-bb099caae36c\", \"limit\": 5, \"verbose\": True } ) # Parse results or empty list if none gbif_data = gbif_response.json().get('results', []) # Score each result using the custom fuzzy matcher all_scores = [GBIF_result_match(search_string, x) for x in gbif_data] # The best score for each result (across all name types) scores = [max([y for y in x.values() if y is not None]) for x in all_scores] # Give a huge boost to exact matches for i in range(len(all_scores)): if all_scores[i][\"max_scientific_score\"] == 100: all_scores[i][\"max_scientific_score\"] = 1000 if all_scores[i][\"max_vernacular_score\"] == 100: all_scores[i][\"max_vernacular_score\"] = 1000 # Compute a combined score for each result (product of all available scores) multi_score = [ math.prod([y for y in x.values() if y is not None]) for x in all_scores ] # Sort results by combined score, descending sorted_items = sorted(enumerate(multi_score), key=lambda x: x[1], reverse=True) gbif_data = [gbif_data[i[0]] for i in sorted_items] scores = [scores[i[0]] for i in sorted_items] return gbif_data, scores GBIF_taxoncode_from_search(search_string, threshold=80) Return taxon codes from a search string if match score is above threshold. Parameters: search_string ( str ) \u2013 The string to search for (usually a species name). threshold ( int , default: 80 ) \u2013 Minimum score for a match to be accepted (default 80). Returns: List [ Tuple [ int , int ]] \u2013 List of (taxon code, taxonomic level) tuples for the best match, or empty list. Source code in observation_editor\\GBIF_functions.py def GBIF_taxoncode_from_search( search_string: str, threshold: int = 80 ) -> List[Tuple[int, int]]: \"\"\" Return taxon codes from a search string if match score is above threshold. Args: search_string: The string to search for (usually a species name). threshold: Minimum score for a match to be accepted (default 80). Returns: List of (taxon code, taxonomic level) tuples for the best match, or empty list. \"\"\" gbif_data, gbif_scores = GBIF_species_search(search_string) if len(gbif_data) == 0: return [] if gbif_scores[0] >= threshold: gbif_data_0 = gbif_data[0] all_keys = GBIF_get_taxoncodes(gbif_data_0) if len(all_keys) > 0: return all_keys return [] GBIF_to_avibase(species_key) Map a GBIF species key to an Avibase taxonConceptID using GBIF occurrence API. Parameters: species_key ( int ) \u2013 Integer GBIF species key. Returns: Optional [ str ] \u2013 Avibase taxonConceptID string if found, otherwise None. Source code in observation_editor\\GBIF_functions.py def GBIF_to_avibase( species_key: int ) -> Optional[str]: \"\"\" Map a GBIF species key to an Avibase taxonConceptID using GBIF occurrence API. Args: species_key: Integer GBIF species key. Returns: Avibase taxonConceptID string if found, otherwise None. \"\"\" gbif_response = requests.get( \"https://api.gbif.org/v1/occurrence/search\", params={ 'taxonKey': species_key, 'limit': 1, 'datasetKey': '4fa7b334-ce0d-4e88-aaae-2e0c138d049e' } ) if gbif_response.status_code != 200: logger.error(\"GBIF to avibase failed: %s %s\", gbif_response.status_code, gbif_response.text) return None gbif_data = gbif_response.json().get('results', []) if gbif_data: avibase = gbif_data[0].get('taxonConceptID') return avibase else: return None","title":"GBIF_functions"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_get_or_create_taxon_object_from_taxon_code","text":"Retrieve or create a Taxon model object from a GBIF taxon code. Parameters: taxon_code ( int ) \u2013 Integer GBIF taxon code. Returns: Tuple [ Any , bool ] \u2013 Tuple of (Taxon object, boolean indicating if it was created). Source code in observation_editor\\GBIF_functions.py def GBIF_get_or_create_taxon_object_from_taxon_code( taxon_code: int ) -> Tuple[Any, bool]: \"\"\" Retrieve or create a Taxon model object from a GBIF taxon code. Args: taxon_code: Integer GBIF taxon code. Returns: Tuple of (Taxon object, boolean indicating if it was created). \"\"\" from .models import Taxon species_data = GBIF_get_species(taxon_code) all_taxon_codes = GBIF_get_taxoncodes(species_data) try: # Try to get or create the Taxon object taxon_obj, created = Taxon.objects.get_or_create( species_name=species_data.get(\"canonicalName\"), species_common_name=species_data.get(\"vernacularName\", \"\"), taxon_source=1, taxon_code=all_taxon_codes[0][0], taxonomic_level=all_taxon_codes[0][1] ) except MultipleObjectsReturned: # If multiple objects exist, just return the first one taxon_obj = Taxon.objects.filter( species_name=species_data.get(\"canonicalName\"), species_common_name=species_data.get(\"vernacularName\", \"\"), taxon_source=1, taxon_code=all_taxon_codes[0][0], taxonomic_level=all_taxon_codes[0][1] ).first() created = False return taxon_obj, created","title":"GBIF_get_or_create_taxon_object_from_taxon_code"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_get_species","text":"Retrieve a species' detailed metadata from GBIF by its key. Parameters: species_key ( int ) \u2013 Integer GBIF species key. Returns: Dict [ str , Any ] \u2013 Dictionary of species metadata returned by GBIF. Source code in observation_editor\\GBIF_functions.py def GBIF_get_species( species_key: int ) -> Dict[str, Any]: \"\"\" Retrieve a species' detailed metadata from GBIF by its key. Args: species_key: Integer GBIF species key. Returns: Dictionary of species metadata returned by GBIF. \"\"\" gbif_response = requests.get( f\"https://api.gbif.org/v1/species/{species_key}\") gbif_data = gbif_response.json() return gbif_data","title":"GBIF_get_species"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_get_taxoncodes","text":"Extract all available taxonomic codes and their levels from a GBIF result. Parameters: gbif_data ( Dict [ str , Any ] ) \u2013 Dictionary containing GBIF taxon data. Returns: List [ Tuple [ int , int ]] \u2013 List of (taxon code, taxonomic level) tuples, ordered from species up to kingdom. Source code in observation_editor\\GBIF_functions.py def GBIF_get_taxoncodes( gbif_data: Dict[str, Any] ) -> List[Tuple[int, int]]: \"\"\" Extract all available taxonomic codes and their levels from a GBIF result. Args: gbif_data: Dictionary containing GBIF taxon data. Returns: List of (taxon code, taxonomic level) tuples, ordered from species up to kingdom. \"\"\" # List of GBIF taxonomic ranks, ordered from most specific to most general rank_keys = [ 'speciesKey', 'genusKey', 'familyKey', 'orderKey', 'classKey', 'phylumKey', 'kingdomKey' ] all_keys: List[Tuple[int, int]] = [] for taxonomic_level, rank_key in enumerate(rank_keys): curr_key = gbif_data.get(rank_key, None) if curr_key is not None: # Validate the key by checking for major issues in GBIF data species_data = GBIF_get_species(curr_key) issues = species_data.get('issues', []) if any([x in issues for x in ['NO_SPECIES']]): continue all_keys.append((curr_key, taxonomic_level)) # Handle special case where the result is a subspecies: use its own key if gbif_data.get('rank') == 'SUBSPECIES' and all_keys: all_keys[0] = (gbif_data.get('key'), 0) return all_keys","title":"GBIF_get_taxoncodes"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_result_match","text":"Perform fuzzy matching between a search string and a single GBIF API result. Parameters: search_string ( str ) \u2013 User input string to search for (e.g., species name). gbif_result ( Dict [ str , Any ] ) \u2013 A dictionary representing a single species result from GBIF. Returns: Dict [ str , Optional [ Union [ int , float ]]] \u2013 A dictionary containing max scientific and vernacular name scores, Dict [ str , Optional [ Union [ int , float ]]] \u2013 median score over all name types, and count of scores > 50. Source code in observation_editor\\GBIF_functions.py def GBIF_result_match( search_string: str, gbif_result: Dict[str, Any] ) -> Dict[str, Optional[Union[int, float]]]: \"\"\" Perform fuzzy matching between a search string and a single GBIF API result. Args: search_string: User input string to search for (e.g., species name). gbif_result: A dictionary representing a single species result from GBIF. Returns: A dictionary containing max scientific and vernacular name scores, median score over all name types, and count of scores > 50. \"\"\" # Normalize the search string for more robust comparison search_string = search_string.lower() keys_to_search = ['scientificName', 'canonicalName'] # Fuzzy match against both scientific and canonical names scores = [ fuzz.ratio(search_string, gbif_result.get(x, \"\").lower()) for x in keys_to_search ] # Fuzzy match against all vernacular (common) names vernacular_scores = [ fuzz.ratio(search_string, x['vernacularName'].lower()) for x in gbif_result.get('vernacularNames', []) ] # Find the highest vernacular score, or None if there are no vernaculars try: max_vernacular_score = max(vernacular_scores) except ValueError: max_vernacular_score = None # Compute the max scientific score maximum_score = max(scores) # The median across all scores (scientific, canonical, vernacular) if vernacular_scores + scores: median_score = statistics.median(vernacular_scores + scores) else: median_score = 0 # Count how many scores are above the threshold of 50 n_scores_gt_50 = len([x for x in scores + vernacular_scores if x > 50]) return { \"max_scientific_score\": maximum_score, \"max_vernacular_score\": max_vernacular_score, \"median_score\": median_score, \"n_scores_gt_50\": n_scores_gt_50 }","title":"GBIF_result_match"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_species_search","text":"Query the GBIF API for species matching the search string, scoring and ranking results. Parameters: search_string ( str ) \u2013 The string (species name) to search for. Returns: Tuple [ List [ Dict [ str , Any ]], List [ float ]] \u2013 Tuple containing: - List of sorted GBIF result dictionaries (best match first) - List of overall scores for each result (matching order) Source code in observation_editor\\GBIF_functions.py def GBIF_species_search( search_string: str ) -> Tuple[List[Dict[str, Any]], List[float]]: \"\"\" Query the GBIF API for species matching the search string, scoring and ranking results. Args: search_string: The string (species name) to search for. Returns: Tuple containing: - List of sorted GBIF result dictionaries (best match first) - List of overall scores for each result (matching order) \"\"\" gbif_response = requests.get( \"https://api.gbif.org/v1/species/search\", params={ \"q\": search_string, \"datasetKey\": \"d7dddbf4-2cf0-4f39-9b2a-bb099caae36c\", \"limit\": 5, \"verbose\": True } ) # Parse results or empty list if none gbif_data = gbif_response.json().get('results', []) # Score each result using the custom fuzzy matcher all_scores = [GBIF_result_match(search_string, x) for x in gbif_data] # The best score for each result (across all name types) scores = [max([y for y in x.values() if y is not None]) for x in all_scores] # Give a huge boost to exact matches for i in range(len(all_scores)): if all_scores[i][\"max_scientific_score\"] == 100: all_scores[i][\"max_scientific_score\"] = 1000 if all_scores[i][\"max_vernacular_score\"] == 100: all_scores[i][\"max_vernacular_score\"] = 1000 # Compute a combined score for each result (product of all available scores) multi_score = [ math.prod([y for y in x.values() if y is not None]) for x in all_scores ] # Sort results by combined score, descending sorted_items = sorted(enumerate(multi_score), key=lambda x: x[1], reverse=True) gbif_data = [gbif_data[i[0]] for i in sorted_items] scores = [scores[i[0]] for i in sorted_items] return gbif_data, scores","title":"GBIF_species_search"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_taxoncode_from_search","text":"Return taxon codes from a search string if match score is above threshold. Parameters: search_string ( str ) \u2013 The string to search for (usually a species name). threshold ( int , default: 80 ) \u2013 Minimum score for a match to be accepted (default 80). Returns: List [ Tuple [ int , int ]] \u2013 List of (taxon code, taxonomic level) tuples for the best match, or empty list. Source code in observation_editor\\GBIF_functions.py def GBIF_taxoncode_from_search( search_string: str, threshold: int = 80 ) -> List[Tuple[int, int]]: \"\"\" Return taxon codes from a search string if match score is above threshold. Args: search_string: The string to search for (usually a species name). threshold: Minimum score for a match to be accepted (default 80). Returns: List of (taxon code, taxonomic level) tuples for the best match, or empty list. \"\"\" gbif_data, gbif_scores = GBIF_species_search(search_string) if len(gbif_data) == 0: return [] if gbif_scores[0] >= threshold: gbif_data_0 = gbif_data[0] all_keys = GBIF_get_taxoncodes(gbif_data_0) if len(all_keys) > 0: return all_keys return []","title":"GBIF_taxoncode_from_search"},{"location":"reference/observation_editor/GBIF_functions/#observation_editor.GBIF_functions.GBIF_to_avibase","text":"Map a GBIF species key to an Avibase taxonConceptID using GBIF occurrence API. Parameters: species_key ( int ) \u2013 Integer GBIF species key. Returns: Optional [ str ] \u2013 Avibase taxonConceptID string if found, otherwise None. Source code in observation_editor\\GBIF_functions.py def GBIF_to_avibase( species_key: int ) -> Optional[str]: \"\"\" Map a GBIF species key to an Avibase taxonConceptID using GBIF occurrence API. Args: species_key: Integer GBIF species key. Returns: Avibase taxonConceptID string if found, otherwise None. \"\"\" gbif_response = requests.get( \"https://api.gbif.org/v1/occurrence/search\", params={ 'taxonKey': species_key, 'limit': 1, 'datasetKey': '4fa7b334-ce0d-4e88-aaae-2e0c138d049e' } ) if gbif_response.status_code != 200: logger.error(\"GBIF to avibase failed: %s %s\", gbif_response.status_code, gbif_response.text) return None gbif_data = gbif_response.json().get('results', []) if gbif_data: avibase = gbif_data[0].get('taxonConceptID') return avibase else: return None","title":"GBIF_to_avibase"},{"location":"reference/observation_editor/filtersets/","text":"ObservationFilter Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for filtering observations based on various fields. Source code in observation_editor\\filtersets.py class ObservationFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for filtering observations based on various fields. \"\"\" field_help_dict = {\"taxon__id\": \"Numeric database ID of the taxon.\", \"owner__id\": \"Numeric database ID of user who created this observation.\", \"data_files__deployment__id\": \"Numeric database ID of deployment to which\\ the datafiles of this observation are attached.\", \"data_files__id\": \"Numeric database ID of datafile of this observation.\", \"validation_of__id\": \"Numeric database ID of the observation of which this observation is a validation of.\"} class Meta: model = Observation fields = { 'data_files__id': ['exact'], 'data_files__deployment__id': ['exact'], 'taxon__id': ['exact', 'in'], 'obs_dt': ['exact', 'lte', 'gte'], 'confidence': ['exact', 'lte', 'gte'], 'source': ['exact'], 'sex': ['exact'], 'lifestage': ['exact'], 'behavior': ['exact'], 'validation_requested': ['exact'], 'validation_of__id': ['exact'], 'taxon__species_name': ['exact', 'contains'], 'taxon__species_common_name': ['exact', 'contains'], 'taxon__taxon_code': ['exact', 'contains'], 'owner__id': ['exact'], }","title":"filtersets"},{"location":"reference/observation_editor/filtersets/#observation_editor.filtersets.ObservationFilter","text":"Bases: GenericFilterMixIn , ExtraDataFilterMixIn FilterSet for filtering observations based on various fields. Source code in observation_editor\\filtersets.py class ObservationFilter(GenericFilterMixIn, ExtraDataFilterMixIn): \"\"\" FilterSet for filtering observations based on various fields. \"\"\" field_help_dict = {\"taxon__id\": \"Numeric database ID of the taxon.\", \"owner__id\": \"Numeric database ID of user who created this observation.\", \"data_files__deployment__id\": \"Numeric database ID of deployment to which\\ the datafiles of this observation are attached.\", \"data_files__id\": \"Numeric database ID of datafile of this observation.\", \"validation_of__id\": \"Numeric database ID of the observation of which this observation is a validation of.\"} class Meta: model = Observation fields = { 'data_files__id': ['exact'], 'data_files__deployment__id': ['exact'], 'taxon__id': ['exact', 'in'], 'obs_dt': ['exact', 'lte', 'gte'], 'confidence': ['exact', 'lte', 'gte'], 'source': ['exact'], 'sex': ['exact'], 'lifestage': ['exact'], 'behavior': ['exact'], 'validation_requested': ['exact'], 'validation_of__id': ['exact'], 'taxon__species_name': ['exact', 'contains'], 'taxon__species_common_name': ['exact', 'contains'], 'taxon__taxon_code': ['exact', 'contains'], 'owner__id': ['exact'], }","title":"ObservationFilter"},{"location":"reference/observation_editor/metadata_functions/","text":"create_metadata_dict(observation_objs) Serialize a queryset of observations to dict. Parameters: observation_objs ( QuerySet [ Observation ] ) \u2013 Queryset of observation objects Returns: dict ( dict ) \u2013 Serialized dictionary of Observations Source code in observation_editor\\metadata_functions.py def create_metadata_dict(observation_objs: QuerySet[Observation]) -> dict: \"\"\" Serialize a queryset of observations to dict. Args: observation_objs (QuerySet[Observation]): Queryset of observation objects Returns: dict: Serialized dictionary of Observations \"\"\" observation_dict = ObservationSerializer(observation_objs, many=True).data return observation_dict","title":"metadata_functions"},{"location":"reference/observation_editor/metadata_functions/#observation_editor.metadata_functions.create_metadata_dict","text":"Serialize a queryset of observations to dict. Parameters: observation_objs ( QuerySet [ Observation ] ) \u2013 Queryset of observation objects Returns: dict ( dict ) \u2013 Serialized dictionary of Observations Source code in observation_editor\\metadata_functions.py def create_metadata_dict(observation_objs: QuerySet[Observation]) -> dict: \"\"\" Serialize a queryset of observations to dict. Args: observation_objs (QuerySet[Observation]): Queryset of observation objects Returns: dict: Serialized dictionary of Observations \"\"\" observation_dict = ObservationSerializer(observation_objs, many=True).data return observation_dict","title":"create_metadata_dict"},{"location":"reference/observation_editor/models/","text":"Observation Bases: BaseModel Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status. Source code in observation_editor\\models.py class Observation(BaseModel): \"\"\" Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status. \"\"\" owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"observations\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"User who created the observation.\\ Null if created by AI.\") label = models.CharField(max_length=300, blank=True, editable=False) taxon = models.ForeignKey( Taxon, on_delete=models.PROTECT, related_name=\"observations\", null=True, db_index=True, help_text=\"Taxon of the observed species.\") data_files = models.ManyToManyField( DataFile, related_name=\"observations\", db_index=True, help_text=\"Data files associated with the observation.\") obs_dt = models.DateTimeField( null=True, blank=True, help_text=\"Date and time of the observation.\") source = models.CharField(max_length=100, default=\"human\", db_index=True, help_text=\"Source of the observation, e.g. 'human' or an AI model name.\") number = models.IntegerField( default=1, help_text=\"Number of individuals in this observation, for use with whole image annotation.\") bounding_box = models.JSONField(default=dict, help_text=\"Bounding box of this observation in the format\\ {'x1':Left coordinate, 'y1':Bottom coordinate, 'x2': Right coordinate, 'y2': Left coordinate}.\") confidence = models.FloatField( default=None, null=True, blank=True, help_text=\"Confidence of the observation from AI model.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that the standard fields do not cover\") sex = models.CharField(max_length=10, blank=True, help_text=\"Sex of the observed species, if known.\") lifestage = models.CharField( max_length=15, blank=True, help_text=\"Lifestage of the observed species, if known.\") behavior = models.CharField( max_length=50, blank=True, help_text=\"Behaviour of the observed species.\") validation_requested = models.BooleanField( default=False, help_text=\"If True, this observation is requested for validation by a human expert.\") validation_of = models.ManyToManyField( \"self\", symmetrical=False, related_name=\"validated_by\", blank=True, help_text=\"If this observation is a validation of another observation, this field will contain the original observation(s).\") objects = ObservationQuerySet.as_manager() def get_absolute_url(self): \"\"\" Get the absolute URL for this observation's associated data file. Returns: str: URL path to the data file. \"\"\" return f\"/datafiles/{self.data_files.all().values_list('pk',flat=True)[0]}\" def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance for this Observation at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" return self.taxon.get_taxonomic_level(level) def __str__(self): \"\"\" Returns the label of the observation. \"\"\" return self.label def get_label(self): \"\"\" Generate a label for the observation based on the taxon and associated data file. Returns: str: Generated label. \"\"\" if self.data_files.all().exists(): return f\"{self.taxon.species_name}_{self.data_files.all().first().file_name}\" else: return self.label def save(self, *args, **kwargs): \"\"\" Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. \"\"\" if self.id: if self.label is None or self.label == \"\": self.label = self.get_label() if self.obs_dt is None and self.data_files.all().exists(): min_recording_dt = self.data_files.all().aggregate( min_recording_dt=Min(\"recording_dt\")) self.obs_dt = min_recording_dt[\"min_recording_dt\"] super().save(*args, **kwargs) def check_data_files_human(self): \"\"\" Check and update associated data files if the observation is of a human. \"\"\" for data_file in self.data_files.all(): data_file.check_human() __str__() Returns the label of the observation. Source code in observation_editor\\models.py def __str__(self): \"\"\" Returns the label of the observation. \"\"\" return self.label check_data_files_human() Check and update associated data files if the observation is of a human. Source code in observation_editor\\models.py def check_data_files_human(self): \"\"\" Check and update associated data files if the observation is of a human. \"\"\" for data_file in self.data_files.all(): data_file.check_human() get_absolute_url() Get the absolute URL for this observation's associated data file. Returns: str \u2013 URL path to the data file. Source code in observation_editor\\models.py def get_absolute_url(self): \"\"\" Get the absolute URL for this observation's associated data file. Returns: str: URL path to the data file. \"\"\" return f\"/datafiles/{self.data_files.all().values_list('pk',flat=True)[0]}\" get_label() Generate a label for the observation based on the taxon and associated data file. Returns: str \u2013 Generated label. Source code in observation_editor\\models.py def get_label(self): \"\"\" Generate a label for the observation based on the taxon and associated data file. Returns: str: Generated label. \"\"\" if self.data_files.all().exists(): return f\"{self.taxon.species_name}_{self.data_files.all().first().file_name}\" else: return self.label get_taxonomic_level(level=0) Retrieve the Taxon instance for this Observation at the specified taxonomic level. Parameters: level ( int , default: 0 ) \u2013 Desired taxonomic level. Returns: \u2013 Taxon or None: The taxon at the specified level or None if not found. Source code in observation_editor\\models.py def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance for this Observation at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" return self.taxon.get_taxonomic_level(level) save(*args, **kwargs) Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. Source code in observation_editor\\models.py def save(self, *args, **kwargs): \"\"\" Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. \"\"\" if self.id: if self.label is None or self.label == \"\": self.label = self.get_label() if self.obs_dt is None and self.data_files.all().exists(): min_recording_dt = self.data_files.all().aggregate( min_recording_dt=Min(\"recording_dt\")) self.obs_dt = min_recording_dt[\"min_recording_dt\"] super().save(*args, **kwargs) ObservationQuerySet Bases: ApproximateCountQuerySet Custom QuerySet for the Observation model, providing helper methods for taxonomic queries. Source code in observation_editor\\models.py class ObservationQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for the Observation model, providing helper methods for taxonomic queries. \"\"\" def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get observations at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__pk')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__pk\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None))), parent_taxon_level=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__taxonomic_level')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__taxonomic_level\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None)) ), ) annotated_qs = annotated_qs.filter( parent_taxon_level=target_level).distinct() return annotated_qs get_taxonomic_level(target_level=1) Annotate and filter queryset to get observations at a specified taxonomic level. Parameters: target_level ( int , default: 1 ) \u2013 The desired taxonomic level. Returns: QuerySet \u2013 The filtered and annotated queryset. Source code in observation_editor\\models.py def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get observations at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__pk')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__pk\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None))), parent_taxon_level=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__taxonomic_level')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__taxonomic_level\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None)) ), ) annotated_qs = annotated_qs.filter( parent_taxon_level=target_level).distinct() return annotated_qs Taxon Bases: BaseModel Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships. Source code in observation_editor\\models.py class Taxon(BaseModel): \"\"\" Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships. \"\"\" species_name = models.CharField( max_length=100, unique=False, db_index=True, help_text=\"Scientific name of the species, e.g. 'Aquila chrysaetos'.\") species_common_name = models.CharField( max_length=100, unique=False, blank=True, db_index=True, help_text=\"Common name of the species, e.g. 'Golden Eagle'.\") taxon_code = models.CharField(max_length=100, blank=True, db_index=True, help_text=\"Taxon code from a taxonomic database, e.g. GBIF ID,\\ or another common identifier such as 'vehicle'.\") taxon_source = models.IntegerField( choices=source_choice, default=0, help_text=\"Source of the taxon code. 0 = custom, 1 = GBIF.\") extra_data = models.JSONField( default=dict, help_text=\"Extra data about the taxon, e.g. avibaseID, or other relevant information.\", blank=True) parents = models.ManyToManyField( \"self\", symmetrical=False, related_name=\"children\", blank=True, help_text=\"Parent taxons of this taxon.\") taxonomic_level = models.IntegerField( choices=taxonomic_level_choice, default=0, help_text=\"Taxonomic level of the species, e.g. 0 = species, 1 = genus, 2 = family, etc.\") objects = TaxonQuerySet.as_manager() def __str__(self): \"\"\" Returns the scientific name of the taxon. \"\"\" return self.species_name def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" logger.info(self.taxonomic_level, level) if self.taxonomic_level == level: return self else: try: taxon_obj = self.parents.all().get(taxonomic_level=level) except Taxon.DoesNotExist: taxon_obj = None return taxon_obj def get_taxon_code(self): \"\"\" Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. \"\"\" if self.taxon_code is None or self.taxon_code == \"\": taxon_codes = GBIF_taxoncode_from_search(self.species_name) logger.info(taxon_codes) if len(taxon_codes) > 0: self.taxon_code = taxon_codes[0][0] self.taxonomic_level = taxon_codes[0][1] self.taxon_source = 1 species_data = GBIF_get_species(self.taxon_code) extra_data = self.extra_data if (gbif_vernacular_name := species_data.get(\"vernacularName\")) is not None: logger.info(gbif_vernacular_name) self.species_common_name = gbif_vernacular_name if species_data.get('class', '') == 'Aves': avibaseID = GBIF_to_avibase(self.taxon_code) if avibaseID is not None: extra_data.update({\"avibaseID\": avibaseID}) self.extra_data = extra_data else: self.taxon_source = 0 def save(self, *args, **kwargs): \"\"\" Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. \"\"\" try: self.get_taxon_code() except ConnectionError or ConnectTimeout as e: logger.info(e) pass try: existing = Taxon.objects.get( species_name__iexact=self.species_name.lower()) if existing != self: if self.pk is not None: # get all existing observations and change them, then delete this instance Observation.objects.filter( taxon=self).update(taxon=existing) self.delete() return except Taxon.DoesNotExist: pass super().save(*args, **kwargs) __str__() Returns the scientific name of the taxon. Source code in observation_editor\\models.py def __str__(self): \"\"\" Returns the scientific name of the taxon. \"\"\" return self.species_name get_taxon_code() Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. Source code in observation_editor\\models.py def get_taxon_code(self): \"\"\" Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. \"\"\" if self.taxon_code is None or self.taxon_code == \"\": taxon_codes = GBIF_taxoncode_from_search(self.species_name) logger.info(taxon_codes) if len(taxon_codes) > 0: self.taxon_code = taxon_codes[0][0] self.taxonomic_level = taxon_codes[0][1] self.taxon_source = 1 species_data = GBIF_get_species(self.taxon_code) extra_data = self.extra_data if (gbif_vernacular_name := species_data.get(\"vernacularName\")) is not None: logger.info(gbif_vernacular_name) self.species_common_name = gbif_vernacular_name if species_data.get('class', '') == 'Aves': avibaseID = GBIF_to_avibase(self.taxon_code) if avibaseID is not None: extra_data.update({\"avibaseID\": avibaseID}) self.extra_data = extra_data else: self.taxon_source = 0 get_taxonomic_level(level=0) Retrieve the Taxon instance at the specified taxonomic level. Parameters: level ( int , default: 0 ) \u2013 Desired taxonomic level. Returns: \u2013 Taxon or None: The taxon at the specified level or None if not found. Source code in observation_editor\\models.py def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" logger.info(self.taxonomic_level, level) if self.taxonomic_level == level: return self else: try: taxon_obj = self.parents.all().get(taxonomic_level=level) except Taxon.DoesNotExist: taxon_obj = None return taxon_obj save(*args, **kwargs) Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. Source code in observation_editor\\models.py def save(self, *args, **kwargs): \"\"\" Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. \"\"\" try: self.get_taxon_code() except ConnectionError or ConnectTimeout as e: logger.info(e) pass try: existing = Taxon.objects.get( species_name__iexact=self.species_name.lower()) if existing != self: if self.pk is not None: # get all existing observations and change them, then delete this instance Observation.objects.filter( taxon=self).update(taxon=existing) self.delete() return except Taxon.DoesNotExist: pass super().save(*args, **kwargs) TaxonQuerySet Bases: ApproximateCountQuerySet Custom QuerySet for the Taxon model, providing helper methods for taxonomic queries. Source code in observation_editor\\models.py class TaxonQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for the Taxon model, providing helper methods for taxonomic queries. \"\"\" def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get taxons at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxonomic_level=target_level, then=F('pk')), When(taxonomic_level__lt=target_level, then=F(\"parents__pk\")))) annotated_qs = annotated_qs.filter( Q(parent_taxon_pk=target_level) | Q(parent_taxon_pk__isnull=True)) return annotated_qs get_taxonomic_level(target_level=1) Annotate and filter queryset to get taxons at a specified taxonomic level. Parameters: target_level ( int , default: 1 ) \u2013 The desired taxonomic level. Returns: QuerySet \u2013 The filtered and annotated queryset. Source code in observation_editor\\models.py def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get taxons at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxonomic_level=target_level, then=F('pk')), When(taxonomic_level__lt=target_level, then=F(\"parents__pk\")))) annotated_qs = annotated_qs.filter( Q(parent_taxon_pk=target_level) | Q(parent_taxon_pk__isnull=True)) return annotated_qs check_human_delete(sender, instance, **kwargs) Signal handler to check and update data files when a human observation is deleted. Source code in observation_editor\\models.py @receiver(post_delete, sender=Observation) def check_human_delete(sender, instance, **kwargs): \"\"\" Signal handler to check and update data files when a human observation is deleted. \"\"\" if instance.taxon.taxon_code == settings.HUMAN_TAXON_CODE: instance.check_data_files_human() check_human_save(sender, instance, created, **kwargs) Signal handler to check and update data files when a human observation is saved. Source code in observation_editor\\models.py @receiver(post_save, sender=Observation) def check_human_save(sender, instance, created, **kwargs): \"\"\" Signal handler to check and update data files when a human observation is saved. \"\"\" if instance.taxon.taxon_code == settings.HUMAN_TAXON_CODE: instance.check_data_files_human() post_taxon_save(sender, instance, created, **kwargs) Signal handler to trigger parent taxon creation after saving a Taxon instance with a GBIF code. Source code in observation_editor\\models.py @receiver(post_save, sender=Taxon) def post_taxon_save(sender, instance, created, **kwargs): \"\"\" Signal handler to trigger parent taxon creation after saving a Taxon instance with a GBIF code. \"\"\" if instance.taxon_code is not None and instance.taxon_code != \"\" and instance.taxon_source == 1: create_taxon_parents.apply_async([instance.pk]) update_observation_data_files(sender, instance, action, reverse, *args, **kwargs) Signal handler to update the Observation instance when its data files are changed. Source code in observation_editor\\models.py @receiver(m2m_changed, sender=Observation.data_files.through) def update_observation_data_files(sender, instance, action, reverse, *args, **kwargs): \"\"\" Signal handler to update the Observation instance when its data files are changed. \"\"\" if (action == 'post_add' or action == 'post_remove'): instance.save()","title":"models"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation","text":"Bases: BaseModel Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status. Source code in observation_editor\\models.py class Observation(BaseModel): \"\"\" Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status. \"\"\" owner = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, related_name=\"observations\", on_delete=models.SET_NULL, null=True, db_index=True, help_text=\"User who created the observation.\\ Null if created by AI.\") label = models.CharField(max_length=300, blank=True, editable=False) taxon = models.ForeignKey( Taxon, on_delete=models.PROTECT, related_name=\"observations\", null=True, db_index=True, help_text=\"Taxon of the observed species.\") data_files = models.ManyToManyField( DataFile, related_name=\"observations\", db_index=True, help_text=\"Data files associated with the observation.\") obs_dt = models.DateTimeField( null=True, blank=True, help_text=\"Date and time of the observation.\") source = models.CharField(max_length=100, default=\"human\", db_index=True, help_text=\"Source of the observation, e.g. 'human' or an AI model name.\") number = models.IntegerField( default=1, help_text=\"Number of individuals in this observation, for use with whole image annotation.\") bounding_box = models.JSONField(default=dict, help_text=\"Bounding box of this observation in the format\\ {'x1':Left coordinate, 'y1':Bottom coordinate, 'x2': Right coordinate, 'y2': Left coordinate}.\") confidence = models.FloatField( default=None, null=True, blank=True, help_text=\"Confidence of the observation from AI model.\") extra_data = models.JSONField( default=dict, blank=True, help_text=\"Extra data that the standard fields do not cover\") sex = models.CharField(max_length=10, blank=True, help_text=\"Sex of the observed species, if known.\") lifestage = models.CharField( max_length=15, blank=True, help_text=\"Lifestage of the observed species, if known.\") behavior = models.CharField( max_length=50, blank=True, help_text=\"Behaviour of the observed species.\") validation_requested = models.BooleanField( default=False, help_text=\"If True, this observation is requested for validation by a human expert.\") validation_of = models.ManyToManyField( \"self\", symmetrical=False, related_name=\"validated_by\", blank=True, help_text=\"If this observation is a validation of another observation, this field will contain the original observation(s).\") objects = ObservationQuerySet.as_manager() def get_absolute_url(self): \"\"\" Get the absolute URL for this observation's associated data file. Returns: str: URL path to the data file. \"\"\" return f\"/datafiles/{self.data_files.all().values_list('pk',flat=True)[0]}\" def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance for this Observation at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" return self.taxon.get_taxonomic_level(level) def __str__(self): \"\"\" Returns the label of the observation. \"\"\" return self.label def get_label(self): \"\"\" Generate a label for the observation based on the taxon and associated data file. Returns: str: Generated label. \"\"\" if self.data_files.all().exists(): return f\"{self.taxon.species_name}_{self.data_files.all().first().file_name}\" else: return self.label def save(self, *args, **kwargs): \"\"\" Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. \"\"\" if self.id: if self.label is None or self.label == \"\": self.label = self.get_label() if self.obs_dt is None and self.data_files.all().exists(): min_recording_dt = self.data_files.all().aggregate( min_recording_dt=Min(\"recording_dt\")) self.obs_dt = min_recording_dt[\"min_recording_dt\"] super().save(*args, **kwargs) def check_data_files_human(self): \"\"\" Check and update associated data files if the observation is of a human. \"\"\" for data_file in self.data_files.all(): data_file.check_human()","title":"Observation"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.__str__","text":"Returns the label of the observation. Source code in observation_editor\\models.py def __str__(self): \"\"\" Returns the label of the observation. \"\"\" return self.label","title":"__str__"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.check_data_files_human","text":"Check and update associated data files if the observation is of a human. Source code in observation_editor\\models.py def check_data_files_human(self): \"\"\" Check and update associated data files if the observation is of a human. \"\"\" for data_file in self.data_files.all(): data_file.check_human()","title":"check_data_files_human"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.get_absolute_url","text":"Get the absolute URL for this observation's associated data file. Returns: str \u2013 URL path to the data file. Source code in observation_editor\\models.py def get_absolute_url(self): \"\"\" Get the absolute URL for this observation's associated data file. Returns: str: URL path to the data file. \"\"\" return f\"/datafiles/{self.data_files.all().values_list('pk',flat=True)[0]}\"","title":"get_absolute_url"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.get_label","text":"Generate a label for the observation based on the taxon and associated data file. Returns: str \u2013 Generated label. Source code in observation_editor\\models.py def get_label(self): \"\"\" Generate a label for the observation based on the taxon and associated data file. Returns: str: Generated label. \"\"\" if self.data_files.all().exists(): return f\"{self.taxon.species_name}_{self.data_files.all().first().file_name}\" else: return self.label","title":"get_label"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.get_taxonomic_level","text":"Retrieve the Taxon instance for this Observation at the specified taxonomic level. Parameters: level ( int , default: 0 ) \u2013 Desired taxonomic level. Returns: \u2013 Taxon or None: The taxon at the specified level or None if not found. Source code in observation_editor\\models.py def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance for this Observation at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" return self.taxon.get_taxonomic_level(level)","title":"get_taxonomic_level"},{"location":"reference/observation_editor/models/#observation_editor.models.Observation.save","text":"Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. Source code in observation_editor\\models.py def save(self, *args, **kwargs): \"\"\" Custom save logic for Observation instances, including automatic label generation and observation datetime assignment. \"\"\" if self.id: if self.label is None or self.label == \"\": self.label = self.get_label() if self.obs_dt is None and self.data_files.all().exists(): min_recording_dt = self.data_files.all().aggregate( min_recording_dt=Min(\"recording_dt\")) self.obs_dt = min_recording_dt[\"min_recording_dt\"] super().save(*args, **kwargs)","title":"save"},{"location":"reference/observation_editor/models/#observation_editor.models.ObservationQuerySet","text":"Bases: ApproximateCountQuerySet Custom QuerySet for the Observation model, providing helper methods for taxonomic queries. Source code in observation_editor\\models.py class ObservationQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for the Observation model, providing helper methods for taxonomic queries. \"\"\" def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get observations at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__pk')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__pk\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None))), parent_taxon_level=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__taxonomic_level')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__taxonomic_level\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None)) ), ) annotated_qs = annotated_qs.filter( parent_taxon_level=target_level).distinct() return annotated_qs","title":"ObservationQuerySet"},{"location":"reference/observation_editor/models/#observation_editor.models.ObservationQuerySet.get_taxonomic_level","text":"Annotate and filter queryset to get observations at a specified taxonomic level. Parameters: target_level ( int , default: 1 ) \u2013 The desired taxonomic level. Returns: QuerySet \u2013 The filtered and annotated queryset. Source code in observation_editor\\models.py def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get observations at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__pk')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__pk\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None))), parent_taxon_level=Case( When(taxon__taxonomic_level=target_level, then=F('taxon__taxonomic_level')), When(taxon__taxonomic_level__lt=target_level, then=F(\"taxon__parents__taxonomic_level\")), When(taxon__taxonomic_level__gt=target_level, then=Value(None)) ), ) annotated_qs = annotated_qs.filter( parent_taxon_level=target_level).distinct() return annotated_qs","title":"get_taxonomic_level"},{"location":"reference/observation_editor/models/#observation_editor.models.Taxon","text":"Bases: BaseModel Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships. Source code in observation_editor\\models.py class Taxon(BaseModel): \"\"\" Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships. \"\"\" species_name = models.CharField( max_length=100, unique=False, db_index=True, help_text=\"Scientific name of the species, e.g. 'Aquila chrysaetos'.\") species_common_name = models.CharField( max_length=100, unique=False, blank=True, db_index=True, help_text=\"Common name of the species, e.g. 'Golden Eagle'.\") taxon_code = models.CharField(max_length=100, blank=True, db_index=True, help_text=\"Taxon code from a taxonomic database, e.g. GBIF ID,\\ or another common identifier such as 'vehicle'.\") taxon_source = models.IntegerField( choices=source_choice, default=0, help_text=\"Source of the taxon code. 0 = custom, 1 = GBIF.\") extra_data = models.JSONField( default=dict, help_text=\"Extra data about the taxon, e.g. avibaseID, or other relevant information.\", blank=True) parents = models.ManyToManyField( \"self\", symmetrical=False, related_name=\"children\", blank=True, help_text=\"Parent taxons of this taxon.\") taxonomic_level = models.IntegerField( choices=taxonomic_level_choice, default=0, help_text=\"Taxonomic level of the species, e.g. 0 = species, 1 = genus, 2 = family, etc.\") objects = TaxonQuerySet.as_manager() def __str__(self): \"\"\" Returns the scientific name of the taxon. \"\"\" return self.species_name def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" logger.info(self.taxonomic_level, level) if self.taxonomic_level == level: return self else: try: taxon_obj = self.parents.all().get(taxonomic_level=level) except Taxon.DoesNotExist: taxon_obj = None return taxon_obj def get_taxon_code(self): \"\"\" Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. \"\"\" if self.taxon_code is None or self.taxon_code == \"\": taxon_codes = GBIF_taxoncode_from_search(self.species_name) logger.info(taxon_codes) if len(taxon_codes) > 0: self.taxon_code = taxon_codes[0][0] self.taxonomic_level = taxon_codes[0][1] self.taxon_source = 1 species_data = GBIF_get_species(self.taxon_code) extra_data = self.extra_data if (gbif_vernacular_name := species_data.get(\"vernacularName\")) is not None: logger.info(gbif_vernacular_name) self.species_common_name = gbif_vernacular_name if species_data.get('class', '') == 'Aves': avibaseID = GBIF_to_avibase(self.taxon_code) if avibaseID is not None: extra_data.update({\"avibaseID\": avibaseID}) self.extra_data = extra_data else: self.taxon_source = 0 def save(self, *args, **kwargs): \"\"\" Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. \"\"\" try: self.get_taxon_code() except ConnectionError or ConnectTimeout as e: logger.info(e) pass try: existing = Taxon.objects.get( species_name__iexact=self.species_name.lower()) if existing != self: if self.pk is not None: # get all existing observations and change them, then delete this instance Observation.objects.filter( taxon=self).update(taxon=existing) self.delete() return except Taxon.DoesNotExist: pass super().save(*args, **kwargs)","title":"Taxon"},{"location":"reference/observation_editor/models/#observation_editor.models.Taxon.__str__","text":"Returns the scientific name of the taxon. Source code in observation_editor\\models.py def __str__(self): \"\"\" Returns the scientific name of the taxon. \"\"\" return self.species_name","title":"__str__"},{"location":"reference/observation_editor/models/#observation_editor.models.Taxon.get_taxon_code","text":"Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. Source code in observation_editor\\models.py def get_taxon_code(self): \"\"\" Retrieve or generate the taxon code for this taxon, updating extra data and common name as necessary. \"\"\" if self.taxon_code is None or self.taxon_code == \"\": taxon_codes = GBIF_taxoncode_from_search(self.species_name) logger.info(taxon_codes) if len(taxon_codes) > 0: self.taxon_code = taxon_codes[0][0] self.taxonomic_level = taxon_codes[0][1] self.taxon_source = 1 species_data = GBIF_get_species(self.taxon_code) extra_data = self.extra_data if (gbif_vernacular_name := species_data.get(\"vernacularName\")) is not None: logger.info(gbif_vernacular_name) self.species_common_name = gbif_vernacular_name if species_data.get('class', '') == 'Aves': avibaseID = GBIF_to_avibase(self.taxon_code) if avibaseID is not None: extra_data.update({\"avibaseID\": avibaseID}) self.extra_data = extra_data else: self.taxon_source = 0","title":"get_taxon_code"},{"location":"reference/observation_editor/models/#observation_editor.models.Taxon.get_taxonomic_level","text":"Retrieve the Taxon instance at the specified taxonomic level. Parameters: level ( int , default: 0 ) \u2013 Desired taxonomic level. Returns: \u2013 Taxon or None: The taxon at the specified level or None if not found. Source code in observation_editor\\models.py def get_taxonomic_level(self, level=0): \"\"\" Retrieve the Taxon instance at the specified taxonomic level. Args: level (int): Desired taxonomic level. Returns: Taxon or None: The taxon at the specified level or None if not found. \"\"\" logger.info(self.taxonomic_level, level) if self.taxonomic_level == level: return self else: try: taxon_obj = self.parents.all().get(taxonomic_level=level) except Taxon.DoesNotExist: taxon_obj = None return taxon_obj","title":"get_taxonomic_level"},{"location":"reference/observation_editor/models/#observation_editor.models.Taxon.save","text":"Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. Source code in observation_editor\\models.py def save(self, *args, **kwargs): \"\"\" Custom save logic for Taxon instances, including taxon code generation and duplicate species handling. \"\"\" try: self.get_taxon_code() except ConnectionError or ConnectTimeout as e: logger.info(e) pass try: existing = Taxon.objects.get( species_name__iexact=self.species_name.lower()) if existing != self: if self.pk is not None: # get all existing observations and change them, then delete this instance Observation.objects.filter( taxon=self).update(taxon=existing) self.delete() return except Taxon.DoesNotExist: pass super().save(*args, **kwargs)","title":"save"},{"location":"reference/observation_editor/models/#observation_editor.models.TaxonQuerySet","text":"Bases: ApproximateCountQuerySet Custom QuerySet for the Taxon model, providing helper methods for taxonomic queries. Source code in observation_editor\\models.py class TaxonQuerySet(ApproximateCountQuerySet): \"\"\" Custom QuerySet for the Taxon model, providing helper methods for taxonomic queries. \"\"\" def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get taxons at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxonomic_level=target_level, then=F('pk')), When(taxonomic_level__lt=target_level, then=F(\"parents__pk\")))) annotated_qs = annotated_qs.filter( Q(parent_taxon_pk=target_level) | Q(parent_taxon_pk__isnull=True)) return annotated_qs","title":"TaxonQuerySet"},{"location":"reference/observation_editor/models/#observation_editor.models.TaxonQuerySet.get_taxonomic_level","text":"Annotate and filter queryset to get taxons at a specified taxonomic level. Parameters: target_level ( int , default: 1 ) \u2013 The desired taxonomic level. Returns: QuerySet \u2013 The filtered and annotated queryset. Source code in observation_editor\\models.py def get_taxonomic_level(self, target_level=1): \"\"\" Annotate and filter queryset to get taxons at a specified taxonomic level. Args: target_level (int): The desired taxonomic level. Returns: QuerySet: The filtered and annotated queryset. \"\"\" annotated_qs = self.annotate( parent_taxon_pk=Case( When(taxonomic_level=target_level, then=F('pk')), When(taxonomic_level__lt=target_level, then=F(\"parents__pk\")))) annotated_qs = annotated_qs.filter( Q(parent_taxon_pk=target_level) | Q(parent_taxon_pk__isnull=True)) return annotated_qs","title":"get_taxonomic_level"},{"location":"reference/observation_editor/models/#observation_editor.models.check_human_delete","text":"Signal handler to check and update data files when a human observation is deleted. Source code in observation_editor\\models.py @receiver(post_delete, sender=Observation) def check_human_delete(sender, instance, **kwargs): \"\"\" Signal handler to check and update data files when a human observation is deleted. \"\"\" if instance.taxon.taxon_code == settings.HUMAN_TAXON_CODE: instance.check_data_files_human()","title":"check_human_delete"},{"location":"reference/observation_editor/models/#observation_editor.models.check_human_save","text":"Signal handler to check and update data files when a human observation is saved. Source code in observation_editor\\models.py @receiver(post_save, sender=Observation) def check_human_save(sender, instance, created, **kwargs): \"\"\" Signal handler to check and update data files when a human observation is saved. \"\"\" if instance.taxon.taxon_code == settings.HUMAN_TAXON_CODE: instance.check_data_files_human()","title":"check_human_save"},{"location":"reference/observation_editor/models/#observation_editor.models.post_taxon_save","text":"Signal handler to trigger parent taxon creation after saving a Taxon instance with a GBIF code. Source code in observation_editor\\models.py @receiver(post_save, sender=Taxon) def post_taxon_save(sender, instance, created, **kwargs): \"\"\" Signal handler to trigger parent taxon creation after saving a Taxon instance with a GBIF code. \"\"\" if instance.taxon_code is not None and instance.taxon_code != \"\" and instance.taxon_source == 1: create_taxon_parents.apply_async([instance.pk])","title":"post_taxon_save"},{"location":"reference/observation_editor/models/#observation_editor.models.update_observation_data_files","text":"Signal handler to update the Observation instance when its data files are changed. Source code in observation_editor\\models.py @receiver(m2m_changed, sender=Observation.data_files.through) def update_observation_data_files(sender, instance, action, reverse, *args, **kwargs): \"\"\" Signal handler to update the Observation instance when its data files are changed. \"\"\" if (action == 'post_add' or action == 'post_remove'): instance.save()","title":"update_observation_data_files"},{"location":"reference/observation_editor/rules/","text":"CanViewObservationDataFile Bases: R Rule to check if a user can view the data files associated with an observation. Source code in observation_editor\\rules.py class CanViewObservationDataFile(R): \"\"\" Rule to check if a user can view the data files associated with an observation. \"\"\" def check(self, user, instance=None): return perms['data_models.view_datafile'].filter( user, instance.data_files.all()).exists() def query(self, user): accumulated_q = Q(data_files__in=perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(observations__isnull=False))) return final_query(accumulated_q)","title":"rules"},{"location":"reference/observation_editor/rules/#observation_editor.rules.CanViewObservationDataFile","text":"Bases: R Rule to check if a user can view the data files associated with an observation. Source code in observation_editor\\rules.py class CanViewObservationDataFile(R): \"\"\" Rule to check if a user can view the data files associated with an observation. \"\"\" def check(self, user, instance=None): return perms['data_models.view_datafile'].filter( user, instance.data_files.all()).exists() def query(self, user): accumulated_q = Q(data_files__in=perms['data_models.view_datafile'].filter( user, DataFile.objects.filter(observations__isnull=False))) return final_query(accumulated_q)","title":"CanViewObservationDataFile"},{"location":"reference/observation_editor/serializers/","text":"EvenShorterTaxonSerialier Bases: ModelSerializer An even more concise serializer for the Taxon model, exposing only a few fields. Adds a combined string of species name and common name. Source code in observation_editor\\serializers.py class EvenShorterTaxonSerialier(serializers.ModelSerializer): \"\"\" An even more concise serializer for the Taxon model, exposing only a few fields. Adds a combined string of species name and common name. \"\"\" class Meta: model = Taxon fields = [\"id\", \"species_name\", \"species_common_name\", \"taxon_source\"] def to_representation(self, instance): \"\"\" Adds a 'full_string' field combining species name and common name. \"\"\" initial_rep = super(EvenShorterTaxonSerialier, self).to_representation(instance) initial_rep[\"full_string\"] = f\"{initial_rep.get('species_name')} - {initial_rep.get('species_common_name')}\" return initial_rep to_representation(instance) Adds a 'full_string' field combining species name and common name. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Adds a 'full_string' field combining species name and common name. \"\"\" initial_rep = super(EvenShorterTaxonSerialier, self).to_representation(instance) initial_rep[\"full_string\"] = f\"{initial_rep.get('species_name')} - {initial_rep.get('species_common_name')}\" return initial_rep ObservationSerializer Bases: OwnerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Serializer for the Observation model, providing detailed taxon handling. Handles relationships to Taxon and customizes output based on context. Source code in observation_editor\\serializers.py class ObservationSerializer(OwnerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Serializer for the Observation model, providing detailed taxon handling. Handles relationships to Taxon and customizes output based on context. \"\"\" taxon_obj = ShortTaxonSerializer( source='taxon', read_only=True, required=False) taxon = serializers.PrimaryKeyRelatedField( queryset=Taxon.objects.all(), required=False) species_name = SlugRelatedGetOrCreateField(many=False, source=\"taxon\", slug_field=\"species_name\", queryset=Taxon.objects.all(), allow_null=True, required=False, read_only=False) def to_representation(self, instance): \"\"\" Customizes output: handles target taxon levels, annotator display, and merging taxon info. \"\"\" initial_rep = super(ObservationSerializer, self).to_representation(instance) initial_rep.pop(\"species_name\") original_taxon_obj = initial_rep.pop(\"taxon_obj\") if (target_taxon_level := self.context.get(\"target_taxon_level\")) is not None: target_taxon_level = int(target_taxon_level) if original_taxon_obj[\"taxonomic_level\"] == target_taxon_level: initial_rep.update(original_taxon_obj) else: try: new_taxon_obj = Taxon.objects.get( pk=instance.parent_taxon_pk) except Taxon.DoesNotExist: new_taxon_obj = None if new_taxon_obj is not None: new_taxon_dict = ShortTaxonSerializer(new_taxon_obj).data initial_rep.update(new_taxon_dict) else: return None else: initial_rep.update(original_taxon_obj) if instance.owner: initial_rep[\"annotated_by\"] = f\"{instance.owner.first_name} {instance.owner.last_name}\" else: initial_rep[\"annotated_by\"] = None return initial_rep def validate(self, data): \"\"\" Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'taxon', 'species_name', data, Taxon, self.form_submission ) if not result: raise serializers.ValidationError(message) return data class Meta: model = Observation exclude = [] to_representation(instance) Customizes output: handles target taxon levels, annotator display, and merging taxon info. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Customizes output: handles target taxon levels, annotator display, and merging taxon info. \"\"\" initial_rep = super(ObservationSerializer, self).to_representation(instance) initial_rep.pop(\"species_name\") original_taxon_obj = initial_rep.pop(\"taxon_obj\") if (target_taxon_level := self.context.get(\"target_taxon_level\")) is not None: target_taxon_level = int(target_taxon_level) if original_taxon_obj[\"taxonomic_level\"] == target_taxon_level: initial_rep.update(original_taxon_obj) else: try: new_taxon_obj = Taxon.objects.get( pk=instance.parent_taxon_pk) except Taxon.DoesNotExist: new_taxon_obj = None if new_taxon_obj is not None: new_taxon_dict = ShortTaxonSerializer(new_taxon_obj).data initial_rep.update(new_taxon_dict) else: return None else: initial_rep.update(original_taxon_obj) if instance.owner: initial_rep[\"annotated_by\"] = f\"{instance.owner.first_name} {instance.owner.last_name}\" else: initial_rep[\"annotated_by\"] = None return initial_rep validate(data) Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. Source code in observation_editor\\serializers.py def validate(self, data): \"\"\" Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'taxon', 'species_name', data, Taxon, self.form_submission ) if not result: raise serializers.ValidationError(message) return data ShortTaxonSerializer Bases: ModelSerializer A shorter serializer for the Taxon model, excluding certain fields. Adds the taxonomic level display, and renames 'extra_data' to 'taxon_extra_data' in output. Source code in observation_editor\\serializers.py class ShortTaxonSerializer(serializers.ModelSerializer): \"\"\" A shorter serializer for the Taxon model, excluding certain fields. Adds the taxonomic level display, and renames 'extra_data' to 'taxon_extra_data' in output. \"\"\" taxonomic_level_name = serializers.CharField( source=\"get_taxonomic_level_display\", read_only=True) class Meta: model = Taxon exclude = [\"id\", \"created_on\", \"modified_on\", \"parents\"] def to_representation(self, instance): \"\"\" Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. \"\"\" initial_rep = super(ShortTaxonSerializer, self).to_representation(instance) initial_rep[\"taxon_extra_data\"] = initial_rep.pop(\"extra_data\") return initial_rep to_representation(instance) Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. \"\"\" initial_rep = super(ShortTaxonSerializer, self).to_representation(instance) initial_rep[\"taxon_extra_data\"] = initial_rep.pop(\"extra_data\") return initial_rep TaxonSerializer Bases: CreatedModifiedMixIn , ModelSerializer Serializer for the Taxon model, including all fields. Adds a read-only field for the taxonomic level display name. Source code in observation_editor\\serializers.py class TaxonSerializer(CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for the Taxon model, including all fields. Adds a read-only field for the taxonomic level display name. \"\"\" taxonomic_level_name = serializers.CharField( source=\"get_taxonomic_level_display\", read_only=True) class Meta: model = Taxon exclude = []","title":"serializers"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.EvenShorterTaxonSerialier","text":"Bases: ModelSerializer An even more concise serializer for the Taxon model, exposing only a few fields. Adds a combined string of species name and common name. Source code in observation_editor\\serializers.py class EvenShorterTaxonSerialier(serializers.ModelSerializer): \"\"\" An even more concise serializer for the Taxon model, exposing only a few fields. Adds a combined string of species name and common name. \"\"\" class Meta: model = Taxon fields = [\"id\", \"species_name\", \"species_common_name\", \"taxon_source\"] def to_representation(self, instance): \"\"\" Adds a 'full_string' field combining species name and common name. \"\"\" initial_rep = super(EvenShorterTaxonSerialier, self).to_representation(instance) initial_rep[\"full_string\"] = f\"{initial_rep.get('species_name')} - {initial_rep.get('species_common_name')}\" return initial_rep","title":"EvenShorterTaxonSerialier"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.EvenShorterTaxonSerialier.to_representation","text":"Adds a 'full_string' field combining species name and common name. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Adds a 'full_string' field combining species name and common name. \"\"\" initial_rep = super(EvenShorterTaxonSerialier, self).to_representation(instance) initial_rep[\"full_string\"] = f\"{initial_rep.get('species_name')} - {initial_rep.get('species_common_name')}\" return initial_rep","title":"to_representation"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.ObservationSerializer","text":"Bases: OwnerMixIn , CreatedModifiedMixIn , CheckFormMixIn , ModelSerializer Serializer for the Observation model, providing detailed taxon handling. Handles relationships to Taxon and customizes output based on context. Source code in observation_editor\\serializers.py class ObservationSerializer(OwnerMixIn, CreatedModifiedMixIn, CheckFormMixIn, serializers.ModelSerializer): \"\"\" Serializer for the Observation model, providing detailed taxon handling. Handles relationships to Taxon and customizes output based on context. \"\"\" taxon_obj = ShortTaxonSerializer( source='taxon', read_only=True, required=False) taxon = serializers.PrimaryKeyRelatedField( queryset=Taxon.objects.all(), required=False) species_name = SlugRelatedGetOrCreateField(many=False, source=\"taxon\", slug_field=\"species_name\", queryset=Taxon.objects.all(), allow_null=True, required=False, read_only=False) def to_representation(self, instance): \"\"\" Customizes output: handles target taxon levels, annotator display, and merging taxon info. \"\"\" initial_rep = super(ObservationSerializer, self).to_representation(instance) initial_rep.pop(\"species_name\") original_taxon_obj = initial_rep.pop(\"taxon_obj\") if (target_taxon_level := self.context.get(\"target_taxon_level\")) is not None: target_taxon_level = int(target_taxon_level) if original_taxon_obj[\"taxonomic_level\"] == target_taxon_level: initial_rep.update(original_taxon_obj) else: try: new_taxon_obj = Taxon.objects.get( pk=instance.parent_taxon_pk) except Taxon.DoesNotExist: new_taxon_obj = None if new_taxon_obj is not None: new_taxon_dict = ShortTaxonSerializer(new_taxon_obj).data initial_rep.update(new_taxon_dict) else: return None else: initial_rep.update(original_taxon_obj) if instance.owner: initial_rep[\"annotated_by\"] = f\"{instance.owner.first_name} {instance.owner.last_name}\" else: initial_rep[\"annotated_by\"] = None return initial_rep def validate(self, data): \"\"\" Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'taxon', 'species_name', data, Taxon, self.form_submission ) if not result: raise serializers.ValidationError(message) return data class Meta: model = Observation exclude = []","title":"ObservationSerializer"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.ObservationSerializer.to_representation","text":"Customizes output: handles target taxon levels, annotator display, and merging taxon info. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Customizes output: handles target taxon levels, annotator display, and merging taxon info. \"\"\" initial_rep = super(ObservationSerializer, self).to_representation(instance) initial_rep.pop(\"species_name\") original_taxon_obj = initial_rep.pop(\"taxon_obj\") if (target_taxon_level := self.context.get(\"target_taxon_level\")) is not None: target_taxon_level = int(target_taxon_level) if original_taxon_obj[\"taxonomic_level\"] == target_taxon_level: initial_rep.update(original_taxon_obj) else: try: new_taxon_obj = Taxon.objects.get( pk=instance.parent_taxon_pk) except Taxon.DoesNotExist: new_taxon_obj = None if new_taxon_obj is not None: new_taxon_dict = ShortTaxonSerializer(new_taxon_obj).data initial_rep.update(new_taxon_dict) else: return None else: initial_rep.update(original_taxon_obj) if instance.owner: initial_rep[\"annotated_by\"] = f\"{instance.owner.first_name} {instance.owner.last_name}\" else: initial_rep[\"annotated_by\"] = None return initial_rep","title":"to_representation"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.ObservationSerializer.validate","text":"Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. Source code in observation_editor\\serializers.py def validate(self, data): \"\"\" Validates that either 'taxon' or 'species_name' is provided during creation, using custom two-key check logic. \"\"\" data = super().validate(data) if not self.partial: result, message, data = check_two_keys( 'taxon', 'species_name', data, Taxon, self.form_submission ) if not result: raise serializers.ValidationError(message) return data","title":"validate"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.ShortTaxonSerializer","text":"Bases: ModelSerializer A shorter serializer for the Taxon model, excluding certain fields. Adds the taxonomic level display, and renames 'extra_data' to 'taxon_extra_data' in output. Source code in observation_editor\\serializers.py class ShortTaxonSerializer(serializers.ModelSerializer): \"\"\" A shorter serializer for the Taxon model, excluding certain fields. Adds the taxonomic level display, and renames 'extra_data' to 'taxon_extra_data' in output. \"\"\" taxonomic_level_name = serializers.CharField( source=\"get_taxonomic_level_display\", read_only=True) class Meta: model = Taxon exclude = [\"id\", \"created_on\", \"modified_on\", \"parents\"] def to_representation(self, instance): \"\"\" Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. \"\"\" initial_rep = super(ShortTaxonSerializer, self).to_representation(instance) initial_rep[\"taxon_extra_data\"] = initial_rep.pop(\"extra_data\") return initial_rep","title":"ShortTaxonSerializer"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.ShortTaxonSerializer.to_representation","text":"Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. Source code in observation_editor\\serializers.py def to_representation(self, instance): \"\"\" Customize the output representation by renaming 'extra_data' to 'taxon_extra_data'. \"\"\" initial_rep = super(ShortTaxonSerializer, self).to_representation(instance) initial_rep[\"taxon_extra_data\"] = initial_rep.pop(\"extra_data\") return initial_rep","title":"to_representation"},{"location":"reference/observation_editor/serializers/#observation_editor.serializers.TaxonSerializer","text":"Bases: CreatedModifiedMixIn , ModelSerializer Serializer for the Taxon model, including all fields. Adds a read-only field for the taxonomic level display name. Source code in observation_editor\\serializers.py class TaxonSerializer(CreatedModifiedMixIn, serializers.ModelSerializer): \"\"\" Serializer for the Taxon model, including all fields. Adds a read-only field for the taxonomic level display name. \"\"\" taxonomic_level_name = serializers.CharField( source=\"get_taxonomic_level_display\", read_only=True) class Meta: model = Taxon exclude = []","title":"TaxonSerializer"},{"location":"reference/observation_editor/tasks/","text":"create_taxon_parents(taxon_pk) Generate and set parent taxon objects for a given taxon. Parameters: taxon_pk ( _type_ ) \u2013 Primary key of the taxon object to process. Source code in observation_editor\\tasks.py @app.task() def create_taxon_parents(taxon_pk): \"\"\" Generate and set parent taxon objects for a given taxon. Args: taxon_pk (_type_): Primary key of the taxon object to process. \"\"\" from .models import Taxon taxon_object = Taxon.objects.get(pk=taxon_pk) logger.info(f\"Check parents {taxon_object.species_name}\") try: species_data = GBIF_get_species(taxon_object.taxon_code) all_taxon_codes = GBIF_get_taxoncodes(species_data) except ConnectionError or ConnectTimeout as e: logger.error(e) return all_parents = [] if len(all_taxon_codes) > 1: for i in range(1, len(all_taxon_codes)): try: parent_taxon_obj, created = GBIF_get_or_create_taxon_object_from_taxon_code( all_taxon_codes[i][0]) except ConnectionError or ConnectTimeout as e: logger.error(e) return all_parents.append(parent_taxon_obj) if created: logger.info(f\"Create {parent_taxon_obj.species_name}\") taxon_object.parents.set(all_parents) logger.info(f\"Set parents {taxon_object.species_name}\")","title":"tasks"},{"location":"reference/observation_editor/tasks/#observation_editor.tasks.create_taxon_parents","text":"Generate and set parent taxon objects for a given taxon. Parameters: taxon_pk ( _type_ ) \u2013 Primary key of the taxon object to process. Source code in observation_editor\\tasks.py @app.task() def create_taxon_parents(taxon_pk): \"\"\" Generate and set parent taxon objects for a given taxon. Args: taxon_pk (_type_): Primary key of the taxon object to process. \"\"\" from .models import Taxon taxon_object = Taxon.objects.get(pk=taxon_pk) logger.info(f\"Check parents {taxon_object.species_name}\") try: species_data = GBIF_get_species(taxon_object.taxon_code) all_taxon_codes = GBIF_get_taxoncodes(species_data) except ConnectionError or ConnectTimeout as e: logger.error(e) return all_parents = [] if len(all_taxon_codes) > 1: for i in range(1, len(all_taxon_codes)): try: parent_taxon_obj, created = GBIF_get_or_create_taxon_object_from_taxon_code( all_taxon_codes[i][0]) except ConnectionError or ConnectTimeout as e: logger.error(e) return all_parents.append(parent_taxon_obj) if created: logger.info(f\"Create {parent_taxon_obj.species_name}\") taxon_object.parents.set(all_parents) logger.info(f\"Set parents {taxon_object.species_name}\")","title":"create_taxon_parents"},{"location":"reference/observation_editor/viewsets/","text":"ObservationViewSet Bases: CheckAttachmentViewSetMixIn , AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn A ViewSet for listing, creating, updating, and deleting observations. Source code in observation_editor\\viewsets.py @extend_schema(summary=\"Observations\", description=\"Observations are annotations of datafiles, made by human or AI.\", tags=[\"Observations\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyObservationSerializer, request=DummyObservationSerializer, ) @extend_schema_view( list=extend_schema(summary='List observations', parameters=obs_extra_parameters, ), retrieve=extend_schema(summary='Get a single observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to get.\")]), partial_update=extend_schema(summary='Partially update an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to update.\")]), update=extend_schema(summary='Update an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to update.\")]), create=extend_schema(summary='Create an observation'), destroy=extend_schema(summary='Delete an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to delete.\")]), datafile_observations=extend_schema(summary=\"Observations from datafile\", description=\"Get observations from a specific datafile.\", filters=True, parameters=obs_extra_parameters + [ OpenApiParameter( \"datafile_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile from which to get observations.\")]), deployment_observations=extend_schema(summary=\"Observations from deployment\", filters=True, description=\"Get observations from a specific deployment.\", parameters=obs_extra_parameters + [ OpenApiParameter( \"deployment_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment from which to get observations.\")]), ) class ObservationViewSet(CheckAttachmentViewSetMixIn, AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" A ViewSet for listing, creating, updating, and deleting observations. \"\"\" search_fields = [\"taxon__species_name\", \"taxon__species_common_name\"] ordering_fields = [\"obs_dt\", \"created_on\"] queryset = Observation.objects.all().prefetch_related(\"taxon\") serializer_class = ObservationSerializer filter_backend = viewsets.ModelViewSet.filter_backends filterset_class = ObservationFilter def get_queryset(self): qs = Observation.objects.all().prefetch_related(\"taxon\") if (target_taxon_level := self.request.GET.get(\"target_taxon_level\")) is not None: qs = qs.get_taxonomic_level(target_taxon_level).filter( parent_taxon_pk__isnull=False) if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_obs_qs(qs) return qs def get_serializer_class(self): if 'ctdp' in self.request.GET.keys(): return ObservationSerializerCTDP else: return ObservationSerializer def get_serializer_context(self): context = super().get_serializer_context() context.update( {\"target_taxon_level\": self.request.GET.get(\"target_taxon_level\")}) return context def check_attachment(self, serializer): data_files_objects = serializer.validated_data.get('data_files') if data_files_objects is not None: for data_file_object in data_files_objects: if not self.request.user.has_perm('data_models.annotate_datafile', data_file_object): raise PermissionDenied( f\"You don't have permission to add an observation to {data_file_object.file_name}\") @action(detail=False, methods=['get'], url_path=r'datafile/(?P<datafile_id>\\w+)', url_name=\"datafile_observations\") def datafile_observations(self, request, datafile_id=None): # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files__pk=datafile_id).select_related('taxon', 'owner') observation_qs = self.filter_queryset(observation_qs) if 'ctdp' in self.request.GET.keys(): observation_qs = get_ctdp_obs_qs(observation_qs) # Paginate the queryset page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_id>\\w+)', url_name=\"deployment_observations\") def deployment_observations(self, request, deployment_id=None): # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files__deployment=deployment_id).select_related('taxon', 'owner') observation_qs = self.filter_queryset(observation_qs) if 'ctdp' in self.request.GET.keys(): observation_qs = get_ctdp_obs_qs(observation_qs) # Paginate the queryset page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = self.get_serializer( observation_qs, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK) TaxonAutocompleteViewset Bases: ReadOnlyModelViewSet A ViewSet for providing autocomplete functionality for taxon names. This ViewSet allows users to search for taxon names and common names, returning a limited number of results for efficient searching. It integrates with the GBIF species search API to supplement local taxon data. Source code in observation_editor\\viewsets.py @extend_schema( exclude=True ) class TaxonAutocompleteViewset(viewsets.ReadOnlyModelViewSet): \"\"\" A ViewSet for providing autocomplete functionality for taxon names. This ViewSet allows users to search for taxon names and common names, returning a limited number of results for efficient searching. It integrates with the GBIF species search API to supplement local taxon data. \"\"\" http_method_names = ['get'] search_fields = [\"species_name\", \"species_common_name\"] queryset = Taxon.objects.all().distinct() serializer_class = EvenShorterTaxonSerialier pagination.PageNumberPagination.page_size = 5 def list(self, request, pk=None): queryset = self.filter_queryset(self.get_queryset()) page = self.paginate_queryset(queryset) serializer = self.serializer_class(page, many=True) serializer_data = serializer.data if (n_database_records := len(serializer_data)) < pagination.PageNumberPagination.page_size: try: gbif_record_n = pagination.PageNumberPagination.page_size - n_database_records existing_species = [x.get(\"species_name\") for x in serializer_data] gbif_results, scores = GBIF_species_search( self.request.GET.get(\"search\")) gbif_results = [x for x in gbif_results if (canon_name := x.get( \"canonicalName\")) and canon_name not in existing_species and canon_name != \"\"] if len(gbif_results) > 0: gbif_results = gbif_results[:gbif_record_n] new_gbif_results = [] for gbif_result in gbif_results: if (vernacular_name := gbif_result.get(\"vernacularName\")) is None: vernacular_names = gbif_result.get( \"vernacularNames\", []) vernacular_name = \"\" for x in vernacular_names: if x.get(\"language\", \"\") == \"eng\": vernacular_name = x.get( \"vernacularName\", \"\") break new_gbif_result = {\"id\": \"\", \"species_name\": gbif_result.get(\"canonicalName\", \"\"), \"species_common_name\": vernacular_name, \"taxon_souce\": 1} new_gbif_results.append(new_gbif_result) serializer_data += new_gbif_results except Exception as e: logger.error(e) return self.get_paginated_response(serializer_data)","title":"viewsets"},{"location":"reference/observation_editor/viewsets/#observation_editor.viewsets.ObservationViewSet","text":"Bases: CheckAttachmentViewSetMixIn , AddOwnerViewSetMixIn , OptionalPaginationViewSetMixIn A ViewSet for listing, creating, updating, and deleting observations. Source code in observation_editor\\viewsets.py @extend_schema(summary=\"Observations\", description=\"Observations are annotations of datafiles, made by human or AI.\", tags=[\"Observations\"], methods=[\"get\", \"post\", \"put\", \"patch\", \"delete\"], responses=DummyObservationSerializer, request=DummyObservationSerializer, ) @extend_schema_view( list=extend_schema(summary='List observations', parameters=obs_extra_parameters, ), retrieve=extend_schema(summary='Get a single observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to get.\")]), partial_update=extend_schema(summary='Partially update an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to update.\")]), update=extend_schema(summary='Update an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to update.\")]), create=extend_schema(summary='Create an observation'), destroy=extend_schema(summary='Delete an observation', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of observation to delete.\")]), datafile_observations=extend_schema(summary=\"Observations from datafile\", description=\"Get observations from a specific datafile.\", filters=True, parameters=obs_extra_parameters + [ OpenApiParameter( \"datafile_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of datafile from which to get observations.\")]), deployment_observations=extend_schema(summary=\"Observations from deployment\", filters=True, description=\"Get observations from a specific deployment.\", parameters=obs_extra_parameters + [ OpenApiParameter( \"deployment_id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of deployment from which to get observations.\")]), ) class ObservationViewSet(CheckAttachmentViewSetMixIn, AddOwnerViewSetMixIn, OptionalPaginationViewSetMixIn): \"\"\" A ViewSet for listing, creating, updating, and deleting observations. \"\"\" search_fields = [\"taxon__species_name\", \"taxon__species_common_name\"] ordering_fields = [\"obs_dt\", \"created_on\"] queryset = Observation.objects.all().prefetch_related(\"taxon\") serializer_class = ObservationSerializer filter_backend = viewsets.ModelViewSet.filter_backends filterset_class = ObservationFilter def get_queryset(self): qs = Observation.objects.all().prefetch_related(\"taxon\") if (target_taxon_level := self.request.GET.get(\"target_taxon_level\")) is not None: qs = qs.get_taxonomic_level(target_taxon_level).filter( parent_taxon_pk__isnull=False) if 'ctdp' in self.request.GET.keys(): qs = get_ctdp_obs_qs(qs) return qs def get_serializer_class(self): if 'ctdp' in self.request.GET.keys(): return ObservationSerializerCTDP else: return ObservationSerializer def get_serializer_context(self): context = super().get_serializer_context() context.update( {\"target_taxon_level\": self.request.GET.get(\"target_taxon_level\")}) return context def check_attachment(self, serializer): data_files_objects = serializer.validated_data.get('data_files') if data_files_objects is not None: for data_file_object in data_files_objects: if not self.request.user.has_perm('data_models.annotate_datafile', data_file_object): raise PermissionDenied( f\"You don't have permission to add an observation to {data_file_object.file_name}\") @action(detail=False, methods=['get'], url_path=r'datafile/(?P<datafile_id>\\w+)', url_name=\"datafile_observations\") def datafile_observations(self, request, datafile_id=None): # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files__pk=datafile_id).select_related('taxon', 'owner') observation_qs = self.filter_queryset(observation_qs) if 'ctdp' in self.request.GET.keys(): observation_qs = get_ctdp_obs_qs(observation_qs) # Paginate the queryset page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK) @action(detail=False, methods=['get'], url_path=r'deployment/(?P<deployment_id>\\w+)', url_name=\"deployment_observations\") def deployment_observations(self, request, deployment_id=None): # Filter observations based on URL query parameters observation_qs = Observation.objects.filter( data_files__deployment=deployment_id).select_related('taxon', 'owner') observation_qs = self.filter_queryset(observation_qs) if 'ctdp' in self.request.GET.keys(): observation_qs = get_ctdp_obs_qs(observation_qs) # Paginate the queryset page = self.paginate_queryset(observation_qs) if page is not None: observation_serializer = self.get_serializer( page, many=True, context={'request': request}) return self.get_paginated_response(observation_serializer.data) # If no pagination, serialize all data observation_serializer = self.get_serializer( observation_qs, many=True, context={'request': request}) return Response(observation_serializer.data, status=status.HTTP_200_OK)","title":"ObservationViewSet"},{"location":"reference/observation_editor/viewsets/#observation_editor.viewsets.TaxonAutocompleteViewset","text":"Bases: ReadOnlyModelViewSet A ViewSet for providing autocomplete functionality for taxon names. This ViewSet allows users to search for taxon names and common names, returning a limited number of results for efficient searching. It integrates with the GBIF species search API to supplement local taxon data. Source code in observation_editor\\viewsets.py @extend_schema( exclude=True ) class TaxonAutocompleteViewset(viewsets.ReadOnlyModelViewSet): \"\"\" A ViewSet for providing autocomplete functionality for taxon names. This ViewSet allows users to search for taxon names and common names, returning a limited number of results for efficient searching. It integrates with the GBIF species search API to supplement local taxon data. \"\"\" http_method_names = ['get'] search_fields = [\"species_name\", \"species_common_name\"] queryset = Taxon.objects.all().distinct() serializer_class = EvenShorterTaxonSerialier pagination.PageNumberPagination.page_size = 5 def list(self, request, pk=None): queryset = self.filter_queryset(self.get_queryset()) page = self.paginate_queryset(queryset) serializer = self.serializer_class(page, many=True) serializer_data = serializer.data if (n_database_records := len(serializer_data)) < pagination.PageNumberPagination.page_size: try: gbif_record_n = pagination.PageNumberPagination.page_size - n_database_records existing_species = [x.get(\"species_name\") for x in serializer_data] gbif_results, scores = GBIF_species_search( self.request.GET.get(\"search\")) gbif_results = [x for x in gbif_results if (canon_name := x.get( \"canonicalName\")) and canon_name not in existing_species and canon_name != \"\"] if len(gbif_results) > 0: gbif_results = gbif_results[:gbif_record_n] new_gbif_results = [] for gbif_result in gbif_results: if (vernacular_name := gbif_result.get(\"vernacularName\")) is None: vernacular_names = gbif_result.get( \"vernacularNames\", []) vernacular_name = \"\" for x in vernacular_names: if x.get(\"language\", \"\") == \"eng\": vernacular_name = x.get( \"vernacularName\", \"\") break new_gbif_result = {\"id\": \"\", \"species_name\": gbif_result.get(\"canonicalName\", \"\"), \"species_common_name\": vernacular_name, \"taxon_souce\": 1} new_gbif_results.append(new_gbif_result) serializer_data += new_gbif_results except Exception as e: logger.error(e) return self.get_paginated_response(serializer_data)","title":"TaxonAutocompleteViewset"},{"location":"reference/user_management/","text":"This module provides custom user management functionality for the sensor portal. It includes user registration, authentication, and profile management.","title":"user_management"},{"location":"reference/user_management/filtersets/","text":"UserFilter Bases: FilterSet FilterSet for filtering User objects. This filter allows filtering by user ID, with an option to exclude certain IDs. Source code in user_management\\filtersets.py class UserFilter(django_filters.FilterSet): \"\"\" FilterSet for filtering User objects. This filter allows filtering by user ID, with an option to exclude certain IDs. \"\"\" id__not_in = NumberInFilter( field_name=\"id\", lookup_expr=\"in\", exclude=True ) class Meta: model = User fields = {'id': ['exact', 'in']}","title":"filtersets"},{"location":"reference/user_management/filtersets/#user_management.filtersets.UserFilter","text":"Bases: FilterSet FilterSet for filtering User objects. This filter allows filtering by user ID, with an option to exclude certain IDs. Source code in user_management\\filtersets.py class UserFilter(django_filters.FilterSet): \"\"\" FilterSet for filtering User objects. This filter allows filtering by user ID, with an option to exclude certain IDs. \"\"\" id__not_in = NumberInFilter( field_name=\"id\", lookup_expr=\"in\", exclude=True ) class Meta: model = User fields = {'id': ['exact', 'in']}","title":"UserFilter"},{"location":"reference/user_management/models/","text":"DeviceUser Bases: User DeviceUser model that extends the User model to represent a user associated with a specific device. Source code in user_management\\models.py class DeviceUser(User): \"\"\" DeviceUser model that extends the User model to represent a user associated with a specific device. \"\"\" device = models.OneToOneField( Device, related_name=\"device_user\", on_delete=models.CASCADE, null=True, help_text=\"Device linked to this device user.\") organisation = None bio = None class Meta: verbose_name = \"DeviceUser\" verbose_name_plural = \"DeviceUsers\" pass User Bases: AbstractUser Custom user model that extends Django's AbstractUser to include additional fields such as organisation and bio, and to manage device users. Source code in user_management\\models.py class User(AbstractUser): \"\"\" Custom user model that extends Django's AbstractUser to include additional fields such as organisation and bio, and to manage device users. \"\"\" is_active = models.BooleanField( _('active'), default=False, help_text=_( 'Designates whether this user should be treated as active. ' 'Unselect this instead of deleting accounts.' ), ) organisation = models.CharField( max_length=100, blank=True, help_text=\"User organisation.\") bio = models.CharField(max_length=200, blank=True, help_text=\"User self description.\")","title":"models"},{"location":"reference/user_management/models/#user_management.models.DeviceUser","text":"Bases: User DeviceUser model that extends the User model to represent a user associated with a specific device. Source code in user_management\\models.py class DeviceUser(User): \"\"\" DeviceUser model that extends the User model to represent a user associated with a specific device. \"\"\" device = models.OneToOneField( Device, related_name=\"device_user\", on_delete=models.CASCADE, null=True, help_text=\"Device linked to this device user.\") organisation = None bio = None class Meta: verbose_name = \"DeviceUser\" verbose_name_plural = \"DeviceUsers\" pass","title":"DeviceUser"},{"location":"reference/user_management/models/#user_management.models.User","text":"Bases: AbstractUser Custom user model that extends Django's AbstractUser to include additional fields such as organisation and bio, and to manage device users. Source code in user_management\\models.py class User(AbstractUser): \"\"\" Custom user model that extends Django's AbstractUser to include additional fields such as organisation and bio, and to manage device users. \"\"\" is_active = models.BooleanField( _('active'), default=False, help_text=_( 'Designates whether this user should be treated as active. ' 'Unselect this instead of deleting accounts.' ), ) organisation = models.CharField( max_length=100, blank=True, help_text=\"User organisation.\") bio = models.CharField(max_length=200, blank=True, help_text=\"User self description.\")","title":"User"},{"location":"reference/user_management/rules/","text":"","title":"rules"},{"location":"reference/user_management/serializers/","text":"MyTokenObtainPairSerializer Bases: TokenObtainPairSerializer Custom serializer for obtaining JWT tokens, extending the default TokenObtainPairSerializer to use captcha validation. Source code in user_management\\serializers.py class MyTokenObtainPairSerializer(TokenObtainPairSerializer): \"\"\" Custom serializer for obtaining JWT tokens, extending the default TokenObtainPairSerializer to use captcha validation. \"\"\" recaptcha = ReCaptchaV3Field( action=\"login\", required_score=0.6, ) @classmethod def get_token(cls, user): token = super().get_token(user) token['username'] = user.username token['id'] = user.pk return token UserProfileSerializer Bases: ModelSerializer Serializer for the User model, used for displaying extensive user information for profile views. This serializer includes related fields for projects, devices, and deployments owned, managed, annotatable, and viewable by the user. It is used to provide a comprehensive overview of the user's interactions with the system. Source code in user_management\\serializers.py class UserProfileSerializer(serializers.ModelSerializer): \"\"\" Serializer for the User model, used for displaying extensive user information for profile views. This serializer includes related fields for projects, devices, and deployments owned, managed, annotatable, and viewable by the user. It is used to provide a comprehensive overview of the user's interactions with the system. \"\"\" owned_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects owned by the user.\") owned_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_projects', help_text=\"Database ID of projects owned by the user.\") managed_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects managed by the user.\") managed_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_projects', help_text=\"Database ID of projects managed by the user.\") annotatable_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects annotatable by the user.\") annotatable_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_projects', help_text=\"Database ID of projects annotatable by the user.\") viewable_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects viewable by the user.\") viewable_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_projects', help_text=\"Database ID of projects viewable by the user.\") owned_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices owned by the user.\") owned_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_devices', help_text=\"Database ID of devices owned by the user.\") managed_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices managed by the user.\") managed_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_devices', help_text=\"Database ID of devices managed by the user.\") annotatable_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices annotatable by the user.\") annotatable_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_devices', help_text=\"Database ID of devices annotatable by the user.\") viewable_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices viewable by the user.\") viewable_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_devices', help_text=\"Database ID of devices viewable by the user.\") owned_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments owned by the user.\") owned_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_deployments', help_text=\"Database ID of deployments owned by the user.\") managed_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of manageable managed by the user.\") managed_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_deployments', help_text=\"Database ID of deployments manageable by the user.\") annotatable_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments annotatable by the user.\") annotatable_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_deployments', help_text=\"Database ID of deployments annotatable by the user.\") viewable_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments viewable by the user.\") viewable_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_deployments', help_text=\"Database ID of deployments viewable by the user.\") class Meta: model = User fields = ('id', 'username', 'email', 'first_name', 'last_name', 'bio', 'organisation', 'owned_projects', 'owned_projects_ID', 'managed_projects', 'managed_projects_ID', 'annotatable_projects', 'annotatable_projects_ID', 'viewable_projects', 'viewable_projects_ID', 'owned_devices', 'owned_devices_ID', 'managed_devices', 'managed_devices_ID', 'annotatable_devices', 'annotatable_devices_ID', 'viewable_devices', 'viewable_devices_ID', 'owned_deployments', 'owned_deployments_ID', 'managed_deployments', 'managed_deployments_ID', 'annotatable_deployments', 'annotatable_deployments_ID', 'viewable_deployments', 'viewable_deployments_ID' ) UserSerializer Bases: ModelSerializer Serializer for the User model, used for displaying basic user information. Source code in user_management\\serializers.py class UserSerializer(serializers.ModelSerializer): \"\"\" Serializer for the User model, used for displaying basic user information. \"\"\" email = serializers.EmailField( required=True, # validators=[UniqueValidator(queryset=User.objects.all())] ) username = serializers.CharField( required=True, validators=[UniqueValidator(queryset=User.objects.all())] ) password = serializers.CharField(write_only=True) confirm_password = serializers.CharField(write_only=True) recaptcha = ReCaptchaV3Field( action=\"register\", required_score=0.8, ) def create(self, validated_data): user = User.objects.create_user(validated_data['username'], validated_data['email'], validated_data['password'], first_name=validated_data['first_name'], last_name=validated_data['last_name'], bio=validated_data['bio'], organisation=validated_data['organisation']) return user def validate(self, data): if data.get('password') != data.get('confirm_password'): raise serializers.ValidationError(\"Passwords don't match.\") data = super().validate(data) return data class Meta: model = User fields = ('id', 'username', 'password', 'bio', 'confirm_password', 'email', 'first_name', 'last_name', 'organisation', 'recaptcha')","title":"serializers"},{"location":"reference/user_management/serializers/#user_management.serializers.MyTokenObtainPairSerializer","text":"Bases: TokenObtainPairSerializer Custom serializer for obtaining JWT tokens, extending the default TokenObtainPairSerializer to use captcha validation. Source code in user_management\\serializers.py class MyTokenObtainPairSerializer(TokenObtainPairSerializer): \"\"\" Custom serializer for obtaining JWT tokens, extending the default TokenObtainPairSerializer to use captcha validation. \"\"\" recaptcha = ReCaptchaV3Field( action=\"login\", required_score=0.6, ) @classmethod def get_token(cls, user): token = super().get_token(user) token['username'] = user.username token['id'] = user.pk return token","title":"MyTokenObtainPairSerializer"},{"location":"reference/user_management/serializers/#user_management.serializers.UserProfileSerializer","text":"Bases: ModelSerializer Serializer for the User model, used for displaying extensive user information for profile views. This serializer includes related fields for projects, devices, and deployments owned, managed, annotatable, and viewable by the user. It is used to provide a comprehensive overview of the user's interactions with the system. Source code in user_management\\serializers.py class UserProfileSerializer(serializers.ModelSerializer): \"\"\" Serializer for the User model, used for displaying extensive user information for profile views. This serializer includes related fields for projects, devices, and deployments owned, managed, annotatable, and viewable by the user. It is used to provide a comprehensive overview of the user's interactions with the system. \"\"\" owned_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects owned by the user.\") owned_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_projects', help_text=\"Database ID of projects owned by the user.\") managed_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects managed by the user.\") managed_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_projects', help_text=\"Database ID of projects managed by the user.\") annotatable_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects annotatable by the user.\") annotatable_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_projects', help_text=\"Database ID of projects annotatable by the user.\") viewable_projects = serializers.SlugRelatedField( many=True, read_only=True, slug_field='project_ID', help_text=\"Project ID of projects viewable by the user.\") viewable_projects_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_projects', help_text=\"Database ID of projects viewable by the user.\") owned_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices owned by the user.\") owned_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_devices', help_text=\"Database ID of devices owned by the user.\") managed_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices managed by the user.\") managed_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_devices', help_text=\"Database ID of devices managed by the user.\") annotatable_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices annotatable by the user.\") annotatable_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_devices', help_text=\"Database ID of devices annotatable by the user.\") viewable_devices = serializers.SlugRelatedField( many=True, read_only=True, slug_field='device_ID', help_text=\"Device ID of devices viewable by the user.\") viewable_devices_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_devices', help_text=\"Database ID of devices viewable by the user.\") owned_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments owned by the user.\") owned_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='owned_deployments', help_text=\"Database ID of deployments owned by the user.\") managed_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of manageable managed by the user.\") managed_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='managed_deployments', help_text=\"Database ID of deployments manageable by the user.\") annotatable_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments annotatable by the user.\") annotatable_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='annotatable_deployments', help_text=\"Database ID of deployments annotatable by the user.\") viewable_deployments = serializers.SlugRelatedField( many=True, read_only=True, slug_field='deployment_device_ID', help_text=\"Deployment device ID of deployments viewable by the user.\") viewable_deployments_ID = serializers.PrimaryKeyRelatedField( many=True, read_only=True, source='viewable_deployments', help_text=\"Database ID of deployments viewable by the user.\") class Meta: model = User fields = ('id', 'username', 'email', 'first_name', 'last_name', 'bio', 'organisation', 'owned_projects', 'owned_projects_ID', 'managed_projects', 'managed_projects_ID', 'annotatable_projects', 'annotatable_projects_ID', 'viewable_projects', 'viewable_projects_ID', 'owned_devices', 'owned_devices_ID', 'managed_devices', 'managed_devices_ID', 'annotatable_devices', 'annotatable_devices_ID', 'viewable_devices', 'viewable_devices_ID', 'owned_deployments', 'owned_deployments_ID', 'managed_deployments', 'managed_deployments_ID', 'annotatable_deployments', 'annotatable_deployments_ID', 'viewable_deployments', 'viewable_deployments_ID' )","title":"UserProfileSerializer"},{"location":"reference/user_management/serializers/#user_management.serializers.UserSerializer","text":"Bases: ModelSerializer Serializer for the User model, used for displaying basic user information. Source code in user_management\\serializers.py class UserSerializer(serializers.ModelSerializer): \"\"\" Serializer for the User model, used for displaying basic user information. \"\"\" email = serializers.EmailField( required=True, # validators=[UniqueValidator(queryset=User.objects.all())] ) username = serializers.CharField( required=True, validators=[UniqueValidator(queryset=User.objects.all())] ) password = serializers.CharField(write_only=True) confirm_password = serializers.CharField(write_only=True) recaptcha = ReCaptchaV3Field( action=\"register\", required_score=0.8, ) def create(self, validated_data): user = User.objects.create_user(validated_data['username'], validated_data['email'], validated_data['password'], first_name=validated_data['first_name'], last_name=validated_data['last_name'], bio=validated_data['bio'], organisation=validated_data['organisation']) return user def validate(self, data): if data.get('password') != data.get('confirm_password'): raise serializers.ValidationError(\"Passwords don't match.\") data = super().validate(data) return data class Meta: model = User fields = ('id', 'username', 'password', 'bio', 'confirm_password', 'email', 'first_name', 'last_name', 'organisation', 'recaptcha')","title":"UserSerializer"},{"location":"reference/user_management/signals/","text":"check_device_user_is_manager(sender, instance, created, **kwargs) Ensures that the DeviceUser is added as a manager of the device after save. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( DeviceUser ) \u2013 The instance being saved. created ( bool ) \u2013 Whether this instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=DeviceUser) def check_device_user_is_manager(sender: type, instance: DeviceUser, created: bool, **kwargs: Any) -> None: \"\"\" Ensures that the DeviceUser is added as a manager of the device after save. Args: sender (type): The model class sending the signal. instance (DeviceUser): The instance being saved. created (bool): Whether this instance was created. **kwargs: Additional keyword arguments. \"\"\" if not instance.device.managers.all().filter(pk=instance.pk).exists(): instance.device.managers.add(instance) create_user_token(sender, instance, created, **kwargs) Signal handler to create an auth token upon creation of a User or DeviceUser. - For DeviceUser, sets password to token key if device password is not set. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Any ) \u2013 The instance being saved. created ( bool ) \u2013 Whether this instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=DeviceUser) @receiver(post_save, sender=User) def create_user_token(sender: type, instance: Any, created: bool, **kwargs: Any) -> None: \"\"\" Signal handler to create an auth token upon creation of a User or DeviceUser. - For DeviceUser, sets password to token key if device password is not set. Args: sender (type): The model class sending the signal. instance: The instance being saved. created (bool): Whether this instance was created. **kwargs: Additional keyword arguments. \"\"\" if created: newtoken = Token(user=instance) if isinstance(instance, DeviceUser): if (instance.device.password is None) or (instance.device.password != \"\"): instance.password = newtoken.key newtoken.save() on_change(sender, instance, **kwargs) Signal handler for pre-save event on User model. - Sends registration confirmation to new users. - Notifies staff on new registrations. - Sends activation/deactivation emails. - Notifies users of password changes and updates device user passwords accordingly. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( User ) \u2013 The instance being saved. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(pre_save, sender=User) def on_change(sender: type, instance: User, **kwargs: Any) -> None: \"\"\" Signal handler for pre-save event on User model. - Sends registration confirmation to new users. - Notifies staff on new registrations. - Sends activation/deactivation emails. - Notifies users of password changes and updates device user passwords accordingly. Args: sender (type): The model class sending the signal. instance (User): The instance being saved. **kwargs: Additional keyword arguments. \"\"\" from utils.email import send_email_to_user, send_email_to_users if instance.id is None: # new object will be created # send email confirming registration send_email_to_user( instance, f\"Thanks for registering with {Site.objects.get_current().name}\", f\"\"\"You have succesfully registered with {Site.objects.get_current().name} \\n A member of staff will activate your account after review. \\n You will receive an email when you account is activated.\"\"\" ) # send email alerting staff of registration send_email_to_users( User.objects.filter(is_staff=True), f\"{instance.username} registered with {Site.objects.get_current().name}\", f\"\"\"{instance.first_name} {instance.last_name} registered with {Site.objects.get_current().name} \\n Their account will need to be activated after review. \\n \"\"\" ) else: previous: User = User.objects.get(id=instance.id) if previous.is_active != instance.is_active: # field will be updated if instance.is_active: # Send email alerting user to their account send_email_to_user( instance, f\"Thanks for registering with {Site.objects.get_current().name}\", f\"\"\"Your account at {Site.objects.get_current().name} has been activated \\n Note that you may still need to be given additional permissions.\"\"\" ) else: # send email saying user account is deactivated? send_email_to_user( instance, f\"Your account at {Site.objects.get_current().name}\", f\"\"\"Your account at {Site.objects.get_current().name} has been deactivated \\n If you feel this was an error, please contact your system administrator.\"\"\" ) pass if previous.password != instance.password: if instance.is_active: # Send email alerting user their password was changed send_email_to_user( instance, f\"{Site.objects.get_current().name} - Password was changed\", f\"\"\"Your account at {Site.objects.get_current().name} has had its password changed. \\n If you did not request this password change, contact your system administrator immediately.\"\"\" ) # Set a users owned devices device users password to be their new password DeviceUser.objects.filter( device__in=instance.owned_devices.all(), password=previous.password ).update(password=instance.password) password_reset_token_created(sender, instance, reset_password_token, *args, **kwargs) Handles password reset tokens. When a token is created, an e-mail is sent to the user. Parameters: sender ( Any ) \u2013 View Class that sent the signal. instance ( Any ) \u2013 View Instance that sent the signal. reset_password_token ( Any ) \u2013 Token Model Object. *args ( Any , default: () ) \u2013 Additional positional arguments. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(reset_password_token_created) def password_reset_token_created(sender: Any, instance: Any, reset_password_token: Any, *args: Any, **kwargs: Any) -> None: \"\"\" Handles password reset tokens. When a token is created, an e-mail is sent to the user. Args: sender: View Class that sent the signal. instance: View Instance that sent the signal. reset_password_token: Token Model Object. *args: Additional positional arguments. **kwargs: Additional keyword arguments. \"\"\" from utils.email import send_email_to_user logger.info(\"Password reset token created\", sender) # send an e-mail to the user context = { 'current_user': reset_password_token.user, 'username': reset_password_token.user.username, 'email': reset_password_token.user.email, 'reset_password_url': f\"{Site.objects.get_current().domain}/reset-password/confirm/?token={reset_password_token.key}\" } # render email text email_html_message = render_to_string( 'user_reset_password.html', context) send_email_to_user( reset_password_token.user, f\"Password Reset for {reset_password_token.user.username}\", email_html_message ) post_save_device(sender, instance, created, **kwargs) Signal handler for post-save event on Device model. - Creates a DeviceUser for the device if one does not exist. - Ensures the DeviceUser is set as a manager. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Device ) \u2013 The Device instance being saved. created ( bool ) \u2013 Whether the instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=Device) def post_save_device(sender: type, instance: Device, created: bool, **kwargs: Any) -> None: \"\"\" Signal handler for post-save event on Device model. - Creates a DeviceUser for the device if one does not exist. - Ensures the DeviceUser is set as a manager. Args: sender (type): The model class sending the signal. instance (Device): The Device instance being saved. created (bool): Whether the instance was created. **kwargs: Additional keyword arguments. \"\"\" add_device_user = False if created: add_device_user = True else: try: device_user = instance.device_user except ObjectDoesNotExist: add_device_user = True if add_device_user: user_name = f\"{instance.device_ID}_user\" if (instance.username is not None) and (instance.username != \":\"): user_name = instance.username device_user = DeviceUser( username=user_name, device=instance, is_active=True ) # if (instance.password is not None) and (instance.password != \"\"): # device_user.password = instance.password if instance.owner: device_user.email = instance.owner.email if (instance.password is not None) and (instance.password != \"\"): # Set device user password to owner password device_user.password = instance.owner.password device_user.save() # always make sure device user as a manager instance.managers.add(device_user) pre_user_save(sender, instance, **kwargs) Signal handler to ensure is_active is set to True: - For DeviceUser, always set is_active to True. - For User, set is_active to True if instance is a superuser. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Any ) \u2013 The instance being saved. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(pre_save, sender=DeviceUser) @receiver(pre_save, sender=User) def pre_user_save(sender: type, instance: Any, **kwargs: Any) -> None: \"\"\" Signal handler to ensure is_active is set to True: - For DeviceUser, always set is_active to True. - For User, set is_active to True if instance is a superuser. Args: sender (type): The model class sending the signal. instance: The instance being saved. **kwargs: Additional keyword arguments. \"\"\" if isinstance(instance, DeviceUser): instance.is_active = True elif hasattr(instance, 'is_superuser') and instance.is_superuser: instance.is_active = True","title":"signals"},{"location":"reference/user_management/signals/#user_management.signals.check_device_user_is_manager","text":"Ensures that the DeviceUser is added as a manager of the device after save. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( DeviceUser ) \u2013 The instance being saved. created ( bool ) \u2013 Whether this instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=DeviceUser) def check_device_user_is_manager(sender: type, instance: DeviceUser, created: bool, **kwargs: Any) -> None: \"\"\" Ensures that the DeviceUser is added as a manager of the device after save. Args: sender (type): The model class sending the signal. instance (DeviceUser): The instance being saved. created (bool): Whether this instance was created. **kwargs: Additional keyword arguments. \"\"\" if not instance.device.managers.all().filter(pk=instance.pk).exists(): instance.device.managers.add(instance)","title":"check_device_user_is_manager"},{"location":"reference/user_management/signals/#user_management.signals.create_user_token","text":"Signal handler to create an auth token upon creation of a User or DeviceUser. - For DeviceUser, sets password to token key if device password is not set. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Any ) \u2013 The instance being saved. created ( bool ) \u2013 Whether this instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=DeviceUser) @receiver(post_save, sender=User) def create_user_token(sender: type, instance: Any, created: bool, **kwargs: Any) -> None: \"\"\" Signal handler to create an auth token upon creation of a User or DeviceUser. - For DeviceUser, sets password to token key if device password is not set. Args: sender (type): The model class sending the signal. instance: The instance being saved. created (bool): Whether this instance was created. **kwargs: Additional keyword arguments. \"\"\" if created: newtoken = Token(user=instance) if isinstance(instance, DeviceUser): if (instance.device.password is None) or (instance.device.password != \"\"): instance.password = newtoken.key newtoken.save()","title":"create_user_token"},{"location":"reference/user_management/signals/#user_management.signals.on_change","text":"Signal handler for pre-save event on User model. - Sends registration confirmation to new users. - Notifies staff on new registrations. - Sends activation/deactivation emails. - Notifies users of password changes and updates device user passwords accordingly. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( User ) \u2013 The instance being saved. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(pre_save, sender=User) def on_change(sender: type, instance: User, **kwargs: Any) -> None: \"\"\" Signal handler for pre-save event on User model. - Sends registration confirmation to new users. - Notifies staff on new registrations. - Sends activation/deactivation emails. - Notifies users of password changes and updates device user passwords accordingly. Args: sender (type): The model class sending the signal. instance (User): The instance being saved. **kwargs: Additional keyword arguments. \"\"\" from utils.email import send_email_to_user, send_email_to_users if instance.id is None: # new object will be created # send email confirming registration send_email_to_user( instance, f\"Thanks for registering with {Site.objects.get_current().name}\", f\"\"\"You have succesfully registered with {Site.objects.get_current().name} \\n A member of staff will activate your account after review. \\n You will receive an email when you account is activated.\"\"\" ) # send email alerting staff of registration send_email_to_users( User.objects.filter(is_staff=True), f\"{instance.username} registered with {Site.objects.get_current().name}\", f\"\"\"{instance.first_name} {instance.last_name} registered with {Site.objects.get_current().name} \\n Their account will need to be activated after review. \\n \"\"\" ) else: previous: User = User.objects.get(id=instance.id) if previous.is_active != instance.is_active: # field will be updated if instance.is_active: # Send email alerting user to their account send_email_to_user( instance, f\"Thanks for registering with {Site.objects.get_current().name}\", f\"\"\"Your account at {Site.objects.get_current().name} has been activated \\n Note that you may still need to be given additional permissions.\"\"\" ) else: # send email saying user account is deactivated? send_email_to_user( instance, f\"Your account at {Site.objects.get_current().name}\", f\"\"\"Your account at {Site.objects.get_current().name} has been deactivated \\n If you feel this was an error, please contact your system administrator.\"\"\" ) pass if previous.password != instance.password: if instance.is_active: # Send email alerting user their password was changed send_email_to_user( instance, f\"{Site.objects.get_current().name} - Password was changed\", f\"\"\"Your account at {Site.objects.get_current().name} has had its password changed. \\n If you did not request this password change, contact your system administrator immediately.\"\"\" ) # Set a users owned devices device users password to be their new password DeviceUser.objects.filter( device__in=instance.owned_devices.all(), password=previous.password ).update(password=instance.password)","title":"on_change"},{"location":"reference/user_management/signals/#user_management.signals.password_reset_token_created","text":"Handles password reset tokens. When a token is created, an e-mail is sent to the user. Parameters: sender ( Any ) \u2013 View Class that sent the signal. instance ( Any ) \u2013 View Instance that sent the signal. reset_password_token ( Any ) \u2013 Token Model Object. *args ( Any , default: () ) \u2013 Additional positional arguments. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(reset_password_token_created) def password_reset_token_created(sender: Any, instance: Any, reset_password_token: Any, *args: Any, **kwargs: Any) -> None: \"\"\" Handles password reset tokens. When a token is created, an e-mail is sent to the user. Args: sender: View Class that sent the signal. instance: View Instance that sent the signal. reset_password_token: Token Model Object. *args: Additional positional arguments. **kwargs: Additional keyword arguments. \"\"\" from utils.email import send_email_to_user logger.info(\"Password reset token created\", sender) # send an e-mail to the user context = { 'current_user': reset_password_token.user, 'username': reset_password_token.user.username, 'email': reset_password_token.user.email, 'reset_password_url': f\"{Site.objects.get_current().domain}/reset-password/confirm/?token={reset_password_token.key}\" } # render email text email_html_message = render_to_string( 'user_reset_password.html', context) send_email_to_user( reset_password_token.user, f\"Password Reset for {reset_password_token.user.username}\", email_html_message )","title":"password_reset_token_created"},{"location":"reference/user_management/signals/#user_management.signals.post_save_device","text":"Signal handler for post-save event on Device model. - Creates a DeviceUser for the device if one does not exist. - Ensures the DeviceUser is set as a manager. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Device ) \u2013 The Device instance being saved. created ( bool ) \u2013 Whether the instance was created. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(post_save, sender=Device) def post_save_device(sender: type, instance: Device, created: bool, **kwargs: Any) -> None: \"\"\" Signal handler for post-save event on Device model. - Creates a DeviceUser for the device if one does not exist. - Ensures the DeviceUser is set as a manager. Args: sender (type): The model class sending the signal. instance (Device): The Device instance being saved. created (bool): Whether the instance was created. **kwargs: Additional keyword arguments. \"\"\" add_device_user = False if created: add_device_user = True else: try: device_user = instance.device_user except ObjectDoesNotExist: add_device_user = True if add_device_user: user_name = f\"{instance.device_ID}_user\" if (instance.username is not None) and (instance.username != \":\"): user_name = instance.username device_user = DeviceUser( username=user_name, device=instance, is_active=True ) # if (instance.password is not None) and (instance.password != \"\"): # device_user.password = instance.password if instance.owner: device_user.email = instance.owner.email if (instance.password is not None) and (instance.password != \"\"): # Set device user password to owner password device_user.password = instance.owner.password device_user.save() # always make sure device user as a manager instance.managers.add(device_user)","title":"post_save_device"},{"location":"reference/user_management/signals/#user_management.signals.pre_user_save","text":"Signal handler to ensure is_active is set to True: - For DeviceUser, always set is_active to True. - For User, set is_active to True if instance is a superuser. Parameters: sender ( type ) \u2013 The model class sending the signal. instance ( Any ) \u2013 The instance being saved. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in user_management\\signals.py @receiver(pre_save, sender=DeviceUser) @receiver(pre_save, sender=User) def pre_user_save(sender: type, instance: Any, **kwargs: Any) -> None: \"\"\" Signal handler to ensure is_active is set to True: - For DeviceUser, always set is_active to True. - For User, set is_active to True if instance is a superuser. Args: sender (type): The model class sending the signal. instance: The instance being saved. **kwargs: Additional keyword arguments. \"\"\" if isinstance(instance, DeviceUser): instance.is_active = True elif hasattr(instance, 'is_superuser') and instance.is_superuser: instance.is_active = True","title":"pre_user_save"},{"location":"reference/user_management/views/","text":"MyTokenObtainPairView Bases: TokenObtainPairView Custom TokenObtainPairView that uses a custom serializer for obtaining JWT tokens. Source code in user_management\\views.py @extend_schema(exclude=True) class MyTokenObtainPairView(TokenObtainPairView): \"\"\" Custom TokenObtainPairView that uses a custom serializer for obtaining JWT tokens. \"\"\" serializer_class = MyTokenObtainPairSerializer MyTokenRefreshView Bases: TokenRefreshView Custom TokenRefreshView that uses the default serializer for refreshing JWT tokens. Placeholder for future customizations if needed. Could exten to use captcha as with MyTokenObtainPairView. Source code in user_management\\views.py @extend_schema(exclude=True) class MyTokenRefreshView(TokenRefreshView): \"\"\" Custom TokenRefreshView that uses the default serializer for refreshing JWT tokens. Placeholder for future customizations if needed. Could exten to use captcha as with MyTokenObtainPairView. \"\"\" pass","title":"views"},{"location":"reference/user_management/views/#user_management.views.MyTokenObtainPairView","text":"Bases: TokenObtainPairView Custom TokenObtainPairView that uses a custom serializer for obtaining JWT tokens. Source code in user_management\\views.py @extend_schema(exclude=True) class MyTokenObtainPairView(TokenObtainPairView): \"\"\" Custom TokenObtainPairView that uses a custom serializer for obtaining JWT tokens. \"\"\" serializer_class = MyTokenObtainPairSerializer","title":"MyTokenObtainPairView"},{"location":"reference/user_management/views/#user_management.views.MyTokenRefreshView","text":"Bases: TokenRefreshView Custom TokenRefreshView that uses the default serializer for refreshing JWT tokens. Placeholder for future customizations if needed. Could exten to use captcha as with MyTokenObtainPairView. Source code in user_management\\views.py @extend_schema(exclude=True) class MyTokenRefreshView(TokenRefreshView): \"\"\" Custom TokenRefreshView that uses the default serializer for refreshing JWT tokens. Placeholder for future customizations if needed. Could exten to use captcha as with MyTokenObtainPairView. \"\"\" pass","title":"MyTokenRefreshView"},{"location":"reference/user_management/viewsets/","text":"UserProfileViewSet Bases: ModelViewSet A ViewSet for managing user profiles, allowing users to retrieve their own profile. This viewset is primarily used for viewing user profiles, and it does not allow listing of all users. Users can only retrieve their own profile unless they are staff members. Source code in user_management\\viewsets.py @extend_schema(summary=\"Users\", description=\"User profiles. A user can only retrieve their own profile\", tags=[\"Users\"], methods=[\"get\"], ) @extend_schema_view(list=extend_schema(exclude=True ), retrieve=extend_schema(summary='Get user profile.', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of user to get.\")]),) class UserProfileViewSet(viewsets.ModelViewSet): \"\"\" A ViewSet for managing user profiles, allowing users to retrieve their own profile. This viewset is primarily used for viewing user profiles, and it does not allow listing of all users. Users can only retrieve their own profile unless they are staff members. \"\"\" http_method_names = ['get', 'head'] filterset_class = UserFilter serializer_class = UserProfileSerializer search_fields = ['email', 'username', 'first_name', 'last_name', 'organisation'] permission_classes = [IsAuthenticated] queryset = User.objects.filter(deviceuser__isnull=True) def list(self, request, *args, **kwargs): return Response({\"detail\": \"Method 'GET' not allowed.\"}, status=status.HTTP_405_METHOD_NOT_ALLOWED) def retrieve(self, request, pk=None, *args, **kwargs): user_obj = get_object_or_404(pk=pk, klass=self.queryset) if not request.user == user_obj and not request.user.is_staff: # Force return of the users own profile serialized_data = UserProfileSerializer(request.user) return Response(serialized_data.data, status=status.HTTP_200_OK) # return Response({\"detail\": \" You do not have permission to view this item.\"}, # status=status.HTTP_403_FORBIDDEN) return super().retrieve(request, *args, **kwargs) UserViewSet Bases: ModelViewSet A ViewSet for managing User instances, allowing users to list, retrieve, create, and search for users. This viewset is primarily used by managers. Source code in user_management\\viewsets.py @extend_schema( exclude=True ) class UserViewSet(viewsets.ModelViewSet): \"\"\" A ViewSet for managing User instances, allowing users to list, retrieve, create, and search for users. This viewset is primarily used by managers. \"\"\" http_method_names = ['get', 'post', 'head'] filterset_class = UserFilter serializer_class = UserSerializer search_fields = ['email', 'username', 'first_name', 'last_name', 'organisation'] def get_permissions(self): if self.action == \"create\": permission_classes = [AllowAny] else: permission_classes = [IsAuthenticated] return [permission() for permission in permission_classes] queryset = User.objects.filter(deviceuser__isnull=True).distinct()","title":"viewsets"},{"location":"reference/user_management/viewsets/#user_management.viewsets.UserProfileViewSet","text":"Bases: ModelViewSet A ViewSet for managing user profiles, allowing users to retrieve their own profile. This viewset is primarily used for viewing user profiles, and it does not allow listing of all users. Users can only retrieve their own profile unless they are staff members. Source code in user_management\\viewsets.py @extend_schema(summary=\"Users\", description=\"User profiles. A user can only retrieve their own profile\", tags=[\"Users\"], methods=[\"get\"], ) @extend_schema_view(list=extend_schema(exclude=True ), retrieve=extend_schema(summary='Get user profile.', parameters=[ OpenApiParameter( \"id\", OpenApiTypes.INT, OpenApiParameter.PATH, description=\"Database ID of user to get.\")]),) class UserProfileViewSet(viewsets.ModelViewSet): \"\"\" A ViewSet for managing user profiles, allowing users to retrieve their own profile. This viewset is primarily used for viewing user profiles, and it does not allow listing of all users. Users can only retrieve their own profile unless they are staff members. \"\"\" http_method_names = ['get', 'head'] filterset_class = UserFilter serializer_class = UserProfileSerializer search_fields = ['email', 'username', 'first_name', 'last_name', 'organisation'] permission_classes = [IsAuthenticated] queryset = User.objects.filter(deviceuser__isnull=True) def list(self, request, *args, **kwargs): return Response({\"detail\": \"Method 'GET' not allowed.\"}, status=status.HTTP_405_METHOD_NOT_ALLOWED) def retrieve(self, request, pk=None, *args, **kwargs): user_obj = get_object_or_404(pk=pk, klass=self.queryset) if not request.user == user_obj and not request.user.is_staff: # Force return of the users own profile serialized_data = UserProfileSerializer(request.user) return Response(serialized_data.data, status=status.HTTP_200_OK) # return Response({\"detail\": \" You do not have permission to view this item.\"}, # status=status.HTTP_403_FORBIDDEN) return super().retrieve(request, *args, **kwargs)","title":"UserProfileViewSet"},{"location":"reference/user_management/viewsets/#user_management.viewsets.UserViewSet","text":"Bases: ModelViewSet A ViewSet for managing User instances, allowing users to list, retrieve, create, and search for users. This viewset is primarily used by managers. Source code in user_management\\viewsets.py @extend_schema( exclude=True ) class UserViewSet(viewsets.ModelViewSet): \"\"\" A ViewSet for managing User instances, allowing users to list, retrieve, create, and search for users. This viewset is primarily used by managers. \"\"\" http_method_names = ['get', 'post', 'head'] filterset_class = UserFilter serializer_class = UserSerializer search_fields = ['email', 'username', 'first_name', 'last_name', 'organisation'] def get_permissions(self): if self.action == \"create\": permission_classes = [AllowAny] else: permission_classes = [IsAuthenticated] return [permission() for permission in permission_classes] queryset = User.objects.filter(deviceuser__isnull=True).distinct()","title":"UserViewSet"},{"location":"reference/utils/","text":"This module contains utility functions and classes that are used across the sensor portal. These utilities help with common tasks such as data validation, formatting, and other shared functionalities.","title":"utils"},{"location":"reference/utils/api_renderer/","text":"BrowsableAPIRendererWithoutForms Bases: BrowsableAPIRenderer Renders the browsable api, but excludes the forms. Source code in utils\\api_renderer.py class BrowsableAPIRendererWithoutForms(BrowsableAPIRenderer): \"\"\"Renders the browsable api, but excludes the forms.\"\"\" def get_context(self, *args, **kwargs): ctx = super().get_context(*args, **kwargs) ctx['display_edit_forms'] = False return ctx def show_form_for_method(self, view, method, request, obj): \"\"\"We never want to do this! So just return False.\"\"\" return False def get_rendered_html_form(self, data, view, method, request): \"\"\"Why render _any_ forms at all. This method should return rendered HTML, so let's simply return an empty string. \"\"\" return \"\" get_rendered_html_form(data, view, method, request) Why render any forms at all. This method should return rendered HTML, so let's simply return an empty string. Source code in utils\\api_renderer.py def get_rendered_html_form(self, data, view, method, request): \"\"\"Why render _any_ forms at all. This method should return rendered HTML, so let's simply return an empty string. \"\"\" return \"\" show_form_for_method(view, method, request, obj) We never want to do this! So just return False. Source code in utils\\api_renderer.py def show_form_for_method(self, view, method, request, obj): \"\"\"We never want to do this! So just return False.\"\"\" return False","title":"api_renderer"},{"location":"reference/utils/api_renderer/#utils.api_renderer.BrowsableAPIRendererWithoutForms","text":"Bases: BrowsableAPIRenderer Renders the browsable api, but excludes the forms. Source code in utils\\api_renderer.py class BrowsableAPIRendererWithoutForms(BrowsableAPIRenderer): \"\"\"Renders the browsable api, but excludes the forms.\"\"\" def get_context(self, *args, **kwargs): ctx = super().get_context(*args, **kwargs) ctx['display_edit_forms'] = False return ctx def show_form_for_method(self, view, method, request, obj): \"\"\"We never want to do this! So just return False.\"\"\" return False def get_rendered_html_form(self, data, view, method, request): \"\"\"Why render _any_ forms at all. This method should return rendered HTML, so let's simply return an empty string. \"\"\" return \"\"","title":"BrowsableAPIRendererWithoutForms"},{"location":"reference/utils/api_renderer/#utils.api_renderer.BrowsableAPIRendererWithoutForms.get_rendered_html_form","text":"Why render any forms at all. This method should return rendered HTML, so let's simply return an empty string. Source code in utils\\api_renderer.py def get_rendered_html_form(self, data, view, method, request): \"\"\"Why render _any_ forms at all. This method should return rendered HTML, so let's simply return an empty string. \"\"\" return \"\"","title":"get_rendered_html_form"},{"location":"reference/utils/api_renderer/#utils.api_renderer.BrowsableAPIRendererWithoutForms.show_form_for_method","text":"We never want to do this! So just return False. Source code in utils\\api_renderer.py def show_form_for_method(self, view, method, request, obj): \"\"\"We never want to do this! So just return False.\"\"\" return False","title":"show_form_for_method"},{"location":"reference/utils/email/","text":"send_email(to_email, subject, body) Sends an HTML email to one or more recipients. Parameters: to_email ( str | List [ str ] ) \u2013 Recipient email address(es). subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 HTML content of the email. Raises: ValueError \u2013 If 'to_email' is not provided. Source code in utils\\email.py def send_email(to_email: str | List[str], subject: str, body: str) -> None: \"\"\" Sends an HTML email to one or more recipients. Args: to_email (str | List[str]): Recipient email address(es). subject (str): Subject of the email. body (str): HTML content of the email. Raises: ValueError: If 'to_email' is not provided. \"\"\" try: settings.EMAIL_HOST_USER except AttributeError: logger.error(\"No email sender configured\") return if not to_email: raise ValueError( \"The 'to_email' address must be provided and cannot be empty.\") elif not isinstance(to_email, list): to_email = [to_email] try: html_message = render_to_string( \"email.html\", {\"body\": body}) message = EmailMessage(subject=subject, body=html_message, from_email=settings.EMAIL_HOST_USER, to=to_email) message.content_subtype = 'html' result = message.send() logger.info( f\"Sending email to {', '.join(to_email)} with subject: {subject} - Status {result}\") except Exception as e: logger.error( f\"Sending email to {', '.join(to_email)} with subject: {subject} - Status 0\") logger.error(e) send_email_to_user(user, subject, body) Sends an email to a single user if the user does not have a related deviceuser. Parameters: user ( User ) \u2013 Django User object to send the email to. subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 Email body content. Source code in utils\\email.py def send_email_to_user(user: User, subject: str, body: str) -> None: \"\"\" Sends an email to a single user if the user does not have a related deviceuser. Args: user (User): Django User object to send the email to. subject (str): Subject of the email. body (str): Email body content. \"\"\" try: user.deviceuser return except User.deviceuser.RelatedObjectDoesNotExist: pass new_body = render_to_string( \"email_body.html\", {\"user\": user, \"body\": body}) send_email(user.email, subject, new_body) send_email_to_users(users, subject, body) Sends an email with the given subject and body to a list of User objects. Parameters: users ( List [ User ] ) \u2013 List of user objects to send the email to. subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 Email body content. Source code in utils\\email.py def send_email_to_users(users: List[User], subject: str, body: str) -> None: \"\"\" Sends an email with the given subject and body to a list of User objects. Args: users (List[User]): List of user objects to send the email to. subject (str): Subject of the email. body (str): Email body content. \"\"\" for user in users: send_email_to_user(user, subject, body)","title":"email"},{"location":"reference/utils/email/#utils.email.send_email","text":"Sends an HTML email to one or more recipients. Parameters: to_email ( str | List [ str ] ) \u2013 Recipient email address(es). subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 HTML content of the email. Raises: ValueError \u2013 If 'to_email' is not provided. Source code in utils\\email.py def send_email(to_email: str | List[str], subject: str, body: str) -> None: \"\"\" Sends an HTML email to one or more recipients. Args: to_email (str | List[str]): Recipient email address(es). subject (str): Subject of the email. body (str): HTML content of the email. Raises: ValueError: If 'to_email' is not provided. \"\"\" try: settings.EMAIL_HOST_USER except AttributeError: logger.error(\"No email sender configured\") return if not to_email: raise ValueError( \"The 'to_email' address must be provided and cannot be empty.\") elif not isinstance(to_email, list): to_email = [to_email] try: html_message = render_to_string( \"email.html\", {\"body\": body}) message = EmailMessage(subject=subject, body=html_message, from_email=settings.EMAIL_HOST_USER, to=to_email) message.content_subtype = 'html' result = message.send() logger.info( f\"Sending email to {', '.join(to_email)} with subject: {subject} - Status {result}\") except Exception as e: logger.error( f\"Sending email to {', '.join(to_email)} with subject: {subject} - Status 0\") logger.error(e)","title":"send_email"},{"location":"reference/utils/email/#utils.email.send_email_to_user","text":"Sends an email to a single user if the user does not have a related deviceuser. Parameters: user ( User ) \u2013 Django User object to send the email to. subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 Email body content. Source code in utils\\email.py def send_email_to_user(user: User, subject: str, body: str) -> None: \"\"\" Sends an email to a single user if the user does not have a related deviceuser. Args: user (User): Django User object to send the email to. subject (str): Subject of the email. body (str): Email body content. \"\"\" try: user.deviceuser return except User.deviceuser.RelatedObjectDoesNotExist: pass new_body = render_to_string( \"email_body.html\", {\"user\": user, \"body\": body}) send_email(user.email, subject, new_body)","title":"send_email_to_user"},{"location":"reference/utils/email/#utils.email.send_email_to_users","text":"Sends an email with the given subject and body to a list of User objects. Parameters: users ( List [ User ] ) \u2013 List of user objects to send the email to. subject ( str ) \u2013 Subject of the email. body ( str ) \u2013 Email body content. Source code in utils\\email.py def send_email_to_users(users: List[User], subject: str, body: str) -> None: \"\"\" Sends an email with the given subject and body to a list of User objects. Args: users (List[User]): List of user objects to send the email to. subject (str): Subject of the email. body (str): Email body content. \"\"\" for user in users: send_email_to_user(user, subject, body)","title":"send_email_to_users"},{"location":"reference/utils/filtersets/","text":"ExtraDataFilterMixIn Bases: FilterSet Implements a method for filtering by extra data JSON fields. Source code in utils\\filtersets.py class ExtraDataFilterMixIn(django_filters.FilterSet): \"\"\" Implements a method for filtering by extra data JSON fields. \"\"\" extra_data = django_filters.CharFilter( method='extra_data_filter', help_text=\"Filter by extra data. Format: extra_data__key__value.\") def extra_data_filter(self, queryset, name, value): # unpack value newvalue = value.split(\"__\")[-1] newname = \"__\".join(value.split(\"__\")[:-1]) newname = name + \"__\" + newname return queryset.filter(**{ newname: newvalue, }) GenericFilterMixIn Bases: FilterSet A mixin for generic filtering of objects based on their ID, creation, and modification dates. Also provides functionality to attach help text to filters based on field names and lookup expressions. Source code in utils\\filtersets.py class GenericFilterMixIn(django_filters.FilterSet): \"\"\" A mixin for generic filtering of objects based on their ID, creation, and modification dates. Also provides functionality to attach help text to filters based on field names and lookup expressions. \"\"\" created_after = django_filters.DateFilter( field_name='created_on', lookup_expr='gt', help_text=\"Object was created after this date\") created_before = django_filters.DateFilter( field_name='created_on', lookup_expr='lte', help_text=\"Object was created before this date\") modified_after = django_filters.DateFilter( field_name='modified_on', lookup_expr='gt', help_text=\"Object was modified after this date\") modified_before = django_filters.DateFilter( field_name='modified_on', lookup_expr='lte', help_text=\"Object was modified before this date\") field_help_dict = {'id': \"Unique numeric identifier of the object.\", } lookup_expr_dict = {\"exact\": \"Exact match\", \"in\": \"Match any of the values in the list\", \"gt\": \"Greater than\", \"gte\": \"Greater than or equal to\", \"lt\": \"Less than\", \"lte\": \"Less than or equal to\", \"contains\": \"Partial match (case-insensitive)\"} class Meta: fields = { 'id': ['exact', 'in'] } @classmethod def filter_for_field(cls, f, name, lookup_expr): filter = super(GenericFilterMixIn, cls).filter_for_field( f, name, lookup_expr) help_text = f.help_text if help_text == \"\": help_text = cls.field_help_dict.get( name, name.replace(\"__\", \" \").replace('_', ' ')+\".\") new_help_text = cls.lookup_expr_dict.get( lookup_expr, lookup_expr.replace(\"__\", \" \").replace('_', ' ')) filter.extra['help_text'] = help_text + \" \" + new_help_text return filter","title":"filtersets"},{"location":"reference/utils/filtersets/#utils.filtersets.ExtraDataFilterMixIn","text":"Bases: FilterSet Implements a method for filtering by extra data JSON fields. Source code in utils\\filtersets.py class ExtraDataFilterMixIn(django_filters.FilterSet): \"\"\" Implements a method for filtering by extra data JSON fields. \"\"\" extra_data = django_filters.CharFilter( method='extra_data_filter', help_text=\"Filter by extra data. Format: extra_data__key__value.\") def extra_data_filter(self, queryset, name, value): # unpack value newvalue = value.split(\"__\")[-1] newname = \"__\".join(value.split(\"__\")[:-1]) newname = name + \"__\" + newname return queryset.filter(**{ newname: newvalue, })","title":"ExtraDataFilterMixIn"},{"location":"reference/utils/filtersets/#utils.filtersets.GenericFilterMixIn","text":"Bases: FilterSet A mixin for generic filtering of objects based on their ID, creation, and modification dates. Also provides functionality to attach help text to filters based on field names and lookup expressions. Source code in utils\\filtersets.py class GenericFilterMixIn(django_filters.FilterSet): \"\"\" A mixin for generic filtering of objects based on their ID, creation, and modification dates. Also provides functionality to attach help text to filters based on field names and lookup expressions. \"\"\" created_after = django_filters.DateFilter( field_name='created_on', lookup_expr='gt', help_text=\"Object was created after this date\") created_before = django_filters.DateFilter( field_name='created_on', lookup_expr='lte', help_text=\"Object was created before this date\") modified_after = django_filters.DateFilter( field_name='modified_on', lookup_expr='gt', help_text=\"Object was modified after this date\") modified_before = django_filters.DateFilter( field_name='modified_on', lookup_expr='lte', help_text=\"Object was modified before this date\") field_help_dict = {'id': \"Unique numeric identifier of the object.\", } lookup_expr_dict = {\"exact\": \"Exact match\", \"in\": \"Match any of the values in the list\", \"gt\": \"Greater than\", \"gte\": \"Greater than or equal to\", \"lt\": \"Less than\", \"lte\": \"Less than or equal to\", \"contains\": \"Partial match (case-insensitive)\"} class Meta: fields = { 'id': ['exact', 'in'] } @classmethod def filter_for_field(cls, f, name, lookup_expr): filter = super(GenericFilterMixIn, cls).filter_for_field( f, name, lookup_expr) help_text = f.help_text if help_text == \"\": help_text = cls.field_help_dict.get( name, name.replace(\"__\", \" \").replace('_', ' ')+\".\") new_help_text = cls.lookup_expr_dict.get( lookup_expr, lookup_expr.replace(\"__\", \" \").replace('_', ' ')) filter.extra['help_text'] = help_text + \" \" + new_help_text return filter","title":"GenericFilterMixIn"},{"location":"reference/utils/general/","text":"call_with_output(command, cwd='/', verbose=False) Calls a shell command and returns its output. Parameters: command ( str | list [ str ] ) \u2013 Command to run, either a string or a list of strings (recommended). cwd ( str , default: '/' ) \u2013 Working directory in which to run the command. Defaults to '/'. verbose ( bool , default: False ) \u2013 If True, logs the command and output to console. Defaults to False. Returns: tuple [ bool , str ] \u2013 tuple[bool, str]: (success, shell output of command) Source code in utils\\general.py def call_with_output( command: str | list[str], cwd: str = '/', verbose: bool = False ) -> tuple[bool, str]: \"\"\" Calls a shell command and returns its output. Args: command (str | list[str]): Command to run, either a string or a list of strings (recommended). cwd (str, optional): Working directory in which to run the command. Defaults to '/'. verbose (bool, optional): If True, logs the command and output to console. Defaults to False. Returns: tuple[bool, str]: (success, shell output of command) \"\"\" success = False if verbose: logger.info(command) try: output = subprocess.check_output( command, stderr=subprocess.STDOUT, cwd=cwd ).decode() success = True except subprocess.CalledProcessError as e: output = e.output.decode() except Exception as e: # check_call can raise other exceptions, such as FileNotFoundError output = str(e) if verbose: logger.info(output) return (success, output) convert_unit(size_in_bytes, unit) Convert the size from bytes to other units like KB, MB, or GB. Parameters: size_in_bytes ( int ) \u2013 Size of file in bytes. unit ( str ) \u2013 Unit to convert to ('kb', 'mb', or 'gb'). Returns: float ( float ) \u2013 Size of file in chosen unit. Source code in utils\\general.py def convert_unit(size_in_bytes: int, unit: str) -> float: \"\"\" Convert the size from bytes to other units like KB, MB, or GB. Args: size_in_bytes (int): Size of file in bytes. unit (str): Unit to convert to ('kb', 'mb', or 'gb'). Returns: float: Size of file in chosen unit. \"\"\" unit = unit.lower() if unit == 'kb': return size_in_bytes / 1024 elif unit == 'mb': return size_in_bytes / (1024 * 1024) elif unit == 'gb': return size_in_bytes / (1024 * 1024 * 1024) else: return size_in_bytes divide_chunks(list_to_chunk, chunk_size) Yield successive chunk_size-sized chunks from list_to_chunk. Parameters: list_to_chunk ( list ) \u2013 The list to divide into chunks. chunk_size ( int ) \u2013 The size of each chunk. Yields: list ( list [ Any ] ) \u2013 Chunk of the original list. Source code in utils\\general.py def divide_chunks(list_to_chunk: list[Any], chunk_size: int) -> Generator[list[Any], None, None]: \"\"\" Yield successive chunk_size-sized chunks from list_to_chunk. Args: list_to_chunk (list): The list to divide into chunks. chunk_size (int): The size of each chunk. Yields: list: Chunk of the original list. \"\"\" for i in range(0, len(list_to_chunk), chunk_size): yield list_to_chunk[i:i + chunk_size] get_md5(file_path) Get MD5 hash of file at file path. Parameters: file_path ( str ) \u2013 Path to file to be hashed. Returns: str ( str ) \u2013 MD5 hash of file. Source code in utils\\general.py def get_md5(file_path: str) -> str: \"\"\" Get MD5 hash of file at file path. Args: file_path (str): Path to file to be hashed. Returns: str: MD5 hash of file. \"\"\" hash_md5 = hashlib.md5() with open(file_path, \"rb\") as f: for chunk in iter(lambda: f.read(4096), b\"\"): hash_md5.update(chunk) return hash_md5.hexdigest() read_in_chunks(file_object, chunk_size) Generator to read a file in chunks. Parameters: file_object ( file-like object ) \u2013 File object to read from. chunk_size ( int ) \u2013 Size of each chunk in bytes. Yields: bytes ( bytes ) \u2013 Chunk of data read from the file. Source code in utils\\general.py def read_in_chunks(file_object: Any, chunk_size: int) -> Generator[bytes, None, None]: \"\"\" Generator to read a file in chunks. Args: file_object (file-like object): File object to read from. chunk_size (int): Size of each chunk in bytes. Yields: bytes: Chunk of data read from the file. \"\"\" while True: data = file_object.read(chunk_size) if not data: break yield data try_remove_file_clean_dirs(file_path) Attempt to remove a file and then clean up its parent directories. Parameters: file_path ( str ) \u2013 Path to the file to remove. Returns: bool ( bool ) \u2013 True if file and directories were removed successfully, False otherwise. Source code in utils\\general.py def try_remove_file_clean_dirs(file_path: str) -> bool: \"\"\" Attempt to remove a file and then clean up its parent directories. Args: file_path (str): Path to the file to remove. Returns: bool: True if file and directories were removed successfully, False otherwise. \"\"\" error = None try: logger.info(f\"{file_path} - Delete\") os.remove(file_path) logger.info(f\"{file_path} - Delete - succesful\") try_to_remove_dirs(os.path.split(file_path)[0]) return True except TypeError as e: error = e except OSError as e: error = e logger.error(error) logger.info(f\"{file_path} - Delete - failed\") return False try_to_remove_dirs(dir_path) Attempt to remove directories and log the outcome. Parameters: dir_path ( str ) \u2013 Path to the directories to remove. Returns: bool ( bool ) \u2013 True if removal was successful, False otherwise. Source code in utils\\general.py def try_to_remove_dirs(dir_path: str) -> bool: \"\"\" Attempt to remove directories and log the outcome. Args: dir_path (str): Path to the directories to remove. Returns: bool: True if removal was successful, False otherwise. \"\"\" try: logger.info(f\"{dir_path} - Clean dir\") os.removedirs(dir_path) logger.info(f\"{dir_path} - Clean dir - success\") return True except OSError as e: logger.error(e) logger.info(f\"{dir_path} - Clean dir - failed\") return False","title":"general"},{"location":"reference/utils/general/#utils.general.call_with_output","text":"Calls a shell command and returns its output. Parameters: command ( str | list [ str ] ) \u2013 Command to run, either a string or a list of strings (recommended). cwd ( str , default: '/' ) \u2013 Working directory in which to run the command. Defaults to '/'. verbose ( bool , default: False ) \u2013 If True, logs the command and output to console. Defaults to False. Returns: tuple [ bool , str ] \u2013 tuple[bool, str]: (success, shell output of command) Source code in utils\\general.py def call_with_output( command: str | list[str], cwd: str = '/', verbose: bool = False ) -> tuple[bool, str]: \"\"\" Calls a shell command and returns its output. Args: command (str | list[str]): Command to run, either a string or a list of strings (recommended). cwd (str, optional): Working directory in which to run the command. Defaults to '/'. verbose (bool, optional): If True, logs the command and output to console. Defaults to False. Returns: tuple[bool, str]: (success, shell output of command) \"\"\" success = False if verbose: logger.info(command) try: output = subprocess.check_output( command, stderr=subprocess.STDOUT, cwd=cwd ).decode() success = True except subprocess.CalledProcessError as e: output = e.output.decode() except Exception as e: # check_call can raise other exceptions, such as FileNotFoundError output = str(e) if verbose: logger.info(output) return (success, output)","title":"call_with_output"},{"location":"reference/utils/general/#utils.general.convert_unit","text":"Convert the size from bytes to other units like KB, MB, or GB. Parameters: size_in_bytes ( int ) \u2013 Size of file in bytes. unit ( str ) \u2013 Unit to convert to ('kb', 'mb', or 'gb'). Returns: float ( float ) \u2013 Size of file in chosen unit. Source code in utils\\general.py def convert_unit(size_in_bytes: int, unit: str) -> float: \"\"\" Convert the size from bytes to other units like KB, MB, or GB. Args: size_in_bytes (int): Size of file in bytes. unit (str): Unit to convert to ('kb', 'mb', or 'gb'). Returns: float: Size of file in chosen unit. \"\"\" unit = unit.lower() if unit == 'kb': return size_in_bytes / 1024 elif unit == 'mb': return size_in_bytes / (1024 * 1024) elif unit == 'gb': return size_in_bytes / (1024 * 1024 * 1024) else: return size_in_bytes","title":"convert_unit"},{"location":"reference/utils/general/#utils.general.divide_chunks","text":"Yield successive chunk_size-sized chunks from list_to_chunk. Parameters: list_to_chunk ( list ) \u2013 The list to divide into chunks. chunk_size ( int ) \u2013 The size of each chunk. Yields: list ( list [ Any ] ) \u2013 Chunk of the original list. Source code in utils\\general.py def divide_chunks(list_to_chunk: list[Any], chunk_size: int) -> Generator[list[Any], None, None]: \"\"\" Yield successive chunk_size-sized chunks from list_to_chunk. Args: list_to_chunk (list): The list to divide into chunks. chunk_size (int): The size of each chunk. Yields: list: Chunk of the original list. \"\"\" for i in range(0, len(list_to_chunk), chunk_size): yield list_to_chunk[i:i + chunk_size]","title":"divide_chunks"},{"location":"reference/utils/general/#utils.general.get_md5","text":"Get MD5 hash of file at file path. Parameters: file_path ( str ) \u2013 Path to file to be hashed. Returns: str ( str ) \u2013 MD5 hash of file. Source code in utils\\general.py def get_md5(file_path: str) -> str: \"\"\" Get MD5 hash of file at file path. Args: file_path (str): Path to file to be hashed. Returns: str: MD5 hash of file. \"\"\" hash_md5 = hashlib.md5() with open(file_path, \"rb\") as f: for chunk in iter(lambda: f.read(4096), b\"\"): hash_md5.update(chunk) return hash_md5.hexdigest()","title":"get_md5"},{"location":"reference/utils/general/#utils.general.read_in_chunks","text":"Generator to read a file in chunks. Parameters: file_object ( file-like object ) \u2013 File object to read from. chunk_size ( int ) \u2013 Size of each chunk in bytes. Yields: bytes ( bytes ) \u2013 Chunk of data read from the file. Source code in utils\\general.py def read_in_chunks(file_object: Any, chunk_size: int) -> Generator[bytes, None, None]: \"\"\" Generator to read a file in chunks. Args: file_object (file-like object): File object to read from. chunk_size (int): Size of each chunk in bytes. Yields: bytes: Chunk of data read from the file. \"\"\" while True: data = file_object.read(chunk_size) if not data: break yield data","title":"read_in_chunks"},{"location":"reference/utils/general/#utils.general.try_remove_file_clean_dirs","text":"Attempt to remove a file and then clean up its parent directories. Parameters: file_path ( str ) \u2013 Path to the file to remove. Returns: bool ( bool ) \u2013 True if file and directories were removed successfully, False otherwise. Source code in utils\\general.py def try_remove_file_clean_dirs(file_path: str) -> bool: \"\"\" Attempt to remove a file and then clean up its parent directories. Args: file_path (str): Path to the file to remove. Returns: bool: True if file and directories were removed successfully, False otherwise. \"\"\" error = None try: logger.info(f\"{file_path} - Delete\") os.remove(file_path) logger.info(f\"{file_path} - Delete - succesful\") try_to_remove_dirs(os.path.split(file_path)[0]) return True except TypeError as e: error = e except OSError as e: error = e logger.error(error) logger.info(f\"{file_path} - Delete - failed\") return False","title":"try_remove_file_clean_dirs"},{"location":"reference/utils/general/#utils.general.try_to_remove_dirs","text":"Attempt to remove directories and log the outcome. Parameters: dir_path ( str ) \u2013 Path to the directories to remove. Returns: bool ( bool ) \u2013 True if removal was successful, False otherwise. Source code in utils\\general.py def try_to_remove_dirs(dir_path: str) -> bool: \"\"\" Attempt to remove directories and log the outcome. Args: dir_path (str): Path to the directories to remove. Returns: bool: True if removal was successful, False otherwise. \"\"\" try: logger.info(f\"{dir_path} - Clean dir\") os.removedirs(dir_path) logger.info(f\"{dir_path} - Clean dir - success\") return True except OSError as e: logger.error(e) logger.info(f\"{dir_path} - Clean dir - failed\") return False","title":"try_to_remove_dirs"},{"location":"reference/utils/models/","text":"","title":"models"},{"location":"reference/utils/paginators/","text":"LargeTablePaginator Bases: Paginator Combination of ideas from https://gist.github.com/safar/3bbf96678f3e479b6cb683083d35cb4d https://medium.com/@hakibenita/optimizing-django-admin-paginator-53c4eb6bfca3 Overrides the count method of QuerySet objects to avoid timeouts. - Try to get the real count limiting the queryset execution time to 150 ms. - If count takes longer than 150 ms the database kills the query and raises OperationError. In that case, get an estimate instead of actual count when not filtered (this estimate can be stale and hence not fit for situations where the count of objects actually matter). - If any other exception occured fall back to default behaviour. Source code in utils\\paginators.py class LargeTablePaginator(Paginator): \"\"\" Combination of ideas from: - https://gist.github.com/safar/3bbf96678f3e479b6cb683083d35cb4d - https://medium.com/@hakibenita/optimizing-django-admin-paginator-53c4eb6bfca3 Overrides the count method of QuerySet objects to avoid timeouts. - Try to get the real count limiting the queryset execution time to 150 ms. - If count takes longer than 150 ms the database kills the query and raises OperationError. In that case, get an estimate instead of actual count when not filtered (this estimate can be stale and hence not fit for situations where the count of objects actually matter). - If any other exception occured fall back to default behaviour. \"\"\" @cached_property def count(self): \"\"\" Returns an estimated number of objects, across all pages. \"\"\" try: with transaction.atomic(), connection.cursor() as cursor: # Limit to 100 ms cursor.execute('SET LOCAL statement_timeout TO 150;') return super().count except OperationalError: pass if not self.object_list.query.where: try: with transaction.atomic(), connection.cursor() as cursor: # Obtain estimated values (only valid with PostgreSQL) cursor.execute( \"SET LOCAL statement_timeout TO 150; SELECT reltuples FROM pg_class WHERE relname = %s\", [self.object_list.query.model._meta.db_table] ) estimate = int(cursor.fetchone()[0]) return estimate except OperationalError: pass except Exception: pass return 9999999999 count() Returns an estimated number of objects, across all pages. Source code in utils\\paginators.py @cached_property def count(self): \"\"\" Returns an estimated number of objects, across all pages. \"\"\" try: with transaction.atomic(), connection.cursor() as cursor: # Limit to 100 ms cursor.execute('SET LOCAL statement_timeout TO 150;') return super().count except OperationalError: pass if not self.object_list.query.where: try: with transaction.atomic(), connection.cursor() as cursor: # Obtain estimated values (only valid with PostgreSQL) cursor.execute( \"SET LOCAL statement_timeout TO 150; SELECT reltuples FROM pg_class WHERE relname = %s\", [self.object_list.query.model._meta.db_table] ) estimate = int(cursor.fetchone()[0]) return estimate except OperationalError: pass except Exception: pass return 9999999999","title":"paginators"},{"location":"reference/utils/paginators/#utils.paginators.LargeTablePaginator","text":"Bases: Paginator Combination of ideas from https://gist.github.com/safar/3bbf96678f3e479b6cb683083d35cb4d https://medium.com/@hakibenita/optimizing-django-admin-paginator-53c4eb6bfca3 Overrides the count method of QuerySet objects to avoid timeouts. - Try to get the real count limiting the queryset execution time to 150 ms. - If count takes longer than 150 ms the database kills the query and raises OperationError. In that case, get an estimate instead of actual count when not filtered (this estimate can be stale and hence not fit for situations where the count of objects actually matter). - If any other exception occured fall back to default behaviour. Source code in utils\\paginators.py class LargeTablePaginator(Paginator): \"\"\" Combination of ideas from: - https://gist.github.com/safar/3bbf96678f3e479b6cb683083d35cb4d - https://medium.com/@hakibenita/optimizing-django-admin-paginator-53c4eb6bfca3 Overrides the count method of QuerySet objects to avoid timeouts. - Try to get the real count limiting the queryset execution time to 150 ms. - If count takes longer than 150 ms the database kills the query and raises OperationError. In that case, get an estimate instead of actual count when not filtered (this estimate can be stale and hence not fit for situations where the count of objects actually matter). - If any other exception occured fall back to default behaviour. \"\"\" @cached_property def count(self): \"\"\" Returns an estimated number of objects, across all pages. \"\"\" try: with transaction.atomic(), connection.cursor() as cursor: # Limit to 100 ms cursor.execute('SET LOCAL statement_timeout TO 150;') return super().count except OperationalError: pass if not self.object_list.query.where: try: with transaction.atomic(), connection.cursor() as cursor: # Obtain estimated values (only valid with PostgreSQL) cursor.execute( \"SET LOCAL statement_timeout TO 150; SELECT reltuples FROM pg_class WHERE relname = %s\", [self.object_list.query.model._meta.db_table] ) estimate = int(cursor.fetchone()[0]) return estimate except OperationalError: pass except Exception: pass return 9999999999","title":"LargeTablePaginator"},{"location":"reference/utils/paginators/#utils.paginators.LargeTablePaginator.count","text":"Returns an estimated number of objects, across all pages. Source code in utils\\paginators.py @cached_property def count(self): \"\"\" Returns an estimated number of objects, across all pages. \"\"\" try: with transaction.atomic(), connection.cursor() as cursor: # Limit to 100 ms cursor.execute('SET LOCAL statement_timeout TO 150;') return super().count except OperationalError: pass if not self.object_list.query.where: try: with transaction.atomic(), connection.cursor() as cursor: # Obtain estimated values (only valid with PostgreSQL) cursor.execute( \"SET LOCAL statement_timeout TO 150; SELECT reltuples FROM pg_class WHERE relname = %s\", [self.object_list.query.model._meta.db_table] ) estimate = int(cursor.fetchone()[0]) return estimate except OperationalError: pass except Exception: pass return 9999999999","title":"count"},{"location":"reference/utils/perm_functions/","text":"cascade_permissions(instance) Cascade permissions from owner through to viewer. This allows simpler queries. Parameters: instance ( Project | Deployment | Device ) \u2013 Object on which to cascade permissions Source code in utils\\perm_functions.py def cascade_permissions(instance: Project | Deployment | Device): \"\"\" Cascade permissions from owner through to viewer. This allows simpler queries. Args: instance (Project | Deployment | Device): Object on which to cascade permissions \"\"\" # OWNER SHOULD BE MANAGER if instance.owner is not None: instance.managers.add(instance.owner) # MANAGERS SHOULD BE ANNOTATORS instance.annotators.add(*instance.managers.all()) # ANNOTATORS SHOULD BE VIEWERS instance.viewers.add(*instance.annotators.all())","title":"perm_functions"},{"location":"reference/utils/perm_functions/#utils.perm_functions.cascade_permissions","text":"Cascade permissions from owner through to viewer. This allows simpler queries. Parameters: instance ( Project | Deployment | Device ) \u2013 Object on which to cascade permissions Source code in utils\\perm_functions.py def cascade_permissions(instance: Project | Deployment | Device): \"\"\" Cascade permissions from owner through to viewer. This allows simpler queries. Args: instance (Project | Deployment | Device): Object on which to cascade permissions \"\"\" # OWNER SHOULD BE MANAGER if instance.owner is not None: instance.managers.add(instance.owner) # MANAGERS SHOULD BE ANNOTATORS instance.annotators.add(*instance.managers.all()) # ANNOTATORS SHOULD BE VIEWERS instance.viewers.add(*instance.annotators.all())","title":"cascade_permissions"},{"location":"reference/utils/querysets/","text":"ApproximateCountQuerySet Bases: QuerySet A QuerySet that provides an approximate count of the number of rows in a table. This is useful for large datasets where an exact count would be too expensive to compute. Source code in utils\\querysets.py class ApproximateCountQuerySet(QuerySet): \"\"\" A QuerySet that provides an approximate count of the number of rows in a table. This is useful for large datasets where an exact count would be too expensive to compute. \"\"\" def approx_count(self): cursor = connections[self.db].cursor() cursor.execute(\"SELECT reltuples FROM pg_class \" \"WHERE relname = '%s';\" % self.model._meta.db_table) return int(cursor.fetchone()[0])","title":"querysets"},{"location":"reference/utils/querysets/#utils.querysets.ApproximateCountQuerySet","text":"Bases: QuerySet A QuerySet that provides an approximate count of the number of rows in a table. This is useful for large datasets where an exact count would be too expensive to compute. Source code in utils\\querysets.py class ApproximateCountQuerySet(QuerySet): \"\"\" A QuerySet that provides an approximate count of the number of rows in a table. This is useful for large datasets where an exact count would be too expensive to compute. \"\"\" def approx_count(self): cursor = connections[self.db].cursor() cursor.execute(\"SELECT reltuples FROM pg_class \" \"WHERE relname = '%s';\" % self.model._meta.db_table) return int(cursor.fetchone()[0])","title":"ApproximateCountQuerySet"},{"location":"reference/utils/rules/","text":"IsOwner Bases: R summary Shared rule to check if a user is the owner of an instance. Source code in utils\\rules.py class IsOwner(R): \"\"\"_summary_ Shared rule to check if a user is the owner of an instance. \"\"\" def check(self, user, instance=None): initial_bool = check_super(user) if initial_bool is not None: return initial_bool else: return user == instance.owner def query(self, user): accumulated_q = query_super(user) if accumulated_q is not None: return accumulated_q else: accumulated_q = Q(owner=user) return final_query(accumulated_q)","title":"rules"},{"location":"reference/utils/rules/#utils.rules.IsOwner","text":"Bases: R summary Shared rule to check if a user is the owner of an instance. Source code in utils\\rules.py class IsOwner(R): \"\"\"_summary_ Shared rule to check if a user is the owner of an instance. \"\"\" def check(self, user, instance=None): initial_bool = check_super(user) if initial_bool is not None: return initial_bool else: return user == instance.owner def query(self, user): accumulated_q = query_super(user) if accumulated_q is not None: return accumulated_q else: accumulated_q = Q(owner=user) return final_query(accumulated_q)","title":"IsOwner"},{"location":"reference/utils/serializers/","text":"CheckFormMixIn A mixin to check if the data was submitted through a form. It retrieves the form submission from the context and makes it available as self.form_submission in the serializer. Source code in utils\\serializers.py class CheckFormMixIn(): \"\"\" A mixin to check if the data was submitted through a form. It retrieves the form submission from the context and makes it available as `self.form_submission` in the serializer. \"\"\" def __init__(self, *args, **kwargs): super(CheckFormMixIn, self).__init__(*args, **kwargs) self.form_submission = self.context.get(\"form\") CreatedModifiedMixIn Bases: ModelSerializer A mixin to add created_on and modified_on fields to a serializer. Source code in utils\\serializers.py class CreatedModifiedMixIn(serializers.ModelSerializer): \"\"\" A mixin to add created_on and modified_on fields to a serializer. \"\"\" created_on = serializers.DateTimeField( default_timezone=djtimezone.utc, read_only=True, required=False) modified_on = serializers.DateTimeField( default_timezone=djtimezone.utc, read_only=True, required=False) InstanceGetMixIn A mixin to retrieve an attribute from the instance or data. If the attribute is not found in the data, it checks if the instance has the attribute. If the attribute is found in the instance, it returns that value; otherwise, it returns None. Source code in utils\\serializers.py class InstanceGetMixIn(): \"\"\" A mixin to retrieve an attribute from the instance or data. If the attribute is not found in the data, it checks if the instance has the attribute. If the attribute is found in the instance, it returns that value; otherwise, it returns None. \"\"\" def instance_get(self, attr_name: str, data: dict) -> any: if attr_name in data: return data[attr_name] if self.instance and hasattr(self.instance, attr_name): return getattr(self.instance, attr_name) return None ManagerMixIn Bases: ModelSerializer A mixin to add management-related fields to a serializer. It includes fields for managers, annotators, and viewers, allowing them to be set by slug or primary key. The to_representation method customizes the output to include user permissions and conditionally removes management-related fields based on the user's permissions. The update method ensures that the instance is saved after updating the management-related fields. Source code in utils\\serializers.py class ManagerMixIn(serializers.ModelSerializer): \"\"\" A mixin to add management-related fields to a serializer. It includes fields for managers, annotators, and viewers, allowing them to be set by slug or primary key. The `to_representation` method customizes the output to include user permissions and conditionally removes management-related fields based on the user's permissions. The `update` method ensures that the instance is saved after updating the management-related fields. \"\"\" managers = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) managers_ID = serializers.PrimaryKeyRelatedField(source=\"managers\", many=True, queryset=User.objects.all(), required=False) annotators = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) annotators_ID = serializers.PrimaryKeyRelatedField(source=\"annotators\", many=True, queryset=User.objects.all(), required=False) viewers = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) viewers_ID = serializers.PrimaryKeyRelatedField(source=\"viewers\", many=True, queryset=User.objects.all(), required=False) # viewers = UserGroupMemberSerializer( # many=True, read_only=False, source='usergroup') # annotators = UserGroupMemberSerializer( # many=True, read_only=False, source='usergroup') def to_representation(self, instance): initial_rep = super(ManagerMixIn, self).to_representation(instance) fields_to_pop = [ 'managers', 'annotators' 'viewers', ] if self.context.get('request'): initial_rep['user_is_manager'] = self.context['request'].user.has_perm( self.management_perm, obj=instance) if not initial_rep['user_is_manager']: [initial_rep.pop(field, '') for field in fields_to_pop] else: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep def update(self, instance, validated_data): instance = super(ManagerMixIn, self).update( instance, validated_data) instance.save() return instance OwnerMixIn Bases: ModelSerializer A mixin to add owner information to a serializer. It includes a read-only field for the owner and a boolean field to indicate if the user is the owner. The to_representation method customizes the output to include the owner and user_is_owner fields. The owner field is removed from the final representation. Source code in utils\\serializers.py class OwnerMixIn(serializers.ModelSerializer): \"\"\" A mixin to add owner information to a serializer. It includes a read-only field for the owner and a boolean field to indicate if the user is the owner. The `to_representation` method customizes the output to include the owner and user_is_owner fields. The owner field is removed from the final representation. \"\"\" owner = serializers.StringRelatedField(read_only=True) def to_representation(self, instance): initial_rep = super(OwnerMixIn, self).to_representation(instance) fields_to_pop = [ 'owner', ] if self.context.get('request'): initial_rep[\"user_is_owner\"] = self.context['request'].user.is_superuser or ( instance.owner == self.context['request'].user) [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep SlugRelatedGetOrCreateField Bases: SlugRelatedField A SlugRelatedField that retrieves or creates an object based on the slug field. Source code in utils\\serializers.py class SlugRelatedGetOrCreateField(serializers.SlugRelatedField): \"\"\" A SlugRelatedField that retrieves or creates an object based on the slug field. \"\"\" def to_internal_value(self, data): queryset = self.get_queryset() try: return queryset.get_or_create(**{self.slug_field: data})[0] except (TypeError, ValueError): self.fail(\"invalid\")","title":"serializers"},{"location":"reference/utils/serializers/#utils.serializers.CheckFormMixIn","text":"A mixin to check if the data was submitted through a form. It retrieves the form submission from the context and makes it available as self.form_submission in the serializer. Source code in utils\\serializers.py class CheckFormMixIn(): \"\"\" A mixin to check if the data was submitted through a form. It retrieves the form submission from the context and makes it available as `self.form_submission` in the serializer. \"\"\" def __init__(self, *args, **kwargs): super(CheckFormMixIn, self).__init__(*args, **kwargs) self.form_submission = self.context.get(\"form\")","title":"CheckFormMixIn"},{"location":"reference/utils/serializers/#utils.serializers.CreatedModifiedMixIn","text":"Bases: ModelSerializer A mixin to add created_on and modified_on fields to a serializer. Source code in utils\\serializers.py class CreatedModifiedMixIn(serializers.ModelSerializer): \"\"\" A mixin to add created_on and modified_on fields to a serializer. \"\"\" created_on = serializers.DateTimeField( default_timezone=djtimezone.utc, read_only=True, required=False) modified_on = serializers.DateTimeField( default_timezone=djtimezone.utc, read_only=True, required=False)","title":"CreatedModifiedMixIn"},{"location":"reference/utils/serializers/#utils.serializers.InstanceGetMixIn","text":"A mixin to retrieve an attribute from the instance or data. If the attribute is not found in the data, it checks if the instance has the attribute. If the attribute is found in the instance, it returns that value; otherwise, it returns None. Source code in utils\\serializers.py class InstanceGetMixIn(): \"\"\" A mixin to retrieve an attribute from the instance or data. If the attribute is not found in the data, it checks if the instance has the attribute. If the attribute is found in the instance, it returns that value; otherwise, it returns None. \"\"\" def instance_get(self, attr_name: str, data: dict) -> any: if attr_name in data: return data[attr_name] if self.instance and hasattr(self.instance, attr_name): return getattr(self.instance, attr_name) return None","title":"InstanceGetMixIn"},{"location":"reference/utils/serializers/#utils.serializers.ManagerMixIn","text":"Bases: ModelSerializer A mixin to add management-related fields to a serializer. It includes fields for managers, annotators, and viewers, allowing them to be set by slug or primary key. The to_representation method customizes the output to include user permissions and conditionally removes management-related fields based on the user's permissions. The update method ensures that the instance is saved after updating the management-related fields. Source code in utils\\serializers.py class ManagerMixIn(serializers.ModelSerializer): \"\"\" A mixin to add management-related fields to a serializer. It includes fields for managers, annotators, and viewers, allowing them to be set by slug or primary key. The `to_representation` method customizes the output to include user permissions and conditionally removes management-related fields based on the user's permissions. The `update` method ensures that the instance is saved after updating the management-related fields. \"\"\" managers = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) managers_ID = serializers.PrimaryKeyRelatedField(source=\"managers\", many=True, queryset=User.objects.all(), required=False) annotators = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) annotators_ID = serializers.PrimaryKeyRelatedField(source=\"annotators\", many=True, queryset=User.objects.all(), required=False) viewers = serializers.SlugRelatedField(many=True, slug_field=\"username\", queryset=User.objects.all(), allow_null=True, required=False, read_only=False) viewers_ID = serializers.PrimaryKeyRelatedField(source=\"viewers\", many=True, queryset=User.objects.all(), required=False) # viewers = UserGroupMemberSerializer( # many=True, read_only=False, source='usergroup') # annotators = UserGroupMemberSerializer( # many=True, read_only=False, source='usergroup') def to_representation(self, instance): initial_rep = super(ManagerMixIn, self).to_representation(instance) fields_to_pop = [ 'managers', 'annotators' 'viewers', ] if self.context.get('request'): initial_rep['user_is_manager'] = self.context['request'].user.has_perm( self.management_perm, obj=instance) if not initial_rep['user_is_manager']: [initial_rep.pop(field, '') for field in fields_to_pop] else: [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep def update(self, instance, validated_data): instance = super(ManagerMixIn, self).update( instance, validated_data) instance.save() return instance","title":"ManagerMixIn"},{"location":"reference/utils/serializers/#utils.serializers.OwnerMixIn","text":"Bases: ModelSerializer A mixin to add owner information to a serializer. It includes a read-only field for the owner and a boolean field to indicate if the user is the owner. The to_representation method customizes the output to include the owner and user_is_owner fields. The owner field is removed from the final representation. Source code in utils\\serializers.py class OwnerMixIn(serializers.ModelSerializer): \"\"\" A mixin to add owner information to a serializer. It includes a read-only field for the owner and a boolean field to indicate if the user is the owner. The `to_representation` method customizes the output to include the owner and user_is_owner fields. The owner field is removed from the final representation. \"\"\" owner = serializers.StringRelatedField(read_only=True) def to_representation(self, instance): initial_rep = super(OwnerMixIn, self).to_representation(instance) fields_to_pop = [ 'owner', ] if self.context.get('request'): initial_rep[\"user_is_owner\"] = self.context['request'].user.is_superuser or ( instance.owner == self.context['request'].user) [initial_rep.pop(field, '') for field in fields_to_pop] return initial_rep","title":"OwnerMixIn"},{"location":"reference/utils/serializers/#utils.serializers.SlugRelatedGetOrCreateField","text":"Bases: SlugRelatedField A SlugRelatedField that retrieves or creates an object based on the slug field. Source code in utils\\serializers.py class SlugRelatedGetOrCreateField(serializers.SlugRelatedField): \"\"\" A SlugRelatedField that retrieves or creates an object based on the slug field. \"\"\" def to_internal_value(self, data): queryset = self.get_queryset() try: return queryset.get_or_create(**{self.slug_field: data})[0] except (TypeError, ValueError): self.fail(\"invalid\")","title":"SlugRelatedGetOrCreateField"},{"location":"reference/utils/ssh_client/","text":"SSH_client Source code in utils\\ssh_client.py class SSH_client: def __init__( self, username: str, password: str, address: str, port: int, ) -> None: \"\"\" Initialize the SSH_client instance. Args: username (str): SSH username. password (str): SSH password. address (str): SSH server address. port (int): SSH port number. \"\"\" self.username = username self.password = password self.address = address self.port = port def check_connection(self) -> None: \"\"\" Checks if the FTP connection is active, and attempts to reconnect if not. \"\"\" while not self.ftp_t.is_active(): logger.info(\"Try to reestablish connection\") self.connect_to_ftp() def connect_to_ftp(self) -> bool: \"\"\" Establishes an SFTP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ftp_t = paramiko.Transport((self.address, self.port)) self.ftp_t.connect(username=self.username, password=self.password) self.ftp_sftp = paramiko.SFTPClient.from_transport(self.ftp_t) sftp_channel = self.ftp_sftp.get_channel() sftp_channel.settimeout(60 * 10) return True except Exception as e: logger.info(repr(e)) return False def close_connection_to_ftp(self) -> None: \"\"\" Closes the SFTP connection. \"\"\" try: self.ftp_t.close() logger.info(\"FTP connection closed\") except Exception as e: logger.info(repr(e)) def connect_to_ssh(self, port: int = None) -> bool: \"\"\" Establishes an SSH connection. Args: port (int, optional): SSH port. Defaults to self.port. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ssh_c.exec_command('ls') logger.info(\"SSH already connected\") return True except AttributeError: pass if port is None: port = self.port try: self.ssh_c = paramiko.SSHClient() self.ssh_c.set_missing_host_key_policy( paramiko.client.AutoAddPolicy) self.ssh_c.connect(self.address, port, username=self.username, password=self.password) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SSH connection\") return False def close_connection(self) -> None: \"\"\" Closes the SSH connection. \"\"\" try: self.ssh_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SSH connection\") def connect_to_scp(self) -> bool: \"\"\" Establishes an SCP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" self.connect_to_ssh() try: self.scp_c = SCPClient( self.ssh_c.get_transport(), progress=lambda file_name, size, sent: self.scp_progress_function( file_name, size, sent) ) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SCP connection\") return False def close_scp_connection(self) -> None: \"\"\" Closes the SCP connection. \"\"\" try: self.scp_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SCP connection\") def scp_progress_function(self, file_name: str, size: int, sent: int) -> None: \"\"\" Callback to report progress during SCP file transfers. Args: file_name (str): The name of the file being transferred. size (int): The total file size in bytes. sent (int): The number of bytes sent so far. \"\"\" sent_mb = convert_unit(sent, 'MB') size_mb = convert_unit(size, 'MB') if sent_mb % 50 != 0: return logger.info( f\"{file_name} progress: {sent_mb}/{size_mb} {float(sent)/float(size)*100}% \\r\" ) def send_ssh_command( self, command: str, sudo: bool = False, max_tries: int = 100, return_strings: bool = True, debug: bool = False, ) -> tuple[int, list[str], str] | tuple[int, any, any]: \"\"\" Executes a command over SSH, optionally with sudo, and returns result. Args: command (str): The command to execute. sudo (bool, optional): Whether to run the command with sudo. Defaults to False. max_tries (int, optional): Maximum number of retry attempts. Defaults to 100. return_strings (bool, optional): If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug (bool, optional): Whether to log debug info. Defaults to False. Returns: tuple: (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) \"\"\" success = False currtries = 0 while (not success) and (currtries < max_tries): try: if sudo: command = \"sudo -S -p '' \" + command stdin, stdout, stderr = self.ssh_c.exec_command(command) if sudo: stdin.write(self.password + \"\\n\") stdin.flush() success = True except Exception as e: logger.info(repr(e)) currtries += 1 self.connect_to_ssh() if debug: logger.info(f\"{command} SUDO {sudo} SUCCESS {success}\") if not success: raise paramiko.ssh_exception.SSHException stdout.channel.set_combine_stderr(True) if return_strings: stdout_lines = stdout.readlines() stdout_lines = [x.strip(\"\\n\") for x in stdout_lines] stderr_str = stderr.read().decode() exit_status = stdout.channel.recv_exit_status() return exit_status, stdout_lines, stderr_str else: return -1, stdout, stderr def mkdir_p(self, remote_path: str, is_dir: bool = True) -> None: \"\"\" Emulates 'mkdir -p' on the remote server via SFTP. Args: remote_path (str): The remote directory path to create. is_dir (bool, optional): Whether the path is a directory. Defaults to True. \"\"\" dirs_: list[str] = [] if is_dir: dir_ = remote_path else: dir_, basename = split(remote_path) while len(dir_) > 1: dirs_.append(dir_) dir_, _ = split(dir_) if len(dir_) == 1 and not dir_.startswith(\"/\"): dirs_.append(dir_) # For a remote path like y/x.txt while len(dirs_): dir_ = dirs_.pop() logger.info(dir_) try: self.ftp_sftp.stat(dir_) except FileNotFoundError: logger.info(f\"making {dir_}\") self.ftp_sftp.mkdir(dir_) __init__(username, password, address, port) Initialize the SSH_client instance. Parameters: username ( str ) \u2013 SSH username. password ( str ) \u2013 SSH password. address ( str ) \u2013 SSH server address. port ( int ) \u2013 SSH port number. Source code in utils\\ssh_client.py def __init__( self, username: str, password: str, address: str, port: int, ) -> None: \"\"\" Initialize the SSH_client instance. Args: username (str): SSH username. password (str): SSH password. address (str): SSH server address. port (int): SSH port number. \"\"\" self.username = username self.password = password self.address = address self.port = port check_connection() Checks if the FTP connection is active, and attempts to reconnect if not. Source code in utils\\ssh_client.py def check_connection(self) -> None: \"\"\" Checks if the FTP connection is active, and attempts to reconnect if not. \"\"\" while not self.ftp_t.is_active(): logger.info(\"Try to reestablish connection\") self.connect_to_ftp() close_connection() Closes the SSH connection. Source code in utils\\ssh_client.py def close_connection(self) -> None: \"\"\" Closes the SSH connection. \"\"\" try: self.ssh_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SSH connection\") close_connection_to_ftp() Closes the SFTP connection. Source code in utils\\ssh_client.py def close_connection_to_ftp(self) -> None: \"\"\" Closes the SFTP connection. \"\"\" try: self.ftp_t.close() logger.info(\"FTP connection closed\") except Exception as e: logger.info(repr(e)) close_scp_connection() Closes the SCP connection. Source code in utils\\ssh_client.py def close_scp_connection(self) -> None: \"\"\" Closes the SCP connection. \"\"\" try: self.scp_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SCP connection\") connect_to_ftp() Establishes an SFTP connection. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_ftp(self) -> bool: \"\"\" Establishes an SFTP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ftp_t = paramiko.Transport((self.address, self.port)) self.ftp_t.connect(username=self.username, password=self.password) self.ftp_sftp = paramiko.SFTPClient.from_transport(self.ftp_t) sftp_channel = self.ftp_sftp.get_channel() sftp_channel.settimeout(60 * 10) return True except Exception as e: logger.info(repr(e)) return False connect_to_scp() Establishes an SCP connection. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_scp(self) -> bool: \"\"\" Establishes an SCP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" self.connect_to_ssh() try: self.scp_c = SCPClient( self.ssh_c.get_transport(), progress=lambda file_name, size, sent: self.scp_progress_function( file_name, size, sent) ) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SCP connection\") return False connect_to_ssh(port=None) Establishes an SSH connection. Parameters: port ( int , default: None ) \u2013 SSH port. Defaults to self.port. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_ssh(self, port: int = None) -> bool: \"\"\" Establishes an SSH connection. Args: port (int, optional): SSH port. Defaults to self.port. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ssh_c.exec_command('ls') logger.info(\"SSH already connected\") return True except AttributeError: pass if port is None: port = self.port try: self.ssh_c = paramiko.SSHClient() self.ssh_c.set_missing_host_key_policy( paramiko.client.AutoAddPolicy) self.ssh_c.connect(self.address, port, username=self.username, password=self.password) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SSH connection\") return False mkdir_p(remote_path, is_dir=True) Emulates 'mkdir -p' on the remote server via SFTP. Parameters: remote_path ( str ) \u2013 The remote directory path to create. is_dir ( bool , default: True ) \u2013 Whether the path is a directory. Defaults to True. Source code in utils\\ssh_client.py def mkdir_p(self, remote_path: str, is_dir: bool = True) -> None: \"\"\" Emulates 'mkdir -p' on the remote server via SFTP. Args: remote_path (str): The remote directory path to create. is_dir (bool, optional): Whether the path is a directory. Defaults to True. \"\"\" dirs_: list[str] = [] if is_dir: dir_ = remote_path else: dir_, basename = split(remote_path) while len(dir_) > 1: dirs_.append(dir_) dir_, _ = split(dir_) if len(dir_) == 1 and not dir_.startswith(\"/\"): dirs_.append(dir_) # For a remote path like y/x.txt while len(dirs_): dir_ = dirs_.pop() logger.info(dir_) try: self.ftp_sftp.stat(dir_) except FileNotFoundError: logger.info(f\"making {dir_}\") self.ftp_sftp.mkdir(dir_) scp_progress_function(file_name, size, sent) Callback to report progress during SCP file transfers. Parameters: file_name ( str ) \u2013 The name of the file being transferred. size ( int ) \u2013 The total file size in bytes. sent ( int ) \u2013 The number of bytes sent so far. Source code in utils\\ssh_client.py def scp_progress_function(self, file_name: str, size: int, sent: int) -> None: \"\"\" Callback to report progress during SCP file transfers. Args: file_name (str): The name of the file being transferred. size (int): The total file size in bytes. sent (int): The number of bytes sent so far. \"\"\" sent_mb = convert_unit(sent, 'MB') size_mb = convert_unit(size, 'MB') if sent_mb % 50 != 0: return logger.info( f\"{file_name} progress: {sent_mb}/{size_mb} {float(sent)/float(size)*100}% \\r\" ) send_ssh_command(command, sudo=False, max_tries=100, return_strings=True, debug=False) Executes a command over SSH, optionally with sudo, and returns result. Parameters: command ( str ) \u2013 The command to execute. sudo ( bool , default: False ) \u2013 Whether to run the command with sudo. Defaults to False. max_tries ( int , default: 100 ) \u2013 Maximum number of retry attempts. Defaults to 100. return_strings ( bool , default: True ) \u2013 If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug ( bool , default: False ) \u2013 Whether to log debug info. Defaults to False. Returns: tuple ( tuple [ int , list [ str ], str ] | tuple [ int , any , any ] ) \u2013 (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) Source code in utils\\ssh_client.py def send_ssh_command( self, command: str, sudo: bool = False, max_tries: int = 100, return_strings: bool = True, debug: bool = False, ) -> tuple[int, list[str], str] | tuple[int, any, any]: \"\"\" Executes a command over SSH, optionally with sudo, and returns result. Args: command (str): The command to execute. sudo (bool, optional): Whether to run the command with sudo. Defaults to False. max_tries (int, optional): Maximum number of retry attempts. Defaults to 100. return_strings (bool, optional): If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug (bool, optional): Whether to log debug info. Defaults to False. Returns: tuple: (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) \"\"\" success = False currtries = 0 while (not success) and (currtries < max_tries): try: if sudo: command = \"sudo -S -p '' \" + command stdin, stdout, stderr = self.ssh_c.exec_command(command) if sudo: stdin.write(self.password + \"\\n\") stdin.flush() success = True except Exception as e: logger.info(repr(e)) currtries += 1 self.connect_to_ssh() if debug: logger.info(f\"{command} SUDO {sudo} SUCCESS {success}\") if not success: raise paramiko.ssh_exception.SSHException stdout.channel.set_combine_stderr(True) if return_strings: stdout_lines = stdout.readlines() stdout_lines = [x.strip(\"\\n\") for x in stdout_lines] stderr_str = stderr.read().decode() exit_status = stdout.channel.recv_exit_status() return exit_status, stdout_lines, stderr_str else: return -1, stdout, stderr","title":"ssh_client"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client","text":"Source code in utils\\ssh_client.py class SSH_client: def __init__( self, username: str, password: str, address: str, port: int, ) -> None: \"\"\" Initialize the SSH_client instance. Args: username (str): SSH username. password (str): SSH password. address (str): SSH server address. port (int): SSH port number. \"\"\" self.username = username self.password = password self.address = address self.port = port def check_connection(self) -> None: \"\"\" Checks if the FTP connection is active, and attempts to reconnect if not. \"\"\" while not self.ftp_t.is_active(): logger.info(\"Try to reestablish connection\") self.connect_to_ftp() def connect_to_ftp(self) -> bool: \"\"\" Establishes an SFTP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ftp_t = paramiko.Transport((self.address, self.port)) self.ftp_t.connect(username=self.username, password=self.password) self.ftp_sftp = paramiko.SFTPClient.from_transport(self.ftp_t) sftp_channel = self.ftp_sftp.get_channel() sftp_channel.settimeout(60 * 10) return True except Exception as e: logger.info(repr(e)) return False def close_connection_to_ftp(self) -> None: \"\"\" Closes the SFTP connection. \"\"\" try: self.ftp_t.close() logger.info(\"FTP connection closed\") except Exception as e: logger.info(repr(e)) def connect_to_ssh(self, port: int = None) -> bool: \"\"\" Establishes an SSH connection. Args: port (int, optional): SSH port. Defaults to self.port. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ssh_c.exec_command('ls') logger.info(\"SSH already connected\") return True except AttributeError: pass if port is None: port = self.port try: self.ssh_c = paramiko.SSHClient() self.ssh_c.set_missing_host_key_policy( paramiko.client.AutoAddPolicy) self.ssh_c.connect(self.address, port, username=self.username, password=self.password) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SSH connection\") return False def close_connection(self) -> None: \"\"\" Closes the SSH connection. \"\"\" try: self.ssh_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SSH connection\") def connect_to_scp(self) -> bool: \"\"\" Establishes an SCP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" self.connect_to_ssh() try: self.scp_c = SCPClient( self.ssh_c.get_transport(), progress=lambda file_name, size, sent: self.scp_progress_function( file_name, size, sent) ) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SCP connection\") return False def close_scp_connection(self) -> None: \"\"\" Closes the SCP connection. \"\"\" try: self.scp_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SCP connection\") def scp_progress_function(self, file_name: str, size: int, sent: int) -> None: \"\"\" Callback to report progress during SCP file transfers. Args: file_name (str): The name of the file being transferred. size (int): The total file size in bytes. sent (int): The number of bytes sent so far. \"\"\" sent_mb = convert_unit(sent, 'MB') size_mb = convert_unit(size, 'MB') if sent_mb % 50 != 0: return logger.info( f\"{file_name} progress: {sent_mb}/{size_mb} {float(sent)/float(size)*100}% \\r\" ) def send_ssh_command( self, command: str, sudo: bool = False, max_tries: int = 100, return_strings: bool = True, debug: bool = False, ) -> tuple[int, list[str], str] | tuple[int, any, any]: \"\"\" Executes a command over SSH, optionally with sudo, and returns result. Args: command (str): The command to execute. sudo (bool, optional): Whether to run the command with sudo. Defaults to False. max_tries (int, optional): Maximum number of retry attempts. Defaults to 100. return_strings (bool, optional): If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug (bool, optional): Whether to log debug info. Defaults to False. Returns: tuple: (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) \"\"\" success = False currtries = 0 while (not success) and (currtries < max_tries): try: if sudo: command = \"sudo -S -p '' \" + command stdin, stdout, stderr = self.ssh_c.exec_command(command) if sudo: stdin.write(self.password + \"\\n\") stdin.flush() success = True except Exception as e: logger.info(repr(e)) currtries += 1 self.connect_to_ssh() if debug: logger.info(f\"{command} SUDO {sudo} SUCCESS {success}\") if not success: raise paramiko.ssh_exception.SSHException stdout.channel.set_combine_stderr(True) if return_strings: stdout_lines = stdout.readlines() stdout_lines = [x.strip(\"\\n\") for x in stdout_lines] stderr_str = stderr.read().decode() exit_status = stdout.channel.recv_exit_status() return exit_status, stdout_lines, stderr_str else: return -1, stdout, stderr def mkdir_p(self, remote_path: str, is_dir: bool = True) -> None: \"\"\" Emulates 'mkdir -p' on the remote server via SFTP. Args: remote_path (str): The remote directory path to create. is_dir (bool, optional): Whether the path is a directory. Defaults to True. \"\"\" dirs_: list[str] = [] if is_dir: dir_ = remote_path else: dir_, basename = split(remote_path) while len(dir_) > 1: dirs_.append(dir_) dir_, _ = split(dir_) if len(dir_) == 1 and not dir_.startswith(\"/\"): dirs_.append(dir_) # For a remote path like y/x.txt while len(dirs_): dir_ = dirs_.pop() logger.info(dir_) try: self.ftp_sftp.stat(dir_) except FileNotFoundError: logger.info(f\"making {dir_}\") self.ftp_sftp.mkdir(dir_)","title":"SSH_client"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.__init__","text":"Initialize the SSH_client instance. Parameters: username ( str ) \u2013 SSH username. password ( str ) \u2013 SSH password. address ( str ) \u2013 SSH server address. port ( int ) \u2013 SSH port number. Source code in utils\\ssh_client.py def __init__( self, username: str, password: str, address: str, port: int, ) -> None: \"\"\" Initialize the SSH_client instance. Args: username (str): SSH username. password (str): SSH password. address (str): SSH server address. port (int): SSH port number. \"\"\" self.username = username self.password = password self.address = address self.port = port","title":"__init__"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.check_connection","text":"Checks if the FTP connection is active, and attempts to reconnect if not. Source code in utils\\ssh_client.py def check_connection(self) -> None: \"\"\" Checks if the FTP connection is active, and attempts to reconnect if not. \"\"\" while not self.ftp_t.is_active(): logger.info(\"Try to reestablish connection\") self.connect_to_ftp()","title":"check_connection"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.close_connection","text":"Closes the SSH connection. Source code in utils\\ssh_client.py def close_connection(self) -> None: \"\"\" Closes the SSH connection. \"\"\" try: self.ssh_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SSH connection\")","title":"close_connection"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.close_connection_to_ftp","text":"Closes the SFTP connection. Source code in utils\\ssh_client.py def close_connection_to_ftp(self) -> None: \"\"\" Closes the SFTP connection. \"\"\" try: self.ftp_t.close() logger.info(\"FTP connection closed\") except Exception as e: logger.info(repr(e))","title":"close_connection_to_ftp"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.close_scp_connection","text":"Closes the SCP connection. Source code in utils\\ssh_client.py def close_scp_connection(self) -> None: \"\"\" Closes the SCP connection. \"\"\" try: self.scp_c.close() except Exception as e: logger.info(repr(e)) logger.info(\"Unable to close SCP connection\")","title":"close_scp_connection"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.connect_to_ftp","text":"Establishes an SFTP connection. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_ftp(self) -> bool: \"\"\" Establishes an SFTP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ftp_t = paramiko.Transport((self.address, self.port)) self.ftp_t.connect(username=self.username, password=self.password) self.ftp_sftp = paramiko.SFTPClient.from_transport(self.ftp_t) sftp_channel = self.ftp_sftp.get_channel() sftp_channel.settimeout(60 * 10) return True except Exception as e: logger.info(repr(e)) return False","title":"connect_to_ftp"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.connect_to_scp","text":"Establishes an SCP connection. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_scp(self) -> bool: \"\"\" Establishes an SCP connection. Returns: bool: True if connection succeeded, False otherwise. \"\"\" self.connect_to_ssh() try: self.scp_c = SCPClient( self.ssh_c.get_transport(), progress=lambda file_name, size, sent: self.scp_progress_function( file_name, size, sent) ) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SCP connection\") return False","title":"connect_to_scp"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.connect_to_ssh","text":"Establishes an SSH connection. Parameters: port ( int , default: None ) \u2013 SSH port. Defaults to self.port. Returns: bool ( bool ) \u2013 True if connection succeeded, False otherwise. Source code in utils\\ssh_client.py def connect_to_ssh(self, port: int = None) -> bool: \"\"\" Establishes an SSH connection. Args: port (int, optional): SSH port. Defaults to self.port. Returns: bool: True if connection succeeded, False otherwise. \"\"\" try: self.ssh_c.exec_command('ls') logger.info(\"SSH already connected\") return True except AttributeError: pass if port is None: port = self.port try: self.ssh_c = paramiko.SSHClient() self.ssh_c.set_missing_host_key_policy( paramiko.client.AutoAddPolicy) self.ssh_c.connect(self.address, port, username=self.username, password=self.password) return True except Exception as e: logger.info(repr(e)) logger.info(\"Unable to start SSH connection\") return False","title":"connect_to_ssh"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.mkdir_p","text":"Emulates 'mkdir -p' on the remote server via SFTP. Parameters: remote_path ( str ) \u2013 The remote directory path to create. is_dir ( bool , default: True ) \u2013 Whether the path is a directory. Defaults to True. Source code in utils\\ssh_client.py def mkdir_p(self, remote_path: str, is_dir: bool = True) -> None: \"\"\" Emulates 'mkdir -p' on the remote server via SFTP. Args: remote_path (str): The remote directory path to create. is_dir (bool, optional): Whether the path is a directory. Defaults to True. \"\"\" dirs_: list[str] = [] if is_dir: dir_ = remote_path else: dir_, basename = split(remote_path) while len(dir_) > 1: dirs_.append(dir_) dir_, _ = split(dir_) if len(dir_) == 1 and not dir_.startswith(\"/\"): dirs_.append(dir_) # For a remote path like y/x.txt while len(dirs_): dir_ = dirs_.pop() logger.info(dir_) try: self.ftp_sftp.stat(dir_) except FileNotFoundError: logger.info(f\"making {dir_}\") self.ftp_sftp.mkdir(dir_)","title":"mkdir_p"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.scp_progress_function","text":"Callback to report progress during SCP file transfers. Parameters: file_name ( str ) \u2013 The name of the file being transferred. size ( int ) \u2013 The total file size in bytes. sent ( int ) \u2013 The number of bytes sent so far. Source code in utils\\ssh_client.py def scp_progress_function(self, file_name: str, size: int, sent: int) -> None: \"\"\" Callback to report progress during SCP file transfers. Args: file_name (str): The name of the file being transferred. size (int): The total file size in bytes. sent (int): The number of bytes sent so far. \"\"\" sent_mb = convert_unit(sent, 'MB') size_mb = convert_unit(size, 'MB') if sent_mb % 50 != 0: return logger.info( f\"{file_name} progress: {sent_mb}/{size_mb} {float(sent)/float(size)*100}% \\r\" )","title":"scp_progress_function"},{"location":"reference/utils/ssh_client/#utils.ssh_client.SSH_client.send_ssh_command","text":"Executes a command over SSH, optionally with sudo, and returns result. Parameters: command ( str ) \u2013 The command to execute. sudo ( bool , default: False ) \u2013 Whether to run the command with sudo. Defaults to False. max_tries ( int , default: 100 ) \u2013 Maximum number of retry attempts. Defaults to 100. return_strings ( bool , default: True ) \u2013 If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug ( bool , default: False ) \u2013 Whether to log debug info. Defaults to False. Returns: tuple ( tuple [ int , list [ str ], str ] | tuple [ int , any , any ] ) \u2013 (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) Source code in utils\\ssh_client.py def send_ssh_command( self, command: str, sudo: bool = False, max_tries: int = 100, return_strings: bool = True, debug: bool = False, ) -> tuple[int, list[str], str] | tuple[int, any, any]: \"\"\" Executes a command over SSH, optionally with sudo, and returns result. Args: command (str): The command to execute. sudo (bool, optional): Whether to run the command with sudo. Defaults to False. max_tries (int, optional): Maximum number of retry attempts. Defaults to 100. return_strings (bool, optional): If True, returns result as strings. Otherwise, returns raw objects. Defaults to True. debug (bool, optional): Whether to log debug info. Defaults to False. Returns: tuple: (exit_status, stdout_lines, stderr_str) if return_strings is True, otherwise (-1, stdout, stderr) \"\"\" success = False currtries = 0 while (not success) and (currtries < max_tries): try: if sudo: command = \"sudo -S -p '' \" + command stdin, stdout, stderr = self.ssh_c.exec_command(command) if sudo: stdin.write(self.password + \"\\n\") stdin.flush() success = True except Exception as e: logger.info(repr(e)) currtries += 1 self.connect_to_ssh() if debug: logger.info(f\"{command} SUDO {sudo} SUCCESS {success}\") if not success: raise paramiko.ssh_exception.SSHException stdout.channel.set_combine_stderr(True) if return_strings: stdout_lines = stdout.readlines() stdout_lines = [x.strip(\"\\n\") for x in stdout_lines] stderr_str = stderr.read().decode() exit_status = stdout.channel.recv_exit_status() return exit_status, stdout_lines, stderr_str else: return -1, stdout, stderr","title":"send_ssh_command"},{"location":"reference/utils/task_functions/","text":"TooManyTasks Bases: Exception Exception raised when too many instances of a specific task are running. Source code in utils\\task_functions.py class TooManyTasks(Exception): \"\"\" Exception raised when too many instances of a specific task are running. \"\"\" def __init__(self, task: Any) -> None: \"\"\" Initialize the TooManyTasks exception. Args: task: The Celery task instance that triggered the exception. \"\"\" self.task_id: str = task.request.id self.task_name: str = task.name self.current_retries: int = task.request.retries self.max_retries: int = task.max_retries super(TooManyTasks, self).__init__() def __str__(self) -> str: \"\"\" Return a descriptive error message. Returns: str: Error message with task details. \"\"\" return ( f\"{self.task_id} not run. Too many {self.task_name} tasks already running. \" f\"Try {self.current_retries}/{self.max_retries}\" ) __init__(task) Initialize the TooManyTasks exception. Parameters: task ( Any ) \u2013 The Celery task instance that triggered the exception. Source code in utils\\task_functions.py def __init__(self, task: Any) -> None: \"\"\" Initialize the TooManyTasks exception. Args: task: The Celery task instance that triggered the exception. \"\"\" self.task_id: str = task.request.id self.task_name: str = task.name self.current_retries: int = task.request.retries self.max_retries: int = task.max_retries super(TooManyTasks, self).__init__() __str__() Return a descriptive error message. Returns: str ( str ) \u2013 Error message with task details. Source code in utils\\task_functions.py def __str__(self) -> str: \"\"\" Return a descriptive error message. Returns: str: Error message with task details. \"\"\" return ( f\"{self.task_id} not run. Too many {self.task_name} tasks already running. \" f\"Try {self.current_retries}/{self.max_retries}\" ) check_simultaneous_tasks(task, max_tasks) Check the number of currently running instances of a given task and raise an exception if the number exceeds the allowed maximum. Parameters: task ( Any ) \u2013 The Celery task instance to check for. max_tasks ( int ) \u2013 The maximum allowed number of simultaneous task instances. Raises: TooManyTasks \u2013 If the maximum number of simultaneous tasks is exceeded. Source code in utils\\task_functions.py def check_simultaneous_tasks(task: Any, max_tasks: int) -> None: \"\"\" Check the number of currently running instances of a given task and raise an exception if the number exceeds the allowed maximum. Args: task: The Celery task instance to check for. max_tasks: The maximum allowed number of simultaneous task instances. Raises: TooManyTasks: If the maximum number of simultaneous tasks is exceeded. \"\"\" active_tasks = app.control.inspect().active() all_tasks = [] for worker, running_tasks in active_tasks.items(): for running_task in running_tasks: if (task.name in running_task[\"name\"] and running_task[\"id\"] != task.request.id): all_tasks.append(task) logger.info(f\"{len(all_tasks)} running\") if len(all_tasks) + 1 > max_tasks: raise TooManyTasks(task)","title":"task_functions"},{"location":"reference/utils/task_functions/#utils.task_functions.TooManyTasks","text":"Bases: Exception Exception raised when too many instances of a specific task are running. Source code in utils\\task_functions.py class TooManyTasks(Exception): \"\"\" Exception raised when too many instances of a specific task are running. \"\"\" def __init__(self, task: Any) -> None: \"\"\" Initialize the TooManyTasks exception. Args: task: The Celery task instance that triggered the exception. \"\"\" self.task_id: str = task.request.id self.task_name: str = task.name self.current_retries: int = task.request.retries self.max_retries: int = task.max_retries super(TooManyTasks, self).__init__() def __str__(self) -> str: \"\"\" Return a descriptive error message. Returns: str: Error message with task details. \"\"\" return ( f\"{self.task_id} not run. Too many {self.task_name} tasks already running. \" f\"Try {self.current_retries}/{self.max_retries}\" )","title":"TooManyTasks"},{"location":"reference/utils/task_functions/#utils.task_functions.TooManyTasks.__init__","text":"Initialize the TooManyTasks exception. Parameters: task ( Any ) \u2013 The Celery task instance that triggered the exception. Source code in utils\\task_functions.py def __init__(self, task: Any) -> None: \"\"\" Initialize the TooManyTasks exception. Args: task: The Celery task instance that triggered the exception. \"\"\" self.task_id: str = task.request.id self.task_name: str = task.name self.current_retries: int = task.request.retries self.max_retries: int = task.max_retries super(TooManyTasks, self).__init__()","title":"__init__"},{"location":"reference/utils/task_functions/#utils.task_functions.TooManyTasks.__str__","text":"Return a descriptive error message. Returns: str ( str ) \u2013 Error message with task details. Source code in utils\\task_functions.py def __str__(self) -> str: \"\"\" Return a descriptive error message. Returns: str: Error message with task details. \"\"\" return ( f\"{self.task_id} not run. Too many {self.task_name} tasks already running. \" f\"Try {self.current_retries}/{self.max_retries}\" )","title":"__str__"},{"location":"reference/utils/task_functions/#utils.task_functions.check_simultaneous_tasks","text":"Check the number of currently running instances of a given task and raise an exception if the number exceeds the allowed maximum. Parameters: task ( Any ) \u2013 The Celery task instance to check for. max_tasks ( int ) \u2013 The maximum allowed number of simultaneous task instances. Raises: TooManyTasks \u2013 If the maximum number of simultaneous tasks is exceeded. Source code in utils\\task_functions.py def check_simultaneous_tasks(task: Any, max_tasks: int) -> None: \"\"\" Check the number of currently running instances of a given task and raise an exception if the number exceeds the allowed maximum. Args: task: The Celery task instance to check for. max_tasks: The maximum allowed number of simultaneous task instances. Raises: TooManyTasks: If the maximum number of simultaneous tasks is exceeded. \"\"\" active_tasks = app.control.inspect().active() all_tasks = [] for worker, running_tasks in active_tasks.items(): for running_task in running_tasks: if (task.name in running_task[\"name\"] and running_task[\"id\"] != task.request.id): all_tasks.append(task) logger.info(f\"{len(all_tasks)} running\") if len(all_tasks) + 1 > max_tasks: raise TooManyTasks(task)","title":"check_simultaneous_tasks"},{"location":"reference/utils/test_functions/","text":"api_check_delete(api_client, api_url) Function to test if object can be deleted through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL which the api_client will DELETE. Source code in utils\\test_functions.py def api_check_delete(api_client, api_url): \"\"\" Function to test if object can be deleted through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL which the api_client will DELETE. \"\"\" response_delete = api_client.delete( api_url, format=\"json\") print(f\"Response: {response_delete.data}\") assert response_delete.status_code == 204 response_read = api_client.get( api_url, format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 404 api_check_post(api_client, api_url, payload, check_key=None) Function to test if object can be created and read through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL to which the api_client will POST. payload ( dict ) \u2013 data to POST. check_key ( string , default: None ) \u2013 key to check in returned data. Source code in utils\\test_functions.py def api_check_post(api_client, api_url, payload, check_key=None): \"\"\" Function to test if object can be created and read through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL to which the api_client will POST. payload (dict): data to POST. check_key (string): key to check in returned data. \"\"\" response_create = api_client.post( api_url, data=payload, format=\"json\") print(f\"{response_create.data}\") assert response_create.status_code == 201 if check_key: assert response_create.data[check_key] == payload[check_key] response_id = response_create.data[\"id\"] response_read = api_client.get( f\"{api_url}{response_id}/\", format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 200 if check_key: assert response_read.data[check_key] == payload[check_key] if \"owner\" in response_read.data.keys(): assert response_read.data[\"owner\"] == api_client.handler._force_user.username api_check_update(api_client, api_url, new_value, check_key=None) Function to test if object can be updated and read through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL which the api_client will PATCH. new_value ( any ) \u2013 New value in the PATCH. check_key ( string , default: None ) \u2013 Key which will be PATCHed and checked in the returned data. Source code in utils\\test_functions.py def api_check_update(api_client, api_url, new_value, check_key=None): \"\"\" Function to test if object can be updated and read through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL which the api_client will PATCH. new_value (any): New value in the PATCH. check_key (string): Key which will be PATCHed and checked in the returned data. \"\"\" response_update = api_client.patch( api_url, data={check_key: new_value}, format=\"json\") print(f\"Response: {response_update.data}\") assert response_update.status_code == 200 if check_key: assert response_update.data[check_key] == new_value response_read = api_client.get( api_url, format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 200 if check_key: assert response_read.data[check_key] == new_value","title":"test_functions"},{"location":"reference/utils/test_functions/#utils.test_functions.api_check_delete","text":"Function to test if object can be deleted through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL which the api_client will DELETE. Source code in utils\\test_functions.py def api_check_delete(api_client, api_url): \"\"\" Function to test if object can be deleted through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL which the api_client will DELETE. \"\"\" response_delete = api_client.delete( api_url, format=\"json\") print(f\"Response: {response_delete.data}\") assert response_delete.status_code == 204 response_read = api_client.get( api_url, format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 404","title":"api_check_delete"},{"location":"reference/utils/test_functions/#utils.test_functions.api_check_post","text":"Function to test if object can be created and read through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL to which the api_client will POST. payload ( dict ) \u2013 data to POST. check_key ( string , default: None ) \u2013 key to check in returned data. Source code in utils\\test_functions.py def api_check_post(api_client, api_url, payload, check_key=None): \"\"\" Function to test if object can be created and read through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL to which the api_client will POST. payload (dict): data to POST. check_key (string): key to check in returned data. \"\"\" response_create = api_client.post( api_url, data=payload, format=\"json\") print(f\"{response_create.data}\") assert response_create.status_code == 201 if check_key: assert response_create.data[check_key] == payload[check_key] response_id = response_create.data[\"id\"] response_read = api_client.get( f\"{api_url}{response_id}/\", format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 200 if check_key: assert response_read.data[check_key] == payload[check_key] if \"owner\" in response_read.data.keys(): assert response_read.data[\"owner\"] == api_client.handler._force_user.username","title":"api_check_post"},{"location":"reference/utils/test_functions/#utils.test_functions.api_check_update","text":"Function to test if object can be updated and read through the API. Parameters: api_client ( APIclient ) \u2013 API client with a forced log in. api_url ( string ) \u2013 URL which the api_client will PATCH. new_value ( any ) \u2013 New value in the PATCH. check_key ( string , default: None ) \u2013 Key which will be PATCHed and checked in the returned data. Source code in utils\\test_functions.py def api_check_update(api_client, api_url, new_value, check_key=None): \"\"\" Function to test if object can be updated and read through the API. Args: api_client (rest_framework.tests.APIclient): API client with a forced log in. api_url (string): URL which the api_client will PATCH. new_value (any): New value in the PATCH. check_key (string): Key which will be PATCHed and checked in the returned data. \"\"\" response_update = api_client.patch( api_url, data={check_key: new_value}, format=\"json\") print(f\"Response: {response_update.data}\") assert response_update.status_code == 200 if check_key: assert response_update.data[check_key] == new_value response_read = api_client.get( api_url, format=\"json\") print(f\"Response: {response_read.data}\") assert response_read.status_code == 200 if check_key: assert response_read.data[check_key] == new_value","title":"api_check_update"},{"location":"reference/utils/validators/","text":"check_two_keys(primary_key, secondary_key, data, target_model, form_submission=False) Function to check if at least one of two keys is included in serialized data. If only secondary key is present, get the value of the primary key from the target_model. Parameters: primary_key ( str ) \u2013 First key to check. secondary_key ( str ) \u2013 Second key to check. data ( SerializedData ) \u2013 Serialized data to check for the presence of the keys target_model ( Model ) \u2013 Model class to pull value of primary key form_submission ( bool , default: False ) \u2013 True if this check is being carried out by a submitted form, Returns: \u2013 success (boolean) \u2013 error message (dict where the key is the associated field name) \u2013 modified serialized data (SerializedData) Source code in utils\\validators.py def check_two_keys(primary_key, secondary_key, data, target_model, form_submission=False): \"\"\" Function to check if at least one of two keys is included in serialized data. If only secondary key is present, get the value of the primary key from the target_model. Args: primary_key (str): First key to check. secondary_key (str): Second key to check. data (SerializedData): Serialized data to check for the presence of the keys target_model (django.db.models.Model): Model class to pull value of primary key form_submission (bool): True if this check is being carried out by a submitted form, modifies the error message returned. Returns: success (boolean) error message (dict where the key is the associated field name) modified serialized data (SerializedData) \"\"\" if data.get(primary_key) is None and data.get(secondary_key) is None: if form_submission: message = {primary_key: f\"{primary_key} is required.\"} else: message = {primary_key: f\"Either {primary_key} or {secondary_key} are required.\", secondary_key: f\"Either {primary_key} or {secondary_key} are required.\"} return False, message, data elif data.get(primary_key) is None: data[primary_key] = target_model.objects.get( pk=data.get(secondary_key)) return True, {}, data","title":"validators"},{"location":"reference/utils/validators/#utils.validators.check_two_keys","text":"Function to check if at least one of two keys is included in serialized data. If only secondary key is present, get the value of the primary key from the target_model. Parameters: primary_key ( str ) \u2013 First key to check. secondary_key ( str ) \u2013 Second key to check. data ( SerializedData ) \u2013 Serialized data to check for the presence of the keys target_model ( Model ) \u2013 Model class to pull value of primary key form_submission ( bool , default: False ) \u2013 True if this check is being carried out by a submitted form, Returns: \u2013 success (boolean) \u2013 error message (dict where the key is the associated field name) \u2013 modified serialized data (SerializedData) Source code in utils\\validators.py def check_two_keys(primary_key, secondary_key, data, target_model, form_submission=False): \"\"\" Function to check if at least one of two keys is included in serialized data. If only secondary key is present, get the value of the primary key from the target_model. Args: primary_key (str): First key to check. secondary_key (str): Second key to check. data (SerializedData): Serialized data to check for the presence of the keys target_model (django.db.models.Model): Model class to pull value of primary key form_submission (bool): True if this check is being carried out by a submitted form, modifies the error message returned. Returns: success (boolean) error message (dict where the key is the associated field name) modified serialized data (SerializedData) \"\"\" if data.get(primary_key) is None and data.get(secondary_key) is None: if form_submission: message = {primary_key: f\"{primary_key} is required.\"} else: message = {primary_key: f\"Either {primary_key} or {secondary_key} are required.\", secondary_key: f\"Either {primary_key} or {secondary_key} are required.\"} return False, message, data elif data.get(primary_key) is None: data[primary_key] = target_model.objects.get( pk=data.get(secondary_key)) return True, {}, data","title":"check_two_keys"},{"location":"reference/utils/views/","text":"AllTimezoneView(request) Return all timezones. Source code in utils\\views.py @extend_schema( exclude=True ) @api_view() @permission_classes([IsAuthenticated]) def AllTimezoneView(request): \"\"\" Return all timezones. \"\"\" combined_timezones = common_timezones + \\ [x for x in all_timezones if \"Etc\" in x and x not in common_timezones] return Response(combined_timezones)","title":"views"},{"location":"reference/utils/views/#utils.views.AllTimezoneView","text":"Return all timezones. Source code in utils\\views.py @extend_schema( exclude=True ) @api_view() @permission_classes([IsAuthenticated]) def AllTimezoneView(request): \"\"\" Return all timezones. \"\"\" combined_timezones = common_timezones + \\ [x for x in all_timezones if \"Etc\" in x and x not in common_timezones] return Response(combined_timezones)","title":"AllTimezoneView"},{"location":"reference/utils/viewsets/","text":"CheckAttachmentViewSetMixIn Bases: ModelViewSet Viewset mixin to call a check_attachment function on create and update. check_attachment should be overriden inside the inheriting viewset. IMPORTANT: Make sure this is first class inherited, otherwise the object can be created before a check is made. Source code in utils\\viewsets.py class CheckAttachmentViewSetMixIn(ModelViewSet): \"\"\" Viewset mixin to call a check_attachment function on create and update. check_attachment should be overriden inside the inheriting viewset. IMPORTANT: Make sure this is first class inherited, otherwise the object can be created before a check is made. \"\"\" def perform_create(self, serializer): logger.info(\"check attachment\") self.check_attachment(serializer) return super().perform_create(serializer) def perform_update(self, serializer): logger.info(\"check attachment\") self.check_attachment(serializer) return super().perform_update(serializer) def check_attachment(self, serializer): pass OptionalPaginationViewSetMixIn Bases: ModelViewSet By default, paginate_queryset returns None when no paginator is set. This extends it to also return None if no 'page' query param is set, so long as the queryset is not too large Source code in utils\\viewsets.py class OptionalPaginationViewSetMixIn(ModelViewSet): \"\"\"By default, paginate_queryset returns None when no paginator is set. This extends it to also return None if no 'page' query param is set, so long as the queryset is not too large\"\"\" def paginate_queryset(self, queryset): logger.info(\"Optional pagination\") if self.paginator \\ and self.request.query_params.get(self.paginator.page_size_query_param, None) is None \\ and self.request.query_params.get(self.paginator.page_query_param, None) is None: if queryset.approx_count() < settings.REST_FRAMEWORK['MAX_PAGE_SIZE']: logger.info(\"No pagination\") return None logger.info(\"Paginate\") logger.info(self.paginator) return super().paginate_queryset(queryset)","title":"viewsets"},{"location":"reference/utils/viewsets/#utils.viewsets.CheckAttachmentViewSetMixIn","text":"Bases: ModelViewSet Viewset mixin to call a check_attachment function on create and update. check_attachment should be overriden inside the inheriting viewset. IMPORTANT: Make sure this is first class inherited, otherwise the object can be created before a check is made. Source code in utils\\viewsets.py class CheckAttachmentViewSetMixIn(ModelViewSet): \"\"\" Viewset mixin to call a check_attachment function on create and update. check_attachment should be overriden inside the inheriting viewset. IMPORTANT: Make sure this is first class inherited, otherwise the object can be created before a check is made. \"\"\" def perform_create(self, serializer): logger.info(\"check attachment\") self.check_attachment(serializer) return super().perform_create(serializer) def perform_update(self, serializer): logger.info(\"check attachment\") self.check_attachment(serializer) return super().perform_update(serializer) def check_attachment(self, serializer): pass","title":"CheckAttachmentViewSetMixIn"},{"location":"reference/utils/viewsets/#utils.viewsets.OptionalPaginationViewSetMixIn","text":"Bases: ModelViewSet By default, paginate_queryset returns None when no paginator is set. This extends it to also return None if no 'page' query param is set, so long as the queryset is not too large Source code in utils\\viewsets.py class OptionalPaginationViewSetMixIn(ModelViewSet): \"\"\"By default, paginate_queryset returns None when no paginator is set. This extends it to also return None if no 'page' query param is set, so long as the queryset is not too large\"\"\" def paginate_queryset(self, queryset): logger.info(\"Optional pagination\") if self.paginator \\ and self.request.query_params.get(self.paginator.page_size_query_param, None) is None \\ and self.request.query_params.get(self.paginator.page_query_param, None) is None: if queryset.approx_count() < settings.REST_FRAMEWORK['MAX_PAGE_SIZE']: logger.info(\"No pagination\") return None logger.info(\"Paginate\") logger.info(self.paginator) return super().paginate_queryset(queryset)","title":"OptionalPaginationViewSetMixIn"},{"location":"structure/","text":"Project Structure Project : Central organizational unit representing a study or research effort. Linked to multiple Sites (locations), Devices (sensors), and Deployments. Has roles: owner, managers, viewers, and annotators. Can define ProjectJobs for automated tasks. Site Site : A physical location. Has a name and optional short name. Used to contextualize Deployments. Devices and Deployments DeviceModel : Blueprint for devices (e.g., camera model). Linked to a DataType (e.g., image, audio). Device : Physical unit based on a DeviceModel. Can be deployed at a Site. May include credentials for external storage. Deployment : A specific installation of a Device at a Site, possibly across Projects. Contains geolocation, time zone, and time range (start and end). Tracks activity and holds reference to the last captured image. Data and Metadata DataFile : Represents individual files collected during a Deployment. Includes recording time, file size, type, and linkage to Observations. Flags like has_human, archived, do_not_remove control processing/cleanup. DataType : Defines the type of data collected (e.g., image, audio). DataPackage : Groups files and metadata for download. Includes a ZIP archive, metadata format, and status flag. Taxonomy and Annotations Taxon : Represents a species or higher-level taxonomy. Hierarchical, using parent relationships. Supports GBIF integration for standard codes. Observation : Annotation or detection of a Taxon in a DataFile. Includes bounding box, confidence, and optional biological metadata. Can be validated by another Observation. Automation ProjectJob : Defines automated Celery tasks for a Project. Stores task name and arguments as JSON. Common Fields All major entities include: Audit fields: created_on , modified_on Permissions: owner , managers , viewers , annotators","title":"Overview"},{"location":"structure/#project-structure","text":"Project : Central organizational unit representing a study or research effort. Linked to multiple Sites (locations), Devices (sensors), and Deployments. Has roles: owner, managers, viewers, and annotators. Can define ProjectJobs for automated tasks.","title":"Project Structure"},{"location":"structure/#site","text":"Site : A physical location. Has a name and optional short name. Used to contextualize Deployments.","title":"Site"},{"location":"structure/#devices-and-deployments","text":"DeviceModel : Blueprint for devices (e.g., camera model). Linked to a DataType (e.g., image, audio). Device : Physical unit based on a DeviceModel. Can be deployed at a Site. May include credentials for external storage. Deployment : A specific installation of a Device at a Site, possibly across Projects. Contains geolocation, time zone, and time range (start and end). Tracks activity and holds reference to the last captured image.","title":"Devices and Deployments"},{"location":"structure/#data-and-metadata","text":"DataFile : Represents individual files collected during a Deployment. Includes recording time, file size, type, and linkage to Observations. Flags like has_human, archived, do_not_remove control processing/cleanup. DataType : Defines the type of data collected (e.g., image, audio). DataPackage : Groups files and metadata for download. Includes a ZIP archive, metadata format, and status flag.","title":"Data and Metadata"},{"location":"structure/#taxonomy-and-annotations","text":"Taxon : Represents a species or higher-level taxonomy. Hierarchical, using parent relationships. Supports GBIF integration for standard codes. Observation : Annotation or detection of a Taxon in a DataFile. Includes bounding box, confidence, and optional biological metadata. Can be validated by another Observation.","title":"Taxonomy and Annotations"},{"location":"structure/#automation","text":"ProjectJob : Defines automated Celery tasks for a Project. Stores task name and arguments as JSON.","title":"Automation"},{"location":"structure/#common-fields","text":"All major entities include: Audit fields: created_on , modified_on Permissions: owner , managers , viewers , annotators","title":"Common Fields"},{"location":"structure/datafile/","text":"Datafile A datafile is a digital file generated by a device during a deployment. This can include images, audio recordings, sensor logs. Each datafile is linked to a specific deployment and contains metadata used for processing, indexing, and analysis. Search function & filters for datafiles A general search function and filters are available to subset datafiles Search Accepted search functions include and are limited to the full, or parts of, a tag , file name , species latin name , and species common name . Deployment active now? Allows a user to filter by datafiles from active deployments True or are innactive deployments False . Site Allows a user to filter datafiles associated with a specific site. Device type Allows a user to filter datafiles by device type. For example, a user can subset all datafiles from wildlife cameras. File type Allows a user to filter by the data type of files. e.g. wildlifecamera will show only images with this data type, excluding timelapsecamera and reports produced by the same device. File recorded after Allows a user to filter for datafiles recorder after a specified date and time. File recorded before Allows a user to filter for datafiles recorded before a specified date and time. File recorded after/before can be used in combination to define a time period. Has observations are datafiles that contain an observation of a species or category (e.g. vehicle, aircraft, empty) - No human observations filters for datafiles that do not contain human observations - All observations filters for all datafiles - Human observations filters for datafiles that contain human observations, but may also contain AI observations - AI observations filters for datafiles that contain AI observations, but may also contain human observations - Human observations only filters for datafiles that contain human observations only, no AI observations - AI observations only filters for datafiles that contain AI observations only, no human observations Uncertain observations are datafiles that have been flagged because they contain an observation that a user is uncertain about. The optional filters include: - No uncertain observations filters out datafiles containing uncertain observations - Uncertain observations filters for datafiles that contain uncertain observations - Other's uncertain observations filters for datafiles with uncertain observations of users other than yourself - My uncertain observations filters for your own datafiles containing uncertain observations File archived allows a user to filter datafiles for those that have already been archived in cold storage True or not False . Notably, files can be present as a copy in both archive and in the portal simultaneously. Select job One job is available at datafiles level. Create data package allows users to download a subset of datafiles, or export metadata in standard or camtrap DP format.","title":"Datafile"},{"location":"structure/datafile/#datafile","text":"A datafile is a digital file generated by a device during a deployment. This can include images, audio recordings, sensor logs. Each datafile is linked to a specific deployment and contains metadata used for processing, indexing, and analysis.","title":"Datafile"},{"location":"structure/datafile/#search-function-filters-for-datafiles","text":"A general search function and filters are available to subset datafiles Search Accepted search functions include and are limited to the full, or parts of, a tag , file name , species latin name , and species common name . Deployment active now? Allows a user to filter by datafiles from active deployments True or are innactive deployments False . Site Allows a user to filter datafiles associated with a specific site. Device type Allows a user to filter datafiles by device type. For example, a user can subset all datafiles from wildlife cameras. File type Allows a user to filter by the data type of files. e.g. wildlifecamera will show only images with this data type, excluding timelapsecamera and reports produced by the same device. File recorded after Allows a user to filter for datafiles recorder after a specified date and time. File recorded before Allows a user to filter for datafiles recorded before a specified date and time. File recorded after/before can be used in combination to define a time period. Has observations are datafiles that contain an observation of a species or category (e.g. vehicle, aircraft, empty) - No human observations filters for datafiles that do not contain human observations - All observations filters for all datafiles - Human observations filters for datafiles that contain human observations, but may also contain AI observations - AI observations filters for datafiles that contain AI observations, but may also contain human observations - Human observations only filters for datafiles that contain human observations only, no AI observations - AI observations only filters for datafiles that contain AI observations only, no human observations Uncertain observations are datafiles that have been flagged because they contain an observation that a user is uncertain about. The optional filters include: - No uncertain observations filters out datafiles containing uncertain observations - Uncertain observations filters for datafiles that contain uncertain observations - Other's uncertain observations filters for datafiles with uncertain observations of users other than yourself - My uncertain observations filters for your own datafiles containing uncertain observations File archived allows a user to filter datafiles for those that have already been archived in cold storage True or not False . Notably, files can be present as a copy in both archive and in the portal simultaneously. Select job One job is available at datafiles level. Create data package allows users to download a subset of datafiles, or export metadata in standard or camtrap DP format.","title":"Search function &amp; filters for datafiles"},{"location":"structure/deployment/","text":"Deployment A deployment refers to the placement of a monitoring device (such as a camera trap or audio recorder) at a specific location and time. It includes metadata such as device ID, geographic coordinates, installation settings, and duration. Deployments are linked to one or more projects and form the basis for data collection in the field. Search function & filters for deployments A general search function and filters are available to subset deployments Search Accepted search functions include and are limited to the full, or parts of, a deployment , device ID , device name , device , and extra data that is appended to the deployment. Deployment active now? Allows a user to filter by deployments that are active True or innactive False . Site Allows a user to filter for deployments in a specific site. Device type Allows a user to filter deployments by device type. For example, a user can subset all deployments of wildlife cameras. Deployment started after Allows a user to filter for deployments that begun after a specified date and time. Deployment started before Allows a user to filter for deployments that begun before a specified date and time. Deployment ended before Allows a user to filter for deployments that ended before a specified date and time. Deployment ended after Allows a user to filter for deployments that ended after a specified date and time. Select job There are currently no job options to select at device level. Add new deployment allows uers with Manager permissions to register a new deployment.","title":"Deployment"},{"location":"structure/deployment/#deployment","text":"A deployment refers to the placement of a monitoring device (such as a camera trap or audio recorder) at a specific location and time. It includes metadata such as device ID, geographic coordinates, installation settings, and duration. Deployments are linked to one or more projects and form the basis for data collection in the field.","title":"Deployment"},{"location":"structure/deployment/#search-function-filters-for-deployments","text":"A general search function and filters are available to subset deployments Search Accepted search functions include and are limited to the full, or parts of, a deployment , device ID , device name , device , and extra data that is appended to the deployment. Deployment active now? Allows a user to filter by deployments that are active True or innactive False . Site Allows a user to filter for deployments in a specific site. Device type Allows a user to filter deployments by device type. For example, a user can subset all deployments of wildlife cameras. Deployment started after Allows a user to filter for deployments that begun after a specified date and time. Deployment started before Allows a user to filter for deployments that begun before a specified date and time. Deployment ended before Allows a user to filter for deployments that ended before a specified date and time. Deployment ended after Allows a user to filter for deployments that ended after a specified date and time. Select job There are currently no job options to select at device level. Add new deployment allows uers with Manager permissions to register a new deployment.","title":"Search function &amp; filters for deployments"},{"location":"structure/device/","text":"Device A device refers to any sensor or equipment used to collect ecological monitoring data, such as camera traps, acoustic recorders, or data loggers. Each device is uniquely identified and can be associated with one or more deployments over time. Search function & filters for devices A general search function and filters are available to subset devices Search Accepted search functions include and are limited to the full, or parts of, a device ID , name , and model name . Deployment active now? Allows a user to filter by devices that currently have an active deployment True or are innactive deployment False . Site Allows a user to filter by devices that are or have previously been deployed at a certain site. Device type allows a user to filter devices by device type. For example, a user can subset all devices which are wildlife cameras. Select job There are currently no job options to select at device level. Add new device allows uers with Manager permissions to register new device.","title":"Device"},{"location":"structure/device/#device","text":"A device refers to any sensor or equipment used to collect ecological monitoring data, such as camera traps, acoustic recorders, or data loggers. Each device is uniquely identified and can be associated with one or more deployments over time.","title":"Device"},{"location":"structure/device/#search-function-filters-for-devices","text":"A general search function and filters are available to subset devices Search Accepted search functions include and are limited to the full, or parts of, a device ID , name , and model name . Deployment active now? Allows a user to filter by devices that currently have an active deployment True or are innactive deployment False . Site Allows a user to filter by devices that are or have previously been deployed at a certain site. Device type allows a user to filter devices by device type. For example, a user can subset all devices which are wildlife cameras. Select job There are currently no job options to select at device level. Add new device allows uers with Manager permissions to register new device.","title":"Search function &amp; filters for devices"},{"location":"structure/project/","text":"Project A project represents a defined ecological or scientific effort under which data is collected. It includes contextual information such as goals, study area, team members, and time frames. Projects serve as containers for organizing related deployments and observations. SEARCH FUNCTION & FILTERS IN PROJECTS A general search function and filters are available to subset projects . Search Accepted search functions include and are limited to the full, or parts of a site name. For example Amsterdamse or Amsterdamse Waterleidingduinen , or AWD . Deployment active now? Allows you to filter by True for projects which are currently active, and False for projects which are no longer active. Select job There are currently no job options to select at project level.","title":"Project"},{"location":"structure/project/#project","text":"A project represents a defined ecological or scientific effort under which data is collected. It includes contextual information such as goals, study area, team members, and time frames. Projects serve as containers for organizing related deployments and observations.","title":"Project"},{"location":"structure/project/#search-function-filters-in-projects","text":"A general search function and filters are available to subset projects . Search Accepted search functions include and are limited to the full, or parts of a site name. For example Amsterdamse or Amsterdamse Waterleidingduinen , or AWD . Deployment active now? Allows you to filter by True for projects which are currently active, and False for projects which are no longer active. Select job There are currently no job options to select at project level.","title":"SEARCH FUNCTION &amp; FILTERS IN PROJECTS"},{"location":"structure/detail/","text":"","title":"index"},{"location":"structure/detail/datafile/","text":"DataFile Description: Represents a data file associated with a deployment. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. deployment ForeignKey Deployment to which this data file is linked. file_type ForeignKey Data type of the file. file_name CharField File name. file_size FileSizeField Size of file in bytes. file_format CharField File extension. upload_dt DateTimeField Datetime when the file was uploaded. recording_dt DateTimeField Datetime when the file was recorded. path CharField Relative path. local_path CharField Absolute path on local storage. extra_data JSONField Extra data that does not fit in existing columns. linked_files JSONField Linked files such as alternative representations. thumb_url CharField Thumbnail URL. local_storage BooleanField Whether the file is available on local storage. archived BooleanField Whether the file has been archived. tar_file ForeignKey TAR file containing this file. favourite_of ManyToManyField Users who have favourited this file. do_not_remove BooleanField If True, the file will not be removed during cleaning. original_name CharField Original name of the file. file_url CharField URL of this file. tag CharField Additional identifying tag of this file. has_human BooleanField True if this image has been annotated with a human. Methods __str__() : Returns the full file name. get_absolute_url() : Returns the URL to this file. add_favourite(user) : Adds a user to the list of favourites. remove_favourite(user) : Removes a user from the list of favourites. full_path() : Returns the full path to this file. thumb_path() : Returns the thumbnail path. set_file_url() : Sets the URL for this file. set_linked_files_urls() : Sets URLs for linked file representations. set_thumb_url(has_thumb=True) : Sets or clears the thumbnail URL. check_human() : Updates the has_human flag based on associated observations. clean_file(delete_obj=False, force_delete=False) : Cleans up file and resources. save() : Assigns file_type , sets file URL, and saves. clean() : Validates file against deployment date range.","title":"DataFile"},{"location":"structure/detail/datafile/#datafile","text":"Description: Represents a data file associated with a deployment.","title":"DataFile"},{"location":"structure/detail/datafile/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. deployment ForeignKey Deployment to which this data file is linked. file_type ForeignKey Data type of the file. file_name CharField File name. file_size FileSizeField Size of file in bytes. file_format CharField File extension. upload_dt DateTimeField Datetime when the file was uploaded. recording_dt DateTimeField Datetime when the file was recorded. path CharField Relative path. local_path CharField Absolute path on local storage. extra_data JSONField Extra data that does not fit in existing columns. linked_files JSONField Linked files such as alternative representations. thumb_url CharField Thumbnail URL. local_storage BooleanField Whether the file is available on local storage. archived BooleanField Whether the file has been archived. tar_file ForeignKey TAR file containing this file. favourite_of ManyToManyField Users who have favourited this file. do_not_remove BooleanField If True, the file will not be removed during cleaning. original_name CharField Original name of the file. file_url CharField URL of this file. tag CharField Additional identifying tag of this file. has_human BooleanField True if this image has been annotated with a human.","title":"Fields"},{"location":"structure/detail/datafile/#methods","text":"__str__() : Returns the full file name. get_absolute_url() : Returns the URL to this file. add_favourite(user) : Adds a user to the list of favourites. remove_favourite(user) : Removes a user from the list of favourites. full_path() : Returns the full path to this file. thumb_path() : Returns the thumbnail path. set_file_url() : Sets the URL for this file. set_linked_files_urls() : Sets URLs for linked file representations. set_thumb_url(has_thumb=True) : Sets or clears the thumbnail URL. check_human() : Updates the has_human flag based on associated observations. clean_file(delete_obj=False, force_delete=False) : Cleans up file and resources. save() : Assigns file_type , sets file URL, and saves. clean() : Validates file against deployment date range.","title":"Methods"},{"location":"structure/detail/datapackage/","text":"DataPackage Description: Model representing a data package, containing files and metadata for download or processing. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField The name of the data package. data_files ManyToManyField The data files included in this package. owner ForeignKey The user who owns this data package. status IntegerField The current status of the data package. metadata_type IntegerField The type of metadata associated with this package. includes_files BooleanField Whether the package includes files. file_url CharField URL to download the zipped data package. Methods __str__() : Returns the name of the data package. set_file_url() : Sets the file_url if the package is marked as ready. make_zip() : Creates a ZIP archive of the data files and updates status. save() : Overrides save to update file_url before saving. clean_data_package() : Removes the ZIP file from storage if ready or failed. Signals pre_delete : Cleans up the ZIP archive before the data package is deleted.","title":"DataPackage"},{"location":"structure/detail/datapackage/#datapackage","text":"Description: Model representing a data package, containing files and metadata for download or processing.","title":"DataPackage"},{"location":"structure/detail/datapackage/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField The name of the data package. data_files ManyToManyField The data files included in this package. owner ForeignKey The user who owns this data package. status IntegerField The current status of the data package. metadata_type IntegerField The type of metadata associated with this package. includes_files BooleanField Whether the package includes files. file_url CharField URL to download the zipped data package.","title":"Fields"},{"location":"structure/detail/datapackage/#methods","text":"__str__() : Returns the name of the data package. set_file_url() : Sets the file_url if the package is marked as ready. make_zip() : Creates a ZIP archive of the data files and updates status. save() : Overrides save to update file_url before saving. clean_data_package() : Removes the ZIP file from storage if ready or failed.","title":"Methods"},{"location":"structure/detail/datapackage/#signals","text":"pre_delete : Cleans up the ZIP archive before the data package is deleted.","title":"Signals"},{"location":"structure/detail/datatype/","text":"DataType Description: Describes a type of data, including its name, color, and optional icon. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Name of data type. colour ColorField Colour to use for this data type. symbol IconField Symbol to use for this data type. Methods __str__() : Returns the name of the data type.","title":"DataType"},{"location":"structure/detail/datatype/#datatype","text":"Description: Describes a type of data, including its name, color, and optional icon.","title":"DataType"},{"location":"structure/detail/datatype/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Name of data type. colour ColorField Colour to use for this data type. symbol IconField Symbol to use for this data type.","title":"Fields"},{"location":"structure/detail/datatype/#methods","text":"__str__() : Returns the name of the data type.","title":"Methods"},{"location":"structure/detail/deployment/","text":"Deployment Description: Records a deployment of a device to a site, within a project. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. deployment_device_ID CharField Unique identifier combining deployment_ID , device_type , and device_n . deployment_ID CharField An identifier for a deployment. device_type ForeignKey Primary data type of deployment. device_n IntegerField Numeric suffix for disambiguating deployments with the same ID/type. deployment_start DateTimeField Start datetime of deployment. deployment_end DateTimeField End datetime of deployment. Can be NULL for ongoing deployments. device ForeignKey Device used in this deployment. site ForeignKey Site where the deployment takes place. project ManyToManyField Projects to which this deployment is attached. latitude DecimalField Latitude of deployment. longitude DecimalField Longitude of deployment. point PointField Spatial point representing this deployment. extra_data JSONField Extra data that doesn't fit in other fields. is_active BooleanField Whether the deployment is currently active. time_zone TimeZoneField Time zone for this deployment. owner ForeignKey Owner of deployment. managers ManyToManyField Managers of deployment. viewers ManyToManyField Viewers of deployment. annotators ManyToManyField Annotators of deployment. combo_project CharField Combined project identifiers string. last_image ForeignKey Last image (if any) linked to this deployment. thumb_url CharField Deployment thumbnail URL. Methods get_absolute_url() : Returns the URL to this deployment's detail page. __str__() : Returns the deployment_device_ID . clean() : Validates deployment start/end time and checks for overlapping deployments. save() : Sets fields such as deployment_device_ID , is_active , point , and combo_project . get_permissions() : Propagates permissions from the device and project. get_combo_project() : Returns a space-separated string of sorted project IDs. check_active() : Returns True if the deployment is currently active. check_dates(dt_list) : Returns a list indicating whether each datetime falls within the deployment range. set_thumb_url() : Sets the thumbnail URL and last image for the deployment.","title":"Deployment"},{"location":"structure/detail/deployment/#deployment","text":"Description: Records a deployment of a device to a site, within a project.","title":"Deployment"},{"location":"structure/detail/deployment/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. deployment_device_ID CharField Unique identifier combining deployment_ID , device_type , and device_n . deployment_ID CharField An identifier for a deployment. device_type ForeignKey Primary data type of deployment. device_n IntegerField Numeric suffix for disambiguating deployments with the same ID/type. deployment_start DateTimeField Start datetime of deployment. deployment_end DateTimeField End datetime of deployment. Can be NULL for ongoing deployments. device ForeignKey Device used in this deployment. site ForeignKey Site where the deployment takes place. project ManyToManyField Projects to which this deployment is attached. latitude DecimalField Latitude of deployment. longitude DecimalField Longitude of deployment. point PointField Spatial point representing this deployment. extra_data JSONField Extra data that doesn't fit in other fields. is_active BooleanField Whether the deployment is currently active. time_zone TimeZoneField Time zone for this deployment. owner ForeignKey Owner of deployment. managers ManyToManyField Managers of deployment. viewers ManyToManyField Viewers of deployment. annotators ManyToManyField Annotators of deployment. combo_project CharField Combined project identifiers string. last_image ForeignKey Last image (if any) linked to this deployment. thumb_url CharField Deployment thumbnail URL.","title":"Fields"},{"location":"structure/detail/deployment/#methods","text":"get_absolute_url() : Returns the URL to this deployment's detail page. __str__() : Returns the deployment_device_ID . clean() : Validates deployment start/end time and checks for overlapping deployments. save() : Sets fields such as deployment_device_ID , is_active , point , and combo_project . get_permissions() : Propagates permissions from the device and project. get_combo_project() : Returns a space-separated string of sorted project IDs. check_active() : Returns True if the deployment is currently active. check_dates(dt_list) : Returns a list indicating whether each datetime falls within the deployment range. set_thumb_url() : Sets the thumbnail URL and last image for the deployment.","title":"Methods"},{"location":"structure/detail/device/","text":"Device Description: Represents a physical device or sensor. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. device_ID CharField Unique identifier for device, such as a serial number. name CharField Optional alternative name for device. model ForeignKey Device model. type ForeignKey Device type, usually inherited from model. owner ForeignKey Device owner. managers ManyToManyField Device managers. viewers ManyToManyField Device viewers. annotators ManyToManyField Device annotators. autoupdate BooleanField Is the device expected to autoupdate? update_time IntegerField Hours between expected updates. username CharField Device username for use with external storage. password EncryptedCharField Device password for use with external storage. input_storage ForeignKey External storage for device. extra_data JSONField Extra data that doesn't fit in existing fields. Methods is_active() : Returns True if the device has at least one active deployment. __str__() : Returns the device ID. get_absolute_url() : Returns the URL to view this device. save() : Sets the type from the model if not explicitly set. clean() : Validates that the device type matches the model's type. deployment_from_date(dt) : Returns the deployment for this device active at the given datetime. check_overlap(new_start, new_end, deployment_pk) : Checks for overlapping deployments in the given date range, excluding a given deployment.","title":"Device"},{"location":"structure/detail/device/#device","text":"Description: Represents a physical device or sensor.","title":"Device"},{"location":"structure/detail/device/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. device_ID CharField Unique identifier for device, such as a serial number. name CharField Optional alternative name for device. model ForeignKey Device model. type ForeignKey Device type, usually inherited from model. owner ForeignKey Device owner. managers ManyToManyField Device managers. viewers ManyToManyField Device viewers. annotators ManyToManyField Device annotators. autoupdate BooleanField Is the device expected to autoupdate? update_time IntegerField Hours between expected updates. username CharField Device username for use with external storage. password EncryptedCharField Device password for use with external storage. input_storage ForeignKey External storage for device. extra_data JSONField Extra data that doesn't fit in existing fields.","title":"Fields"},{"location":"structure/detail/device/#methods","text":"is_active() : Returns True if the device has at least one active deployment. __str__() : Returns the device ID. get_absolute_url() : Returns the URL to view this device. save() : Sets the type from the model if not explicitly set. clean() : Validates that the device type matches the model's type. deployment_from_date(dt) : Returns the deployment for this device active at the given datetime. check_overlap(new_start, new_end, deployment_pk) : Checks for overlapping deployments in the given date range, excluding a given deployment.","title":"Methods"},{"location":"structure/detail/devicemodel/","text":"DeviceModel Description: Represents a type of device, its manufacturer, and data type. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Name of device model. Used to find a data handler if available. manufacturer CharField Device model manufacturer. type ForeignKey Primary data type of device. owner ForeignKey User who registered this device model. colour ColorField Override data type colour. Leave blank to use the default from data type. symbol IconField Override data type symbol. Leave blank to use the default from data type. Methods __str__() : Returns the name of the device model.","title":"DeviceModel"},{"location":"structure/detail/devicemodel/#devicemodel","text":"Description: Represents a type of device, its manufacturer, and data type.","title":"DeviceModel"},{"location":"structure/detail/devicemodel/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Name of device model. Used to find a data handler if available. manufacturer CharField Device model manufacturer. type ForeignKey Primary data type of device. owner ForeignKey User who registered this device model. colour ColorField Override data type colour. Leave blank to use the default from data type. symbol IconField Override data type symbol. Leave blank to use the default from data type.","title":"Fields"},{"location":"structure/detail/devicemodel/#methods","text":"__str__() : Returns the name of the device model.","title":"Methods"},{"location":"structure/detail/observation/","text":"Observation Description: Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. owner ForeignKey User who created the observation. Null if created by AI. label CharField Generated label for the observation. taxon ForeignKey Taxon of the observed species. data_files ManyToManyField Data files associated with the observation. obs_dt DateTimeField Date and time of the observation. source CharField Source of the observation (e.g. 'human', or AI model name). number IntegerField Number of individuals observed. bounding_box JSONField Bounding box in format: {x1, y1, x2, y2} . confidence FloatField Confidence score from AI. extra_data JSONField Extra metadata that doesn\u2019t fit standard fields. sex CharField Sex of the observed species, if known. lifestage CharField Lifestage of the observed species, if known. behavior CharField Behavior of the observed species. validation_requested BooleanField Whether human validation is requested. validation_of ManyToManyField Link to the original observation(s) if this is a validation. Methods __str__() : Returns the label of the observation. get_absolute_url() : Returns the URL of the first associated data file. get_label() : Generates a label from the taxon and file name. get_taxonomic_level(level) : Returns the Taxon at the specified level. save() : Sets label and observation date automatically. check_data_files_human() : Updates associated files if the taxon is human. Signals m2m_changed : Updates observation when its data files are modified. post_save : Re-checks has_human flag on save. post_delete : Re-checks has_human flag when deleted.","title":"Observation"},{"location":"structure/detail/observation/#observation","text":"Description: Model representing an observation of a taxon, potentially including metadata such as data files, bounding box, confidence, sex, lifestage, and validation status.","title":"Observation"},{"location":"structure/detail/observation/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. owner ForeignKey User who created the observation. Null if created by AI. label CharField Generated label for the observation. taxon ForeignKey Taxon of the observed species. data_files ManyToManyField Data files associated with the observation. obs_dt DateTimeField Date and time of the observation. source CharField Source of the observation (e.g. 'human', or AI model name). number IntegerField Number of individuals observed. bounding_box JSONField Bounding box in format: {x1, y1, x2, y2} . confidence FloatField Confidence score from AI. extra_data JSONField Extra metadata that doesn\u2019t fit standard fields. sex CharField Sex of the observed species, if known. lifestage CharField Lifestage of the observed species, if known. behavior CharField Behavior of the observed species. validation_requested BooleanField Whether human validation is requested. validation_of ManyToManyField Link to the original observation(s) if this is a validation.","title":"Fields"},{"location":"structure/detail/observation/#methods","text":"__str__() : Returns the label of the observation. get_absolute_url() : Returns the URL of the first associated data file. get_label() : Generates a label from the taxon and file name. get_taxonomic_level(level) : Returns the Taxon at the specified level. save() : Sets label and observation date automatically. check_data_files_human() : Updates associated files if the taxon is human.","title":"Methods"},{"location":"structure/detail/observation/#signals","text":"m2m_changed : Updates observation when its data files are modified. post_save : Re-checks has_human flag on save. post_delete : Re-checks has_human flag when deleted.","title":"Signals"},{"location":"structure/detail/project/","text":"Project Description: Represents a project and its metadata, including ownership and relations. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. project_ID CharField Unique project identifier. Auto-generated from name if not set. name CharField Full project name. objectives CharField Project objectives description. principal_investigator CharField Full name of principal investigator. principal_investigator_email CharField Principal investigator email. contact CharField Name of primary contact. contact_email CharField Contact email. organisation CharField Organisation with which this project is associated. data_storages ManyToManyField External data storages available to this project. archive ForeignKey Data archive for project data. automated_tasks ManyToManyField Automated project jobs. owner ForeignKey Project owner. managers ManyToManyField Project managers. viewers ManyToManyField Project viewers. annotators ManyToManyField Project annotators. clean_time IntegerField Days after last modification before archived file is removed from storage. Methods is_active() : Returns True if this project has at least one active deployment. __str__() : Returns the project_ID . get_absolute_url() : Returns the URL for the project detail view. save() : Auto-generates project_ID from name if not already set.","title":"Project"},{"location":"structure/detail/project/#project","text":"Description: Represents a project and its metadata, including ownership and relations.","title":"Project"},{"location":"structure/detail/project/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. project_ID CharField Unique project identifier. Auto-generated from name if not set. name CharField Full project name. objectives CharField Project objectives description. principal_investigator CharField Full name of principal investigator. principal_investigator_email CharField Principal investigator email. contact CharField Name of primary contact. contact_email CharField Contact email. organisation CharField Organisation with which this project is associated. data_storages ManyToManyField External data storages available to this project. archive ForeignKey Data archive for project data. automated_tasks ManyToManyField Automated project jobs. owner ForeignKey Project owner. managers ManyToManyField Project managers. viewers ManyToManyField Project viewers. annotators ManyToManyField Project annotators. clean_time IntegerField Days after last modification before archived file is removed from storage.","title":"Fields"},{"location":"structure/detail/project/#methods","text":"is_active() : Returns True if this project has at least one active deployment. __str__() : Returns the project_ID . get_absolute_url() : Returns the URL for the project detail view. save() : Auto-generates project_ID from name if not already set.","title":"Methods"},{"location":"structure/detail/site/","text":"Site Description: Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name . Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Site name. short_name CharField Site short name. Methods __str__() : Returns the site name. save() : Sets short_name from name if it is blank before saving.","title":"Site"},{"location":"structure/detail/site/#site","text":"Description: Represents a site with a name and an optional short name. If short_name is blank, it defaults to the first 10 characters of name .","title":"Site"},{"location":"structure/detail/site/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. name CharField Site name. short_name CharField Site short name.","title":"Fields"},{"location":"structure/detail/site/#methods","text":"__str__() : Returns the site name. save() : Sets short_name from name if it is blank before saving.","title":"Methods"},{"location":"structure/detail/taxon/","text":"Taxon Description: Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships. Fields Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. species_name CharField Scientific name of the species (e.g., 'Aquila chrysaetos'). species_common_name CharField Common name of the species (e.g., 'Golden Eagle'). taxon_code CharField Identifier from a taxonomic database (e.g., GBIF ID or custom code like 'vehicle'). taxon_source IntegerField Source of the taxon code. 0 = custom, 1 = GBIF. extra_data JSONField Extra metadata about the taxon (e.g., Avibase ID). parents ManyToManyField Parent taxons of this taxon. taxonomic_level IntegerField Taxonomic level (0 = species, 1 = genus, 2 = family, etc.). Methods __str__() : Returns the scientific name. get_taxonomic_level(level) : Retrieve the taxon at the specified taxonomic level. get_taxon_code() : Retrieves or generates the taxon code using GBIF and updates metadata if applicable. save() : Triggers code generation, de-duplicates similar entries, and persists updates. Signals post_save : When a Taxon with a GBIF code is saved, it triggers asynchronous creation of parent taxons.","title":"Taxon"},{"location":"structure/detail/taxon/#taxon","text":"Description: Model representing a biological taxon (e.g., species, genus, family) with support for hierarchical relationships.","title":"Taxon"},{"location":"structure/detail/taxon/#fields","text":"Field Type Description created_on DateTimeField Auto timestamp on object creation. modified_on DateTimeField Auto timestamp on every save. species_name CharField Scientific name of the species (e.g., 'Aquila chrysaetos'). species_common_name CharField Common name of the species (e.g., 'Golden Eagle'). taxon_code CharField Identifier from a taxonomic database (e.g., GBIF ID or custom code like 'vehicle'). taxon_source IntegerField Source of the taxon code. 0 = custom, 1 = GBIF. extra_data JSONField Extra metadata about the taxon (e.g., Avibase ID). parents ManyToManyField Parent taxons of this taxon. taxonomic_level IntegerField Taxonomic level (0 = species, 1 = genus, 2 = family, etc.).","title":"Fields"},{"location":"structure/detail/taxon/#methods","text":"__str__() : Returns the scientific name. get_taxonomic_level(level) : Retrieve the taxon at the specified taxonomic level. get_taxon_code() : Retrieves or generates the taxon code using GBIF and updates metadata if applicable. save() : Triggers code generation, de-duplicates similar entries, and persists updates.","title":"Methods"},{"location":"structure/detail/taxon/#signals","text":"post_save : When a Taxon with a GBIF code is saved, it triggers asynchronous creation of parent taxons.","title":"Signals"}]}